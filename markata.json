{"config": {"glob_patterns": ["pages/**/*.md"], "hooks": ["markata.plugins.publish_source", "markata.plugins.publish_dev_to_source", "plugins.render_template_variables", "plugins.codeblocks", "plugins.index", "plugins.slug", "plugins.one_line_link", "plugins.include", "default", "plugins.custom_seo", "plugins.gif_to_mp4", "plugins.youtube", "plugins.rss_tag", "plugins.auto_publish"], "disabled_hooks": ["markata.plugins.covers", "markata.plugins.flat_slug"], "markdown_extensions": [], "default_cache_expire": 1209600, "output_dir": "markout", "assets_dir": "static", "url": "https://pype.dev", "images_url": "/static/images", "title": "Pype's mental data lake", "description": "I write about how I use python for data-centric work and my homelab journey", "rss_description": "Pypeaday posts", "author_name": "Nicholas Payne", "author_email": "pypeaday@pm.me", "icon": "bitmoji.png", "lang": "en", "post_template": "pages/templates/post_template.html", "amp_template": "pages/templates/post_template.amp.html", "theme_color": "#3A8695", "background_color": "#3A9577", "start_url": "/", "site_name": "Pypeaday", "short_name": "pype", "display": "minimal-ui", "twitter_card": "summary_large_image", "twitter_creator": "@pypeaday", "twitter_site": "@pypeaday", "auto_publish": {"filters": {"til": "templateKey==\"til\" and date<=today"}, "cache_expire": 1209600, "config_key": "auto_publish"}, "archive": {"archive_template": "pages/templates/archive_template.html", "pages": {"archive": {"filter": "date<=today and templateKey in ['blog-post', 'til'] and status.lower()=='published'"}, "blog": {"filter": "date<=today and templateKey in ['blog-post'] and status.lower()=='published'"}, "draft": {"filter": "status=='draft'"}, "scheduled": {"filter": "date>today"}, "today": {"filter": "date==today"}, "python": {"filter": "date<=today and 'python' in tags"}, "python-draft": {"filter": "date<=today and 'python' in tags and status=='draft'"}, "linux": {"filter": "date<=today and 'linux' in tags"}, "linux-draft": {"filter": "date<=today and 'linux' in tags and status=='draft'"}, "vim": {"filter": "date<=today and 'vim' in tags"}, "vim-draft": {"filter": "date<=today and 'vim' in tags and status=='draft'"}, "kedro": {"filter": "date<=today and 'kedro' in tags"}, "kedro-draft": {"filter": "date<=today and 'kedro' in tags and status=='draft'"}, "homelab": {"filter": "date<=today and 'homelab' in tags"}, "homelab-draft": {"filter": "date<=today and 'homelab' in tags and status=='draft'"}, "til": {"template": "pages/templates/archive_template.html", "filter": "date<=today and templateKey == 'til'"}, "til-vim": {"template": "pages/templates/archive_template.html", "filter": "date<=today and templateKey == 'til' and 'vim' in tags"}}}, "covers": [{"name": "-dev", "template": "static/cover-template.png", "font": "./.github/scripts/waterlilly-script.ttf", "font_color": "rgb(255,255,255)"}, {"name": "-og", "template": "static/og-template.png", "font": "./.github/scripts/waterlilly-script.ttf", "font_color": "rgb(255,255,255)", "text_font": "./static/JosefinSans-Regular.ttf", "text_font_color": "rgb(200,200,200)"}, {"name": "-hashnode", "template": "static/hashnode-template.png", "font": "./.github/scripts/waterlilly-script.ttf", "font_color": "rgb(255,255,255)"}], "seo": [{"name": "og:type", "property": "og:type", "content": "article"}, {"name": "og:author", "property": "og:author", "content": "Nicholas Payne"}, {"name": "og:site_name", "property": "og:site_name", "content": "Nicholas Payne"}, {"name": "theme-color", "content": "#3A5895"}]}, "articles": [{"cover": "/static/plotly-and-streamlit.png", "title": "Plotly-And-Streamlit", "tags": ["python"], "status": "published", "templateKey": "blog-post", "path": "pages/blog/plotly-and-streamlit.md", "description": "I use  At the highest level, streamlit lets you write a python script and call  I I Suffice it to say it For my  See  First step is to initialize some objects t", "content": "## Streamlit\n\n\nI use `streamlit` for any EDA I ever have to do at work.\nIt's super easy to spin up a small dashboard to filter and view dataframes in, live, without the fallbacks of Jupyter notebooks (kernels dying, memory bloat, a billion \"Untitled N.ipynb\" files, etc.)\n\nAt the highest level, streamlit lets you write a python script and call `streamlit run my_script.py` which will open up a web server with your streamlit stuff. \nThe dashboard refreshes whenever you change the script so you can add capabilities in real time, super fast!\n\n\nI'll show an example of using `streamlit` and `plotly` to make a live dashboard to monitor system memory usage with `psutil`.\nThis is apart of my posts on [psutil](/psutil) and [deques](/deques)...\n\n__example at the bottom!__\n\n\n\n## Plotly\n\nI'm not going to make a big time intro to plotly here - there's a billion resources on the interwebs and the docs are really good.\n\nSuffice it to say it's my goto plotting library for basically any and all needs.\nI'm currently exploring it for live data streaming as I'm not sure it's the best solution but it's the one I'm familiar with.\n\nFor my [ not-netdata ](https://github.com/nicpayne713/not-netdata) project of visualizing live system resource data I  first need a way of appending data and popping data in and out of an array at every data refresh cycle to keep my plots looking nice with a fixed time window.\n\nSee [deques](/deques) for a short intro to the datatype I'm using.\n\nFirst step is to initialize some objects to store data in.\n\n```python\ndata: Dict[str, MutableSequence[Optional[float]]] = defaultdict(deque)\n\narr_size = 10\n\ndata[\"time\"] = deque([None] * arr_size)\ndata[\"used_memory\"] = deque([None] * arr_size)\n```\n\n`data` is a dictionary that I'll store deques in. The dictionary keys will be the type of data, in this case `time` and `used_memory`.\n\nI fix an array size, `arr_size` to just 10 for now\n\nThen I initialize the values for `time` and `used_memory` as `deque`s of length `arr_size`.\nSimple enough!\n\nNext is to fill those deques with some relevant data.\nI'm not actually sure if this is the best way to do this but here's what I have done so far:\n\n```python\ndef refresh_data():\n    global data\n    memory = psutil.virtual_memory()\n\n    data[\"time\"].append(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n    data[\"used_memory\"].append(memory.used // (1024**3))\n\n    data[\"time\"].popleft()\n    data[\"used_memory\"].popleft()\n```\n\nIf you ignore my usage of `global` you'll see that I can just `append` to each deque like it was a list.\n\nBut then to keep the relevant data in the deque, and to keep the length fixed, I simply `popleft` to remove the oldest datapoint!\n\n\n## A trivial dashboard\n\nNow I'll prove just how easy it is to get a live data dashboard up and running with just a few lines of code thanks to streamlit!\n\n```python\n\nif __name__ == \"__main__\":\n    st.header(\"memory chart\")\n    stats = st.empty()\n    while True:\n        refresh_data()\n        stats.plotly_chart(\n            px.line(\n                data,\n                x=\"time\",\n                y=\"used_memory\",\n                title=f\"Memory usage stored in a deque!\",\n               )\n            )\n        time.sleep(0.5)\n```\n\n`st` is the streamlit alias (imports shows at the bottom full example).\n`st.header` puts a nice header on the page.\n`st.empty` initializes an empty `streamlit container` in which we'll put a `plotly.express` figure.\n\nAt each iteration we'll `refresh_data()` which `appends` and `pops` data in the deques in the `data` dictionary.\nThen we update the `stats` container with a plotly graph and the refresh happens seamlessly.\n\nAll in all the script looks like this:\n\n```python\n\nfrom collections import defaultdict, deque\nimport time\nfrom typing import Dict, MutableSequence, Optional\n\nfrom plotly import express as px\nimport psutil\nimport streamlit as st\n\ndata: Dict[str, MutableSequence[Optional[float]]] = defaultdict(deque)\n\narr_size = 10\n\ndata[\"time\"] = deque([None] * arr_size)\ndata[\"used_memory\"] = deque([None] * arr_size)\n\n\ndef refresh_data():\n    global data\n    memory = psutil.virtual_memory()\n\n    data[\"time\"].append(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n    data[\"used_memory\"].append(memory.used // (1024**3))\n\n    data[\"time\"].popleft()\n    data[\"used_memory\"].popleft()\n\n\ndef memory_chart():\n    fig = px.line(\n        data,\n        x=\"time\",\n        y=\"used_memory\",\n        title=f\"Memory usage stored in a deque!\",\n    )\n    return fig\n\n\nif __name__ == \"__main__\":\n    st.header(\"memory chart\")\n    stats = st.empty()\n    while True:\n        refresh_data()\n        stats.plotly_chart(memory_chart())\n        time.sleep(0.5)\n```\n\nYou can save this as `my_dash.py` and run with `streamlit run my_dash.py` and should see something like the following!\n\n![Alt Text](/images/plotly-streamlit.gif \"plotly-streamlit-gif\")", "date": "2022-03-31", "edit_link": "https://github.com/edit/main/pages/blog/plotly-and-streamlit.md", "today": "2022-06-25", "now": "2022-06-25 12:28:31.767776", "datetime": "2022-03-31 00:00:00+00:00", "slug": "plotly-and-streamlit", "jinja": false, "output_html": "markout/plotly-and-streamlit/index.html", "long_description": "I use  At the highest level, streamlit lets you write a python script and call  I I Suffice it to say it For my  See  First step is to initialize some objects to store data in. data I fix an array size,  Then I initialize the values for  Next is to f", "year": 2022}, {"cover": "/static/tdarr.png", "title": "Tdarr", "tags": ["blog", "homelab"], "status": "draft", "templateKey": "blog-post", "path": "pages/blog/tdarr.md", "description": "", "content": "", "date": "2022-04-28", "edit_link": "https://github.com/edit/main/pages/blog/tdarr.md", "today": "2022-06-25", "now": "2022-06-25 12:28:31.767789", "datetime": "2022-04-28 00:00:00+00:00", "slug": "tdarr", "jinja": false, "output_html": "markout/tdarr/index.html", "long_description": "", "year": 2022}, {"cover": "/static/pipx.png", "title": "Pipx", "tags": ["til", "python"], "status": "published", "templateKey": "blog-post", "path": "pages/blog/pipx.md", "description": "pipx pinning formatting tools like  keeping virtual environments clean of things like  python utilities I want system wide but not in the global environment, li", "content": "`pipx` is a tool I've been using to solve a few problems of mine...\n\n1. pinning formatting tools like `black`, `flake8`, `isort`, etc. to the same version for all my projects\n2. keeping virtual environments clean of things like `cookiecutter`\n3. python utilities I want system wide but not in the global environment, like `visidata`\n\n## What is it?\n\n`pip` itself is just a package manager like `homebrew`, `apt`, etc. But it is tied to a python environment.\nIf you aren't using a virtual environment then `pip` will operate inside the global installation of python.\n\nOperating within that environment has burned me several times and now I have a strict virtual environment usage policy.\n\nBut there are still things I don't want to have to put in every virtual environment - enter `pipx`\n\n## What's it do?\n\nWhen you `pipx install {package}` a stand alone virtual environment gets created (by default in `~/.local/pipx/venvs`).\nTHen you can install extra dependencies with `pipx inject {package} {dependency}`\n\n> ex. After `pipx install visidata` in order to open Excel files you need to `pipx inject visidata xlrd`\n\nIn the example with `visidata`, I can then use it anywhere, in any project, without re-installing with `pip` in every env.\n\nAlso for the formatting tools - I configure vim to run the `pipx` versions of them on save - this way I don't have to put them in every project's virtual environment!\n\n## What about pip?\n\nSo obviously you can't `pipx` everything, nor do you want to. \nI see it as a safe and better alternative to global package installation.\n\nHow can you then be sure that you never `pip install` into the global env?\n\nAdd `require-virtualenv = True` to your `pip.conf` and you're good to go!\n\nWith that set, if you try to `pip install pandas` into the global env you'll get a message like this:\n\n\n```bash\n\n~ on \ue33d (us-east-1)  NO PYTHON VENV SET\n\u276f pip install pandas\nERROR: Could not find an activated virtualenv (required).\n\n\n```\n\n## End\n\n1. Disable your system `pip` to keep your base python safe\n2. Use `pipx` for tools you want available everywhere or don't have to need in a virtual environment!", "date": "2022-04-22", "edit_link": "https://github.com/edit/main/pages/blog/pipx.md", "today": "2022-06-25", "now": "2022-06-25 12:28:31.767799", "datetime": "2022-04-22 00:00:00+00:00", "slug": "pipx", "jinja": false, "output_html": "markout/pipx/index.html", "long_description": "pipx pinning formatting tools like  keeping virtual environments clean of things like  python utilities I want system wide but not in the global environment, like  pip Operating within that environment has burned me several times and now I have a str", "year": 2022}, {"cover": "/static/wish-list-with-fastapi.png", "title": "Wish-List-With-Fastapi", "tags": ["python", "blog"], "status": "published", "templateKey": "blog-post", "path": "pages/blog/wish-list-with-fastapi.md", "description": "Amazon has crossed the line with me just one too many times now so we are looking to drop them like every other Big Tech provider.... However, one key feature o", "content": "Amazon has crossed the line with me just one too many times now so we are looking to drop them like every other Big Tech provider....\n\nHowever, one key feature of Amazon that has been so useful for us is Lists... We can just maintain a list for each of us and then family members can login anytime and check it out... \nThis really alleviates any last minute gift idea stress right before a birthday or something.\n\nSo I need a nice gift list service but I don't want to be locked into one company (like a Target registry or something) and I'd like to host it myself\n\nThe internets had a few options but nothing looked/felt like I wanted to I decided to build my own.\n\n# The Frontend\n\n__I have no idea how to do front end so stay tuned__\n\n# The Backend\n\nFastAPI for the win on this one... I followed a few examples online and what I was able to build in just a few minutes is pretty impressive thanks to the design of FastAPI.\n\nSome key features are:\n1. Auto doc generation\n2. Required typing (which makes #1 possible)\n3. Built-in api testing in the browser\n4. Easy integration with sqlalchemy\n5. Development time so short you won't be done with your coffee before having something up and running!\n\n## Database\n\nStarting with a simple `database.py` we can create a sqlalchemy session with a base model with about 7 lines of code...\n\n```python\n\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy.orm import sessionmaker\n\n\nSQLALCHEMY_DATABASE_URL = \"sqlite:///wishes.sqlite3\"\nengine = create_engine(SQLALCHEMY_DATABASE_URL)\nSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\nBase = declarative_base()\n```\n\n## Model\n\nFor my wish list I needed just a simple table:\n\n|   id | person   | item         | link         | purchased   | purchased_by   | date_added          |\n|-----:|:---------|:-------------|:-------------|:------------|:---------------|:--------------------|\n|    1 | pypeaday | A sweet item | www.mystore.store | False        | dad| 2022-05-05 21:55:09 |\n|    2 | pypeaday   | A bitter item| www.bitterstore.com | True       |Mrs. pypeaday |  2022-05-06 06:55:54 |\n\n\nThe table is simple enough... A unique key, the person who the wish belongs to, the item (or wish), a link to the item, whether it's been purchased or not and by whom, and the date it was added.\n\nTo make this model with sqlalchemy we can make a `model.py` like so:\n\n```python\nfrom database import Base\nfrom sqlalchemy.schema import Column\nfrom sqlalchemy.types import Boolean, Integer, String, Text\n\n\nclass Wishes(Base):\n    __tablename__ = \"Wishes\"\n    id = Column(Integer, primary_key=True, index=True)\n    person = Column(String(20))\n    item = Column(Text())\n    link = Column(Text())\n    purchased = Column(Boolean())\n    purchased_by = Column(String(90))\n    date_added = Column(String(15))\n```\n\n## Schema\n\nOne of the best things about FastAPI is trivial integration with pydantic.\nWe can define a schema to ensure any data posted is not missing anything!\n\nMake a `schema.py` with the following:\n\n```python\nfrom pydantic import BaseModel\nimport time\nfrom typing import Optional\n\n\nclass wish_schema(BaseModel):\n\n    person: str\n    item: str\n    link: str\n    purchased: bool = False\n    purchased_by: Optional[str] = None\n    date_added: Optional[str] = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n\n    class Config:\n        orm_mode = True\n\n\nclass patch_schema(BaseModel):\n\n    purchased: bool\n    purchased_by: Optional[str] = None\n\n    class Config:\n        orm_mode = True\n\n```\n\nI have 2 schemas - one for a `wish` which you'll see down below is used to validate any `post` requests.\n\nTo simplify things for me I made another schema, `patch_schema` which I use for the route that updates the table (ie. marking an existing wish as purchased) \n\n## Session\n\nOne of the last things we need is a Session\n\nSo make a `session.py`...\n\n```python\nfrom database import SessionLocal, engine\nimport model\n\nmodel.Base.metadata.create_all(bind=engine)\n\n\ndef create_get_session():\n    try:\n        db = SessionLocal()\n        yield db\n    finally:\n        db.close()\n```\n\nOur routes will depend on this `create_get_session` function that will yield a `db` object through which we'll udpate our database\n\n# Ok just do it already!\n\nSo our `main.py` will have a few routes in it...\n\nWhat do we want to support?\n\n1. Getting all wishes\n2. Getting a specific wish\n3. Updating a specific wish\n4. Deleting a wish\n\nI think the script is fairly self explanatory but here's a few notes...\n\n1. We decorate each function with `@app.<method>` and define `response_model` as well as `status_code`\n2. The functions are defined with `async` (this was my first exposure to this so I can't go in depth on it yet)\n3. The functions all take a `db` which is from `session.py` and that `db` depends on the `create_get_session` function\n4. If the db is being updtes then we type the object used for the update with the appropriate schema (either `wish_schema` or `patch_schema`)\n\nFrom there we're in true python-land where you can basically guess the methods on `db` and you'd probably be right... (like `query`, `upddate`, `delete` etc.)\n\n\n```python\nfrom fastapi import FastAPI, Depends, HTTPException\nfrom sqlalchemy.orm import Session\nfrom typing import List\nfrom model import Wishes\nfrom schema import wish_schema, patch_schema\nfrom session import create_get_session\n\napp = FastAPI()\n\n\n@app.get(\"/\")\ndef read_root():\n    return {\"message\": \"server is up!\"}\n\n\n@app.get(\"/wishes\", response_model=List[wish_schema], status_code=200)\nasync def read_wishes(db: Session = Depends(create_get_session)):\n    wishes = db.query(Wishes).all()\n    return wishes\n\n\n@app.post(\"/wishes\", response_model=wish_schema, status_code=201)\nasync def add_wish(wish: wish_schema, db: Session = Depends(create_get_session)):\n    new_wish = Wishes(\n        person=wish.person,\n        item=wish.item,\n        link=wish.link,\n        purchased=wish.purchased,\n        purchased_by=wish.purchased_by,\n        date_added=wish.date_added,\n    )\n    db.add(new_wish)\n    db.commit()\n\n    return new_wish\n\n\n@app.get(\"/wishes/{id}\", response_model=wish_schema, status_code=200)\nasync def get_wish(id: int, db: Session = Depends(create_get_session)):\n    wish = db.query(Wishes).get(id)\n    return wish\n\n\n@app.patch(\"/wishes/{id}\", response_model=wish_schema, status_code=200)\nasync def update_wish(\n    id: int, patch: patch_schema, db: Session = Depends(create_get_session)\n):\n    db_wish = db.query(Wishes).get(id)\n    db_wish.purchased = patch.purchased\n    db_wish.purchased_by = patch.purchased_by\n    db.commit()\n    db.refresh(db_wish)\n\n    return db_wish\n\n\n@app.delete(\"/wishes/{id}\", status_code=200)\nasync def delete_wish(id: int, db: Session = Depends(create_get_session)):\n    db_wish = db.query(Wishes).get(id)\n    if not db_wish:\n        raise HTTPException(status_code=\"404\", detail=\"Wish id does not exist\")\n\n    db.delete(db_wish)\n    db.commit()\n\n    return None\n\n```\n\n# My Code\n\nYou can find my repo [here](https://github.com/nicpayne713/wish-lists).\n\nI'll plan to update and maintain for as long as I use it", "date": "2022-05-06", "edit_link": "https://github.com/edit/main/pages/blog/wish-list-with-fastapi.md", "today": "2022-06-25", "now": "2022-06-25 12:28:31.767808", "datetime": "2022-05-06 00:00:00+00:00", "slug": "wish-list-with-fastapi", "jinja": false, "output_html": "markout/wish-list-with-fastapi/index.html", "long_description": "Amazon has crossed the line with me just one too many times now so we are looking to drop them like every other Big Tech provider.... However, one key feature of Amazon that has been so useful for us is Lists... We can just maintain a list for each o", "year": 2022}, {"cover": "/static/self-hosted-media.png", "title": "self-hosted-media", "tags": ["python", "homelab"], "status": "published", "templateKey": "blog-post", "path": "pages/blog/self-hosted-media.md", "description": "Self-hosting 1 or several media servers is another common homelab use-case. you-get pip install you-get For example if I wanted to catch up on ancient Chinese m", "content": "Self-hosting 1 or several media servers is another common homelab use-case.\nGetting content for your media servers is up to you, but I'll show a few ways here to get content somewhat easily!\n\n__YouTube Disclaimer at Bottom__\n\n\n## you-get\n\n`you-get` is a nice cli for grabbing media content off the web. \n\n### Installation\n\n`pip install you-get` or use ad-hoc with `pipx run you-get <url>`\n\n\n### Usage\n\nFor example if I wanted to catch up on ancient Chinese military tactics I may go for `The Art of War` off the Internet Archive...\n\n```bash\nsandbox  \ud83c\udf31 main \ud83d\uddd1\ufe0f  \u00d73\ud83d\udee4\ufe0f  \u00d76via \ud83d\udc0d v3.8.11 (sandbox)  took 15s\n\u276f you-get https://archive.org/details/art_of_war_librivox -i\nSite:       Archive.org\nTitle:      The Art of War : Sun Tzu : Free Download, Borrow, and Streaming : Internet Archive\nType:       MP3 (audio/mpeg)\nSize:       3.87 MiB (4055167 Bytes)\n\n```\n\nthe `-i` is showing me the info of what would be downloaded without the flag (it's like a dry run)\n\n```bash\nsandbox  \ud83c\udf31 main \ud83d\uddd1\ufe0f  \u00d73\ud83d\udee4\ufe0f  \u00d76via \ud83d\udc0d v3.8.11 (sandbox)\n\u276f you-get https://archive.org/details/art_of_war_librivox\nSite:       Archive.org\nTitle:      The Art of War : Sun Tzu : Free Download, Borrow, and Streaming : Internet Archive\nType:       MP3 (audio/mpeg)\nSize:       3.87 MiB (4055167 Bytes)\n\nDownloading The Art of War : Sun Tzu : Free Download, Borrow, and Streaming : Internet Archi.mp3 ...\n 100% (  3.9/  3.9MB) \u251c\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2524[1/1]  917 kB/s\n\n```\n\nNow I can toss that mp3 onto my `booksonic` server and study for world domination while I do the dishes!\n\n\n## pytube\n\n`pytube` is a python implementation of a [youtube downloader ](##YouTube) that works at the command line or in python!\n\n### Installation\n\n[docs](https://pytube.io/en/latest/)\n\n`pip install pytube`\n\n\n### Usage\n\n`pytube` has a lot of functionality, but a quick one would be the `--list` so you can see what qualities are available\n\n```bash\nsandbox   main \ufe0f  \u00d73\ufe0f  \u00d77via  v3.8.11 (sandbox)  took 2m49s\n\u276f pytube https://www.youtube.com/watch\\?v\\=LDU_Txk06tM  --list\nLoading video...\n<Stream: itag=\"17\" mime_type=\"video/3gpp\" res=\"144p\" fps=\"8fps\" vcodec=\"mp4v.20.3\" acodec=\"mp4a.40.2\" progressive=\"True\" type=\"video\">\n<Stream: itag=\"18\" mime_type=\"video/mp4\" res=\"360p\" fps=\"30fps\" vcodec=\"avc1.42001E\" acodec=\"mp4a.40.2\" progressive=\"True\" type=\"video\">\n<Stream: itag=\"22\" mime_type=\"video/mp4\" res=\"720p\" fps=\"30fps\" vcodec=\"avc1.64001F\" acodec=\"mp4a.40.2\" progressive=\"True\" type=\"video\">\n<Stream: itag=\"313\" mime_type=\"video/webm\" res=\"2160p\" fps=\"30fps\" vcodec=\"vp9\" progressive=\"False\" type=\"video\">\n<Stream: itag=\"401\" mime_type=\"video/mp4\" res=\"2160p\" fps=\"30fps\" vcodec=\"av01.0.12M.08\" progressive=\"False\" type=\"video\">\n<Stream: itag=\"271\" mime_type=\"video/webm\" res=\"1440p\" fps=\"30fps\" vcodec=\"vp9\" progressive=\"False\" type=\"video\">\n<Stream: itag=\"400\" mime_type=\"video/mp4\" res=\"1440p\" fps=\"30fps\" vcodec=\"av01.0.12M.08\" progressive=\"False\" type=\"video\">\n<Stream: itag=\"137\" mime_type=\"video/mp4\" res=\"1080p\" fps=\"30fps\" vcodec=\"avc1.640028\" progressive=\"False\" type=\"video\">\n<Stream: itag=\"248\" mime_type=\"video/webm\" res=\"1080p\" fps=\"30fps\" vcodec=\"vp9\" progressive=\"False\" type=\"video\">\n<Stream: itag=\"399\" mime_type=\"video/mp4\" res=\"1080p\" fps=\"30fps\" vcodec=\"av01.0.08M.08\" progressive=\"False\" type=\"video\">\n<Stream: itag=\"136\" mime_type=\"video/mp4\" res=\"720p\" fps=\"30fps\" vcodec=\"avc1.4d401f\" progressive=\"False\" type=\"video\">\n<Stream: itag=\"247\" mime_type=\"video/webm\" res=\"720p\" fps=\"30fps\" vcodec=\"vp9\" progressive=\"False\" type=\"video\">\n<Stream: itag=\"398\" mime_type=\"video/mp4\" res=\"720p\" fps=\"30fps\" vcodec=\"av01.0.05M.08\" progressive=\"False\" type=\"video\">\n<Stream: itag=\"135\" mime_type=\"video/mp4\" res=\"480p\" fps=\"30fps\" vcodec=\"avc1.4d401f\" progressive=\"False\" type=\"video\">\n<Stream: itag=\"244\" mime_type=\"video/webm\" res=\"480p\" fps=\"30fps\" vcodec=\"vp9\" progressive=\"False\" type=\"video\">\n<Stream: itag=\"397\" mime_type=\"video/mp4\" res=\"480p\" fps=\"30fps\" vcodec=\"av01.0.04M.08\" progressive=\"False\" type=\"video\">\n<Stream: itag=\"134\" mime_type=\"video/mp4\" res=\"360p\" fps=\"30fps\" vcodec=\"avc1.4d401e\" progressive=\"False\" type=\"video\">\n<Stream: itag=\"243\" mime_type=\"video/webm\" res=\"360p\" fps=\"30fps\" vcodec=\"vp9\" progressive=\"False\" type=\"video\">\n<Stream: itag=\"396\" mime_type=\"video/mp4\" res=\"360p\" fps=\"30fps\" vcodec=\"av01.0.01M.08\" progressive=\"False\" type=\"video\">\n<Stream: itag=\"133\" mime_type=\"video/mp4\" res=\"240p\" fps=\"30fps\" vcodec=\"avc1.4d4015\" progressive=\"False\" type=\"video\">\n<Stream: itag=\"242\" mime_type=\"video/webm\" res=\"240p\" fps=\"30fps\" vcodec=\"vp9\" progressive=\"False\" type=\"video\">\n<Stream: itag=\"395\" mime_type=\"video/mp4\" res=\"240p\" fps=\"30fps\" vcodec=\"av01.0.00M.08\" progressive=\"False\" type=\"video\">\n<Stream: itag=\"160\" mime_type=\"video/mp4\" res=\"144p\" fps=\"30fps\" vcodec=\"avc1.4d400c\" progressive=\"False\" type=\"video\">\n<Stream: itag=\"278\" mime_type=\"video/webm\" res=\"144p\" fps=\"30fps\" vcodec=\"vp9\" progressive=\"False\" type=\"video\">\n<Stream: itag=\"394\" mime_type=\"video/mp4\" res=\"144p\" fps=\"30fps\" vcodec=\"av01.0.00M.08\" progressive=\"False\" type=\"video\">\n<Stream: itag=\"139\" mime_type=\"audio/mp4\" abr=\"48kbps\" acodec=\"mp4a.40.5\" progressive=\"False\" type=\"audio\">\n<Stream: itag=\"140\" mime_type=\"audio/mp4\" abr=\"128kbps\" acodec=\"mp4a.40.2\" progressive=\"False\" type=\"audio\">\n<Stream: itag=\"249\" mime_type=\"audio/webm\" abr=\"50kbps\" acodec=\"opus\" progressive=\"False\" type=\"audio\">\n<Stream: itag=\"250\" mime_type=\"audio/webm\" abr=\"70kbps\" acodec=\"opus\" progressive=\"False\" type=\"audio\">\n<Stream: itag=\"251\" mime_type=\"audio/webm\" abr=\"160kbps\" acodec=\"opus\" progressive=\"False\" type=\"audio\">\n\n```\n\n`pytube <url> --itag <>` will download the specific `itag` from the list.\n\nNotice that some `itags` are videos and others audio - so you can download just the music of a YT video.\n\n\n`pytube` also works in python...\n\n```python\nsandbox \u21aa main v3.8.11 ipython\n\u276f from pytube import YouTube\n\nsandbox \u21aa main v3.8.11 ipython\n\u276f [x for x in YouTube(\"https://www.youtube.com/watch?v=LDU_Txk06tM\").streams]\n\n[\n    <Stream: itag=\"17\" mime_type=\"video/3gpp\" res=\"144p\" fps=\"8fps\" vcodec=\"mp4v.20.3\" acodec=\"mp4a.40.2\" progressive=\"True\" type=\"video\">,\n    <Stream: itag=\"18\" mime_type=\"video/mp4\" res=\"360p\" fps=\"30fps\" vcodec=\"avc1.42001E\" acodec=\"mp4a.40.2\" progressive=\"True\" type=\"video\">,\n    <Stream: itag=\"22\" mime_type=\"video/mp4\" res=\"720p\" fps=\"30fps\" vcodec=\"avc1.64001F\" acodec=\"mp4a.40.2\" progressive=\"True\" type=\"video\">,\n    <Stream: itag=\"313\" mime_type=\"video/webm\" res=\"2160p\" fps=\"30fps\" vcodec=\"vp9\" progressive=\"False\" type=\"video\">,\n    <Stream: itag=\"401\" mime_type=\"video/mp4\" res=\"2160p\" fps=\"30fps\" vcodec=\"av01.0.12M.08\" progressive=\"False\" type=\"video\">,\n    <Stream: itag=\"271\" mime_type=\"video/webm\" res=\"1440p\" fps=\"30fps\" vcodec=\"vp9\" progressive=\"False\" type=\"video\">,\n    <Stream: itag=\"400\" mime_type=\"video/mp4\" res=\"1440p\" fps=\"30fps\" vcodec=\"av01.0.12M.08\" progressive=\"False\" type=\"video\">,\n    <Stream: itag=\"137\" mime_type=\"video/mp4\" res=\"1080p\" fps=\"30fps\" vcodec=\"avc1.640028\" progressive=\"False\" type=\"video\">,\n    <Stream: itag=\"248\" mime_type=\"video/webm\" res=\"1080p\" fps=\"30fps\" vcodec=\"vp9\" progressive=\"False\" type=\"video\">,\n    <Stream: itag=\"399\" mime_type=\"video/mp4\" res=\"1080p\" fps=\"30fps\" vcodec=\"av01.0.08M.08\" progressive=\"False\" type=\"video\">,\n    <Stream: itag=\"136\" mime_type=\"video/mp4\" res=\"720p\" fps=\"30fps\" vcodec=\"avc1.4d401f\" progressive=\"False\" type=\"video\">,\n    <Stream: itag=\"247\" mime_type=\"video/webm\" res=\"720p\" fps=\"30fps\" vcodec=\"vp9\" progressive=\"False\" type=\"video\">,\n    <Stream: itag=\"398\" mime_type=\"video/mp4\" res=\"720p\" fps=\"30fps\" vcodec=\"av01.0.05M.08\" progressive=\"False\" type=\"video\">,\n    <Stream: itag=\"135\" mime_type=\"video/mp4\" res=\"480p\" fps=\"30fps\" vcodec=\"avc1.4d401f\" progressive=\"False\" type=\"video\">,\n    <Stream: itag=\"244\" mime_type=\"video/webm\" res=\"480p\" fps=\"30fps\" vcodec=\"vp9\" progressive=\"False\" type=\"video\">,\n    <Stream: itag=\"397\" mime_type=\"video/mp4\" res=\"480p\" fps=\"30fps\" vcodec=\"av01.0.04M.08\" progressive=\"False\" type=\"video\">,\n    <Stream: itag=\"134\" mime_type=\"video/mp4\" res=\"360p\" fps=\"30fps\" vcodec=\"avc1.4d401e\" progressive=\"False\" type=\"video\">,\n    <Stream: itag=\"243\" mime_type=\"video/webm\" res=\"360p\" fps=\"30fps\" vcodec=\"vp9\" progressive=\"False\" type=\"video\">,\n    <Stream: itag=\"396\" mime_type=\"video/mp4\" res=\"360p\" fps=\"30fps\" vcodec=\"av01.0.01M.08\" progressive=\"False\" type=\"video\">,\n    <Stream: itag=\"133\" mime_type=\"video/mp4\" res=\"240p\" fps=\"30fps\" vcodec=\"avc1.4d4015\" progressive=\"False\" type=\"video\">,\n    <Stream: itag=\"242\" mime_type=\"video/webm\" res=\"240p\" fps=\"30fps\" vcodec=\"vp9\" progressive=\"False\" type=\"video\">,\n    <Stream: itag=\"395\" mime_type=\"video/mp4\" res=\"240p\" fps=\"30fps\" vcodec=\"av01.0.00M.08\" progressive=\"False\" type=\"video\">,\n    <Stream: itag=\"160\" mime_type=\"video/mp4\" res=\"144p\" fps=\"30fps\" vcodec=\"avc1.4d400c\" progressive=\"False\" type=\"video\">,\n    <Stream: itag=\"278\" mime_type=\"video/webm\" res=\"144p\" fps=\"30fps\" vcodec=\"vp9\" progressive=\"False\" type=\"video\">,\n    <Stream: itag=\"394\" mime_type=\"video/mp4\" res=\"144p\" fps=\"30fps\" vcodec=\"av01.0.00M.08\" progressive=\"False\" type=\"video\">,\n    <Stream: itag=\"139\" mime_type=\"audio/mp4\" abr=\"48kbps\" acodec=\"mp4a.40.5\" progressive=\"False\" type=\"audio\">,\n    <Stream: itag=\"140\" mime_type=\"audio/mp4\" abr=\"128kbps\" acodec=\"mp4a.40.2\" progressive=\"False\" type=\"audio\">,\n    <Stream: itag=\"249\" mime_type=\"audio/webm\" abr=\"50kbps\" acodec=\"opus\" progressive=\"False\" type=\"audio\">,\n    <Stream: itag=\"250\" mime_type=\"audio/webm\" abr=\"70kbps\" acodec=\"opus\" progressive=\"False\" type=\"audio\">,\n    <Stream: itag=\"251\" mime_type=\"audio/webm\" abr=\"160kbps\" acodec=\"opus\" progressive=\"False\" type=\"audio\">\n]\n\n\n```\n\n## YouTube Frontends\n\nThere's 2 really good options for self-hosting a YT front-end...\n\n[Tube Archivist](https://github.com/bbilly1/tubearchivist)\n\n[YouTubeDL-Material](https://github.com/Tzahi12345/YoutubeDL-Material)\n\nThey have their pros and cons.\nYou can also build yourself with the above utilities and use Plex or Jellyfin to serve up videos...\n\n__Your self-hosting journey is up to you!__\n\n\n## YouTube\n\nDownloading YouTube videos is a bit of a sore topic... Mainly you don't to hurt creators who rely on YT ad revenue for their livlihood.\n\nThen again, maybe you're a vigilante who knows that YT also monetizes videos for their _own_ gain and that the creators don't see that money either!\n\nThe solution is pretty easy and is 2-fold...\n\n1. Download YT videos\n2. Personally support the content creators you follow via paypall, patreon, or whatever else they might have set-up.... even a buck or two a month is more than they'd get from your ad revenue explicitly plus it all goes to them!", "date": "2022-03-24", "edit_link": "https://github.com/edit/main/pages/blog/self-hosted-media.md", "today": "2022-06-25", "now": "2022-06-25 12:28:31.767816", "datetime": "2022-03-24 00:00:00+00:00", "slug": "self-hosted-media", "jinja": false, "output_html": "markout/self-hosted-media/index.html", "long_description": "Self-hosting 1 or several media servers is another common homelab use-case. you-get pip install you-get For example if I wanted to catch up on ancient Chinese military tactics I may go for  the  Now I can toss that mp3 onto my  pytube pip install pyt", "year": 2022}, {"cover": "/static/tiddly-wiki.png", "title": "Tiddly-Wiki", "tags": ["meta"], "status": "published", "templateKey": "blog-post", "path": "pages/blog/tiddly-wiki.md", "description": "The tiddlywiki is just an  Taking notes in the tidlywiki is nice because it supports a format similar to Markdown although it is specific to tidlywiki. Here The", "content": "[Tiddly Wiki](https://tiddlywiki.com/) is a great note taking utility for organizing non-linear notes.\nI used it to replace my OneNote workflow and my only complaint is I don't have an easy way to access and edit my tiddlers (posts) if I'm not at home.\n\nThe tiddlywiki is just an `html` file with a ton of stuff above my head baked in. \nI have a barebones repo with some notes and a nice starter tiddly wiki init on [my github](https://github.com/nicpayne713/tiddlywiki-tutorial).\nUsage is pretty basic... Just grab the `notebook/template.htlm` and save it to anywhere convenient on your computer.\nI put mine on my NAS to have the security of backups since I don't keep my tidldlywiki in a git repo (I don't really want to look at the diff).\n\nTaking notes in the tidlywiki is nice because it supports a format similar to Markdown although it is specific to tidlywiki. \nTiddlers (each post in the wiki) can be tagged and linked together and it's really easy to send notes to someone by just exporting an html file and emailing it since it'll open up by default in a broswer with all the nice formatting already apart of it.\nI was using it primarily for taking notes for a small group I lead and sending those notes each week.\nThe group benefited from nicely formatted notes and I benefited from a centralized place to keep them all that Microsoft didn't own!\n\nHere's an example of the body of a tiddler with some tiddlywiki specific formatting:\n\n```\n! Static IPs on Linux\n\n//Ubuntu 20//\n\nSetting static IP on Ubuntu 20.04\n\n# Navigate to /etc/netplan\n# Open the yaml file (the name seems to be kind of random but it seems to starts with 00 or 05)\n# Change the file as below with your desired settings\n# Run `sudo netplan apply` to have changes reflected\n\n    ```yaml\n    network:\n      version: 2\n      ethernets:\n        enp0s4:\n          addresses: [192.168.1.{Static IP}/24]\n          gateway4: 192.168.1.1\n          nameservers:\n            addresses: [8.8.4.4, 8.8.8.8]\n    ```\n\n```\n\n\n![Alt text](/images/tiddlywiki-example.png \"A Tiddler\")\n\nThe `#` create a numbered list, `//` creates an italicized heading, and `!` creates headings similar to Markdown's `#`. The differences aren't too bad to keep in mind and what renders out is totally depenent on the tidlywiki itself. \nMy template has a nice nord feel to it, feel free to download from my github and try it out!\n\n> I have moved away from my tiddlywiki workflow in favor of sites like this since I can git commit markdown files and build with [markata](https://markata.dev/) pretty easily (credit [waylon walker](www.waylonwalker.com))\n\n> I still use tiddlywiki for tracking some todo items and questions --- I may have another solution in the future", "date": "2022-03-05", "edit_link": "https://github.com/edit/main/pages/blog/tiddly-wiki.md", "today": "2022-06-25", "now": "2022-06-25 12:28:31.767824", "datetime": "2022-03-05 00:00:00+00:00", "slug": "tiddly-wiki", "jinja": false, "output_html": "markout/tiddly-wiki/index.html", "long_description": "The tiddlywiki is just an  Taking notes in the tidlywiki is nice because it supports a format similar to Markdown although it is specific to tidlywiki. Here The  I have moved away from my tiddlywiki workflow in favor of sites like this since I can gi", "year": 2022}, {"cover": "/static/starship.png", "title": "Starship", "tags": ["linux"], "status": "published", "templateKey": "blog-post", "path": "pages/blog/starship.md", "description": "If you spend time in the terminal then you One of the first steps for me loving my terminal was a beautiful prompt... The default sh/bash/zsh prompts are... to ", "content": "If you spend time in the terminal then you'll want it to look somewhat pleasing to the eye.\nI used to ssh into servers with no customization, use `vi`  to edit a file or two, then get back to my regularly scheduled programming in VS C**e...\n\nOne of the first steps for me loving my terminal was a beautiful prompt... \n\n## Prompt\n\nThe default sh/bash/zsh prompts are... to put it lightly... garbage... I can't speak for other shells like fish simply because I do not use them but let me justify my trash talk.\n\nHere's the default `sh` prompt...\n\n![Alt Text](/images/sh-prompt.png)\n\nThen switching to `zsh` you get something marginally better (plus tab completion!)\n\n![Alt Text](/images/zsh-prompt.png)\n\nBut this still is super gross... there's nothing to indicate file types and no status information readily available (ie. `git status` etc.)\n\n## Oh-My-Zsh!\n\nNow there are several ways to make your prmompt nicer depending on your shell (terminal emulator plays a role too).\nNow I use `zsh` and there's a great tool out there [oh-my-zsh](https://ohmyz.sh/) that brings a crazy amount of customization to the terminal experience.\n\nI do not use `oh-my-zsh` for theming though and that's simply because of my other choices - I use `kitty` themes since I understood the implementation better.\nKitty themes though - do not give me a nice prompt.\n\nThe default prompt you get with `oh-my-zsh` themes isn't bad though (and you can pick from several default themes)...\n\n![Alt Text](/images/zsh-oh-my-zsh-prompt.png)\n\nNotice that you get some nice coloring and some default `git` status stuff, mainly the branch you are on.\nThere's plugins to show you more and that's all well and good, but again it's not my choice...\n\nIf I don't use this then what's my goto?\n\n## Starship\n\n[starship](https://starship.rs/) is a cross-shell prompt with nice default and super easy customizaton!\n\nTo get started click that link and follow the \"Getting Started\" button - it's incredibly fast to get up and running with sane defaults.\n\nThe default starship config is plenty nice but I got a little tired of emojis in my prompt and wanted to switch to icons instead...\n\nTo get started with your own customizaton you add a `starship.toml` file to `~/.config` \nMy starship config is found [here](https://github.com/nicpayne713/dotfiles/blob/main/starship/.config/starship.toml).\n\n>Note you need a font installed patched with nerdfonts - I use JetBrains Mono\n\nNow I have a beautiful prompt with relevant information that's a dream to look at!\n\n![Alt Text](/images/zsh-starship-prompt.png)\n\nI have configured my starship to show me relevant `git status` options (stashes, untracked files, etc etc.)\nI also have starship show me if I'm in a git repo, what branch I'm on, if I'm in a python project and if so what virtual environment is active.\nI do some work in AWS at work and so I have starship show me if my `aws cli` is configured to the right region for whichever project I'm in!\n\nThere's a billion more options and after a few minutes of play it becomes really easy and intuitive to customize colors, icons, etc.", "date": "2022-03-25", "edit_link": "https://github.com/edit/main/pages/blog/starship.md", "today": "2022-06-25", "now": "2022-06-25 12:28:31.767832", "datetime": "2022-03-25 00:00:00+00:00", "slug": "starship", "jinja": false, "output_html": "markout/starship/index.html", "long_description": "If you spend time in the terminal then you One of the first steps for me loving my terminal was a beautiful prompt... The default sh/bash/zsh prompts are... to put it lightly... garbage... I can Here Then switching to  But this still is super gross..", "year": 2022}, {"cover": "/static/ipython-prompt.png", "title": "Ipython-Prompt", "tags": ["python"], "status": "published", "templateKey": "blog-post", "path": "pages/blog/ipython-prompt.md", "description": "I have a  Now... I spend quite a bit of time in ipython every day and I got kind of sick of the vanilla experience and wanted something that more closely matche", "content": "I have a [post on starship](/starship) where I have some notes on how I use starship to make my zsh experience great with a sweet terminal prompt.\n\nNow... I spend quite a bit of time in ipython every day and I got kind of sick of the vanilla experience and wanted something that more closely matched my starship prompt.\n\nThere's more to customizing ipython I know for sure but here's 2 things I have going for me...\n\n1. I use [`rich`](https://pypi.org/project/rich/) authored by @[Will McGugan](https://twitter.com/willmcgugan) which makes much of my ipython experience great.\nI won't write about that here but you can find my `rich` config [here](https://github.com/nicpayne713/dotfiles/blob/main/ipython/.ipython/profile_default/startup/01-rich_init.py)\n\n2. I used `pygments` to customize the ipython prompt with my `ipython_config.py` and a startup script, next to my `rich` one, called `99-prompt.py`.\n\n> The scripts inside `~/.ipython/<profile>/startup` are executed in lexigraphical order, so it's nice to name things in the 10's to give room for adding scripts in between others down the line.\n\n## My prompt\n\nMy zsh prompt looks a little something like this:\n\n![Alt Text](/images/zsh-starship-prompt.png)\n\nAnd after my ipython customiztion it currently (subject to much change but this is as of my dotfiles commit #d22088f6be81a58b5f7dfb73b7a4088cbdd9fece on `main`).\n\n![Alt Text](/images/ipython-prompt.png)\n\nNow in ipython I have an indicator of my working directory, git branch, python environment, and a note that I'm in `ipython` and not `zsh`.\nI also configured my prompt to warn me if I'm _not_ in a git directory!\n\n![Alt Text](/images/ipython-prompt-no-git.png)\n\nAll in all the customization isn't too bad with just 2 specific files.\n\n## ipython_config.py\n\nThere's several use cases for `ipython_config.py` files in several areas on a pc - sometimes you want a common config across users, so you'd drop one in `/etc/ipython` and othertimes you have your own which is probably at `~/.ipython`\n\nMy ipython config mostly has colors defined on `pygment tokens` plus a few autorun commands and `pyflyby` (see my friend Waylon's post on pyflyby [here](https://waylonwalker.com/pyflyby/))\n\nI wanted to match my ipython somewhat to my tmux and vim color schemes, which I model after the vim-airline theme `night owl`.\n\nAfter picking some some colors and saving variables it's a matter of setting colors per token and then referencing those tokens in your version of `99-prompt.py`.\n\nYou can check out my `ipython_config.py` [here](https://github.com/nicpayne713/dotfiles/blob/home/ipython/.ipython/profile_default/ipython_config.py)\n\nFor example, I can set `Token.Name.Function` to black, and in `ipython` then a function's definition will appear in black text. I set mine to cyan to match my theme.\n\nFor the prompt colors just match the keyword in `c.TerminalInteractiveShell.highlighting_style_overrides` with what is referenced inside [99-prompt.py](https://github.com/nicpayne713/dotfiles/blob/home/ipython/.ipython/profile_default/startup/99-prompt.py)\n\nFor example, `Token.Prompt` is set to `bold grey` which gives me the bold chevron symbol you see in the above image that looks like my zsh prompt \n\nThen in `99-prompt.py` I have this set for the prompt:\n\n```python\nToken.Prompt \"\u276f \"\n```\n\n## 99-prompt.py\n\nYou don't need to name your script `99-prompt.py`, but I wanted to know that it was for my prompt and I wanted it executed last so it made sense.\n\nHere I have `MyPrompt` class with the prompt symbol defined as above and several other things... \n\n```python\nclass MyPrompt(Prompts):\n    def in_prompt_tokens(self, cli=None):\n        return [\n            (Token, \"\"),\n            (Token.OutPrompt, Path().absolute().stem),\n            (Token, \" \"),\n            (Token.Generic.Subheading, get_branch()[0]),\n            (Token, \" \"),\n            (Token.Generic.Heading, get_branch()[1]),\n            (Token, \" \"),\n            (Token.Name.Class, \"via \" + get_venv()),\n            (Token, \" \"),\n            (Token.Name.Entity, \"ipython\"),\n            (Token, \"\\n\"),\n            (\n                Token.Prompt\n                if self.shell.last_execution_succeeded\n                else Token.Generic.Error,\n                \"\u276f \",\n            ),\n        ]\n\n```\n\nNotice I have 2 custom functions here, `get_branch` and `get_venv` which grab some git info and python env info and return strings I can dump into my prompt as shown above.\n\nTo finish you drop `ip = get_ipython()` and `ip.prompts = MyPrompt(ip)` at the bottom of your prompt script and you should be in custom prompt city!\n\n## End\n\nThis is more or less notes for myself on how this works - drop by my [ipython config](https://github.com/nicpayne713/dotfiles/tree/home/ipython) in my dotfiles repo to see my full configs for ipython!", "date": "2022-04-02", "edit_link": "https://github.com/edit/main/pages/blog/ipython-prompt.md", "today": "2022-06-25", "now": "2022-06-25 12:28:31.767839", "datetime": "2022-04-02 00:00:00+00:00", "slug": "ipython-prompt", "jinja": false, "output_html": "markout/ipython-prompt/index.html", "long_description": "I have a  Now... I spend quite a bit of time in ipython every day and I got kind of sick of the vanilla experience and wanted something that more closely matched my starship prompt. There I use  I used  The scripts inside  My zsh prompt looks a littl", "year": 2022}, {"cover": "/static/traefik-01.png", "title": "Traefik-01", "tags": ["homelab"], "status": "published", "templateKey": "blog-post", "path": "pages/blog/traefik-01.md", "description": "If you don I like Traefik a lot because once I get some basic config up it In 2022 I A simple docker-compose file for traefik might look like this: I use Ansibl", "content": "# Traefik\n\nIf you don't know about [traefik](https://doc.traefik.io/traefik/) and you need a reverse-proxy then you might want to check it out.\nI used to use nginx for my reverse proxy but the config was over my head, and once it was working I was afraid to touch it.\nTraefik brings a lot to the table, my main uses are reverse proxy and ip whitelisting, but it's doing even more under the hood that I don't have a full-grasp of yet.\n\nI like Traefik a lot because once I get some basic config up it's incredibly easy to add services into my homelab whether they run on my primary server or not.\nThis will not be exhaustive but I'll outline my simple setup process of traefik and how I add services whether they are in docker or not.\n\n# Docker\n\nIn 2022 I'm still a docker fan-boy and I run my traefik instance in a docker container. \nThis isn't necessary but I love the portability since my homelab is very dynamic at the moment.\nAnd even if it wasn't I'd still want to keep traefik in docker because deployment and updating are just so flipping easy\n\nA simple docker-compose file for traefik might look like this:\n\n```yaml\nname: traefik\nimage: \"traefik:v2.4\"\nnetwork_mode: host\nvolumes:\n  - \"docker-data/traefik/traefik.toml:/etc/traefik/traefik.toml:ro\"\n  - \"docker-data/traefik/config.yml:/etc/traefik/config.yml:ro\"\n  - \"docker-data/traefik/letsencrypt:/letsencrypt:rw\"\n  - \"/var/run/docker.sock:/var/run/docker.sock:ro\"  # for auto-discovery\nenv: \"\"\nrestart_policy: unless-stopped\nmemory: \"1g\"\n```\n\n# Ansible deployment\n\n__I plan to have more on my homelab and Ansible on this site eventually...__\n\nI use Ansible to deploy most of my services at home, including traefik. My main homelab repo is [here](https://github.com/nicpayne713/ansible-nas) which is a fork of [Ansible NAS](https://github.com/davestephens/ansible-nas).\n\n> If you want my stuff then be sure to go to the `user/nic` branch on my fork\n\nYou can see the ansible stuff for traefik [here](https://github.com/davestephens/ansible-nas/tree/master/roles/traefik)\n\n# Config\n\nI use a `traefik.toml` as the main config and it looks something like this.\nWith ansible a lot of this is done through template variables but this is the general idea.\nThis config tells traefik what ports to listen and forward on, and gives the names to be referenced by docker labels (down below). \n\nTraefik also has a handy web ui that with this config you can find on port `8080`.\nThere is a `providers` section - which is one of the biggest selling points of traefik for me.\nI have a docker provider configured  and a static file. \n\nThe docker provider lets traefik auto-discover new services that I deploy and automatically handle the routing!\nThe static file lets me easily add non-dockerized service routing, or routing to dockerized services on another host (I think traefik has an easier way to do this automatically but I don't do it often enough to need that kind of automation).\nThen at the bottom is the SSL cert stuff. \nUsing Let's Encrypt is pretty easy and I use Cloudflare as my DNS provider\n\n```toml\n\n[entryPoints]\n[entryPoints.web]\naddress = \":80\"\n\n[entryPoints.web.http.redirections.entryPoint]\nto = \"websecure\"\n\n[entryPoints.websecure]\naddress = \":443\"\n\n[entryPoints.websecure.http.tls]\ncertResolver = \"letsencrypt\"\n\n[entryPoints.websecure.http.tls.domains]\nmain = \"example.com\"\nsans = [\n\"*.example.com\"\n]\n\n[entryPoints.traefik]\naddress = \":8080\"\n\n[providers]\nprovidersThrottleDuration = \"1s\"\n[providers.docker]\nexposedbydefault = false\n[providers.file]\nfilename = \"/etc/traefik/config.yml\"\n\n[api]\ninsecure = true\ndashboard = true\n\n[log]\nlevel = \"INFO\"\n\n[ping]\nterminatingStatusCode = 0\n\n[certificatesResolvers]\n[certificatesResolvers.letsencrypt]\n[certificatesResolvers.letsencrypt.acme]\nemail = \"my_email@example.com\"\nstorage = \"/letsencrypt/acme.json\"\ncaserver = \"https://acme-staging-v02.api.letsencrypt.org/directory\"  # le staging, not prod\n\n[certificatesResolvers.letsencrypt.acme.dnsChallenge]\nprovider = \"cloudflare\"\n```\n\n# Providers.file\n\nTo my knowledge there isn't much to configure on the docker provider side of things until you deploy a service.\nBut the provider config file should get a little screen time here.\n\nThe file defines a traefik http router for each service you define, in this case just `pihole`. \n\nHere I am adding my pihole instance which is not run inside docker but is inside a VM on another host.\nI want the `entryPoints` to be set to `websecure` which is configured above in the http redirects.\nI want some middlewares, `addprefix-pihole` and `default-headers`, which I'll explain below.\nI set letsencrypt as the cert certResolver.\nFinally I name the service `pihole`.\n\nThen in the `services` section I configure where pihole is located by just giving the internal IP for traefik to route to.\nFinally I define my middlewares. \nTo get to the pihole homepage you need to use the route `/admin` so I want that added automatically when I go to `pihole.example.com` so I come to `pihole.example.com/admin`.\nAnd I wanted to restrict access to just my internal network and my wireguard network - this is done with the `default-whitelist`. \nThe last thing is to configure a chain of middlewares that I called `secured` which is just easier for the docker labels later on.\n\nWith this config in play though, traefik will know about the route `pihole.example.com` and handle the ip whitelisting and load balancing for me.\n\n```yaml\nhttp:\n #region routers \n  routers:\n    pihole:\n      entryPoints:\n        - \"websecure\"\n      rule: \"Host(`pihole.example.com`)\"\n      middlewares:\n        # - default-headers\n        - addprefix-pihole\n        - default-whitelist\n      tls: \n        certResolver: letsencrypt\n      service: pihole\n  #region services\n  services:\n    pihole:\n      loadBalancer:\n        servers:\n          - url: \"http://192.168.1.3:80\"\n        passHostHeader: true\n  #endregion\n  middlewares:\n    addprefix-pihole:\n      addPrefix:\n        prefix: \"/admin\"\n    https-redirect:\n      redirectScheme:\n        scheme: https\n\n    default-headers:\n      headers:\n        frameDeny: true\n        sslRedirect: true\n        browserXssFilter: true\n        contentTypeNosniff: true\n        forceSTSHeader: true\n        stsIncludeSubdomains: true\n        stsPreload: true\n        stsSeconds: 15552000\n        customFrameOptionsValue: SAMEORIGIN\n\n    default-whitelist:\n      ipWhiteList:\n        sourceRange:\n        - \"10.6.0.0/24\"  # wg\n        - \"192.168.1.0/24\"  # lan\n        - \"172.17.0.0/16\"  # docker\n\n    secured:\n      chain:\n        middlewares:\n        - default-whitelist\n        - default-headers\n```\n\n\n# Docker labels\n\nNow the real magic is with Docker.\nHere is an example docker-compose file for spinning up a [jellyfin](https://jellyfin.org/) server that you want to expose to the world, or at least access at home with `jellyfin.example.com` instead of `http://192.168.1.N:8096`...\n\nI left some of the ansible variable stuff in here, but the main part to be concerned with is the `labels` section...\n\nWe define just a few labels to throw onto this docker container which let's traefik discover it automatically and apply any settings necessary (like my `ipWhiteList`).\n\n* `traefik.enable` is either True or False. \n* `traefik.http.router.jellyfin.rule` defines an http router called jellyfin and sets the url to `jellyfin.example.com` (if example.com was my `ansible_nas_domain`)\n* `traefik.http.routers.jellyfin.tls.certresolver` is set to letsencrypt since I use LE for my wildcard certs.\n* `traefik.http.routers.jellyfin.tls.domains[0].main` will just be `example.com` -> and this should remind you of the toml file above\n* `traefik.http.routers.jellyfin.tls.domains[0].sans` is set to `*.example.com`\n* `traefik.http.services.jellyfin.loadbalancer.server.port` is set to jellyfin's default http port of 8096, which tells traefik which port to point to for this service.\n\n```yaml\nname: jellyfin\nimage: linuxserver/jellyfin\nvolumes:\n  - \":/config:rw\"\n  - \":/movies:\"\n  - \":/music:\"\n  - \":/photos:\"\n  - \":/tv:\"\n  - \":/books:\"\n  - \":/audiobooks:\"\nports:\n  - \":8096\"\n  - \":8920\"\nenv:\n  TZ: \"\"\n  PUID: \"\"\n  PGID: \"\"\nrestart_policy: unless-stopped\nmemory: 1g\nlabels:\n  traefik.enable: \"\"\n  traefik.http.routers.jellyfin.rule: \"Host(`jellyfin.`)\"\n  traefik.http.routers.jellyfin.tls.certresolver: \"letsencrypt\"\n  traefik.http.routers.jellyfin.tls.domains[0].main: \"\"\n  traefik.http.routers.jellyfin.tls.domains[0].sans: \"*.\"\n  traefik.http.services.jellyfin.loadbalancer.server.port: \"8096\"\n```\n\n\nAnd just like that traefik will automagically find your jellyfin container and route `jellyfin.example.com` to it!", "date": "2022-03-06", "edit_link": "https://github.com/edit/main/pages/blog/traefik-01.md", "today": "2022-06-25", "now": "2022-06-25 12:28:31.767847", "datetime": "2022-03-06 00:00:00+00:00", "slug": "traefik-01", "jinja": false, "output_html": "markout/traefik-01/index.html", "long_description": "If you don I like Traefik a lot because once I get some basic config up it In 2022 I A simple docker-compose file for traefik might look like this: I use Ansible to deploy most of my services at home, including traefik. My main homelab repo is  If yo", "year": 2022}, {"cover": "/static/terraform-01.png", "title": "Terraform-01", "tags": ["homelab"], "status": "draft", "templateKey": "blog-post", "path": "pages/blog/terraform-01.md", "description": "I I Here We start with the ", "content": "I've started using [Terraform](https://www.terraform.io/) to manage Snowflake infrastructure at work.\n\nI'm still a noobie but I've got a workflow that I think makes sense...\n\nHere's the directory setup for a simple project with some databases, schemas, and tables to manage.\n\n```bash\nterraform-dir\n\u251c\u2500\u2500 .auto.tfvars\n\u251c\u2500\u2500 databases.tf\n\u251c\u2500\u2500 main.tf\n\u251c\u2500\u2500 schemas.tf\n\u251c\u2500\u2500 tables.tf\n\u251c\u2500\u2500 .terraform\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 providers\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 registry.terraform.io\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 chanzuckerberg\n\u2502\u00a0\u00a0             \u2514\u2500\u2500 snowflake\n\u2502\u00a0\u00a0                 \u251c\u2500\u2500 0.25.6\n\u2502\u00a0\u00a0                 \u2502\u00a0\u00a0 \u2514\u2500\u2500 linux_amd64\n\u2502\u00a0\u00a0                 \u2502\u00a0\u00a0     \u251c\u2500\u2500 LICENSE\n\u2502\u00a0\u00a0                 \u2502\u00a0\u00a0     \u251c\u2500\u2500 README.md\n\u2502\u00a0\u00a0                 \u2502\u00a0\u00a0     \u2514\u2500\u2500 terraform-provider-snowflake_v0.25.6\n\u2502\u00a0\u00a0                 \u2514\u2500\u2500 0.31.0\n\u2502\u00a0\u00a0                     \u2514\u2500\u2500 linux_amd64\n\u2502\u00a0\u00a0                         \u251c\u2500\u2500 CHANGELOG.md\n\u2502\u00a0\u00a0                         \u251c\u2500\u2500 LICENSE\n\u2502\u00a0\u00a0                         \u251c\u2500\u2500 README.md\n\u2502\u00a0\u00a0                         \u2514\u2500\u2500 terraform-provider-snowflake_v0.31.0\n\u251c\u2500\u2500 .terraform.lock.hcl\n\u251c\u2500\u2500 terraform.tfstate\n```\n\n\nWe start with the `main.tf` and I have in here [providers](https://www.terraform.io/language/providers) and [variables](https://www.terraform.io/language/values/variables)\n\n```terraform\nterraform {\n  required_providers {\n    snowflake = {\n      source  = \"chanzuckerberg/snowflake\"\n      version = \"0.31.0\"\n    }\n  }\n}\n\nprovider \"snowflake\" {\n  // required\n  username = \"SNOWFLAKE_USER\"\n  account  = \"URL\"\n\n  password = var.snowflake_password\n  role     = \"ROLE WITH DBA LIKE PERMISSIONS\"\n}\n\nvariable \"snowflake_password\" {\n  type      = string\n  sensitive = true\n}\n\nvariable \"public\" {\n  type = map(any)\n}\n\nvariable \"environment\" {\n  type = map(any)\n}\n\nvariable \"roles\" {\n  type = map(any)\n}\n\n```", "date": "2022-04-15", "edit_link": "https://github.com/edit/main/pages/blog/terraform-01.md", "today": "2022-06-25", "now": "2022-06-25 12:28:31.767855", "datetime": "2022-04-15 00:00:00+00:00", "slug": "terraform-01", "jinja": false, "output_html": "markout/terraform-01/index.html", "long_description": "I I Here We start with the ", "year": 2022}, {"cover": "/static/polybar-01.png", "title": "Polybar-01", "tags": ["linux"], "status": "published", "templateKey": "blog-post", "path": "pages/blog/polybar-01.md", "description": "I use it with i3-gaps on Ubuntu for work and it makes my day just that much better to have a clean and elegant bar with the things in it that I care about. The ", "content": "[polybar](https://github.com/polybar/polybar) is an awesome and super customizable status bar for your desktop environment.\n\nI use it with i3-gaps on Ubuntu for work and it makes my day just that much better to have a clean and elegant bar with the things in it that I care about.\n\nThe GitHub has all the instructions you'd need to install and get started with an example.\n\nI want to make some notes about how I use polybar and customize it.\n\n## Organization\n\nFirst of all, I recently moved my polybar config out of one config file into a modular structure that keeps my config files small and easiser to edit.\n\nYou can find my config [here](https://github.com/nicpayne713/dotfiles/tree/main/polybar)\n\nThe apps or services you put into polybar are called `modules`.\nI have moved all of my modules into their own config files and I source them with one centralized `include-modules.ini` config.\n\nThis separation also makes it easier for me to keep my home and work polybars as in sync as possible without duplicating a ton of config!\n\n```bash\n./polybar\n\u251c\u2500\u2500 colors.ini\n\u251c\u2500\u2500 config.ini\n\u251c\u2500\u2500 fonts.ini\n\u251c\u2500\u2500 home-modules.ini\n\u251c\u2500\u2500 include-modules.ini\n\u251c\u2500\u2500 launch.sh\n\u251c\u2500\u2500 modules\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 aws.ini\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 battery.ini\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 bluetooth.ini\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 cisco.ini\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 cpu.ini\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 date.ini\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 eth.ini\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 eth_work.ini\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 i3.ini\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 memory.ini\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 nm-editor.ini\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 powermenu.ini\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 pulseaudio-control.ini\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 pulseaudio.ini\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 rofi.ini\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 vpn.ini\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 wlan.ini\n\u2514\u2500\u2500 work-modules.ini\n\n1 directory, 24 files\n```\n\nTo break this down there are several configs to see:\n\n1. `colors.ini` is what you'd expect - a set of defined colors like `foreground`, `underline`, etc.\n2. `config.ini` is the general polybar config file where bars are defined. Currently in mine there is a `work` and `home` bar defined with the modules sourced in from the explicit config files.\n3. `fonts.ini ` is like `colors.ini` -> you put fonts here. I recommend using a font patched with NerdFont so you get fancy icons! (I use JetBrains Mono)\n4. `include-modules.ini` is where I list out all the config files in `modules/` so I can basically source just the `include-modules.ini` without explicitly sourcing every module's config in every polybar defintion.\n5. `launch.sh` is a simple shell script to launch the polybar! You'll see mine takes multiple monitors into consideration which I manage via environment variables setup in my `.zshenv` file that is different for my work and home setups.\n6. Finally there are `home-modules.ini` and `work-modules.ini` which is where, for each of my bars, I define which modules I want!\n\n## Config\n\nMy `config.ini` file has 2 bar definitions in it - here's my home one:\n\n```ini\ninclude-file = $DOTFILES/polybar/include-modules.ini\n\n[bar/home]\nmonitor = ${env:MONITOR:}\nwidth = 100%\nheight = 25\nradius = 8.0\nfixed-center = true\nbottom = false\n\nbackground = ${colors.background}\nforeground = ${colors.foreground}\n\ninclude-file = $DOTFILES/polybar/fonts.ini\ninclude-file = $DOTFILES/polybar/home-modules.ini\n```\n\nIt should be easy to follow - I bring in the `include-modules`, set a few colors for the bar like `background` and `foreground` which are sourced by the `colors.ini`, and finally bring in my fonts and home modules via their config files!\n\nIt's super easy to then change one or two things in the appropriate places rather than combing through one massive config. This also makes it easy for me to seperate my work and home setups.\n\n\n## Modules\n\nThere are several builtin modules, like `wlan` which gives your wifi status right there in polybar.\n\nYou can also make custom ones. \nA big-time custom one for me is an indicator of whether or not I have an active AWS token for working with the `aws` cli.\n\nThis is defined in` modules/aws.ini` and it looks like this:\n\n```ini\n[module/aws]\ninterval = 5.0\ntype = custom/script\nexec = has_aws_token\nclick-left = $HOME/.local/bin/auto_get_aws_token\nclick-right = rm -rf ~/.aws/credentials\n```\n\nEvery `5` seconds my `has_aws_token` script is ran.\nThat script looks like this:\n\n```bash\n#!/bin/bash\nsource auto_proxy\naws sts get-caller-identity &> /dev/null && echo \"%{T5}%{F#00ff00}\ue26b  %{F-}%{T-}\"  ||( echo \"%{T5}%{F#ff0000}\uf12a %{F-}%{T-}\" )\n```\n\nSee how the script echos out a colored icon to indicate the status of my token -> that icon is displayed in the polybar so I have real-time (5 second latency) status of whether or not I can do things in my AWS environment.\n\nIn the module I also configured actions for `click-left` and `click-right` which are as straight forward as could be.\n\n## My issues with i3\n\n\nThere's a few things to be considerate of if you use `i3` such as needing a workaround for a centered bar that __is not__ the full width of the monitor.\nPolybar can look really nice by not taking up the full width of the bar which you can configure in `config.ini` with these options:\n\n```ini\nwidth = 90%\noffset-x = 5%  # set to (100 - width) / 2\n```\n\nHowever due to an issue with polybar and i3 you need to also set `override-redirect = true`. \nBUT then you'll notice that the bar overlaps your i3 windows... ARGH! what do we do?\n\nQuick work around is to set `gaps top` in your i3 config if you use i3-gaps... if not? well, idk... use gaps... lol\n\nHowever this introduces another issue - which is then full screen windows will  have polybar sitting on top of them...\n\nThis isn't necessarily a deal breaker, but for me it's worth it to just have the bar go 100% width.\n\n\n## FIN\n\nThere's a tiny intro to polybar and how I organize my config files so things are easy to edit and manage!\nFeel free to grab mine and try it out!", "date": "2022-04-01", "edit_link": "https://github.com/edit/main/pages/blog/polybar-01.md", "today": "2022-06-25", "now": "2022-06-25 12:28:31.767864", "datetime": "2022-04-01 00:00:00+00:00", "slug": "polybar-01", "jinja": false, "output_html": "markout/polybar-01/index.html", "long_description": "I use it with i3-gaps on Ubuntu for work and it makes my day just that much better to have a clean and elegant bar with the things in it that I care about. The GitHub has all the instructions you I want to make some notes about how I use polybar and ", "year": 2022}, {"cover": "/static/wireguard.png", "title": "Wireguard", "tags": ["meta", "homelab"], "status": "published", "templateKey": "blog-post", "path": "pages/blog/wireguard.md", "description": "Virtual Private Networks are a big deal, and this shouldn Wireguard is an awesome peer-to-peer VPN tunnel that makes it really easy for me to get into my home n", "content": "## VPN\n\nVirtual Private Networks are a big deal, and this shouldn't be considered anything even close to a guide on using them.\nHere are just my notes and some setup for how I use [wireguard](https://www.wireguard.com/) at home.\n\n## Wireguard\n\nWireguard is an awesome peer-to-peer VPN tunnel that makes it really easy for me to get into my home network when I'm out and about.\nMy main reasons for this are 1. I don't trust public wi-fi and 2. I want to use pi-hole for ad blocking when I'm not at home\n\nWireguard can be configured as a \"peer-to-site\" VPN tunnel as well.\nMy vpn setup let's me jump to various machines on my network from anywhere!\n\nI use [pivpn](https://pivpn.io/) in a VM that's already running `pi-hole` to host my wireguard server.\nIt's super easy to setup just by following the instructions on the pivpn site.\n\nThe reason I like it is that I have a nice `cli` for managing wireguard configs.\n\n```bash\ndumbledore@pihole-vpn:~$ pivpn\n::: Control all PiVPN specific functions!\n:::\n::: Usage: pivpn <command> [option]\n:::\n::: Commands:\n:::    -a, add              Create a client conf profile\n:::    -c, clients          List any connected clients to the server\n:::    -d, debug            Start a debugging session if having trouble\n:::    -l, list             List all clients\n:::   -qr, qrcode           Show the qrcode of a client for use with the mobile app\n:::    -r, remove           Remove a client\n:::  -off, off              Disable a user\n:::   -on, on               Enable a user\n:::    -h, help             Show this help dialog\n:::    -u, uninstall        Uninstall pivpn from your system!\n:::   -up, update           Updates PiVPN Scripts\n:::   -bk, backup           Backup VPN configs and user profiles\n```\n\n\nWhen I'm ready to add a new client to my `wg` network, it's as easy as `pivpn add` and follow the instructions.\nThe easiest part here is that you'll be given a QR code in the terminal that you can just scan with the client (like a smart phone) and you'll have your wireguard config handled by the app (oh right, download the wireguard app) in no time!", "date": "2022-03-12", "edit_link": "https://github.com/edit/main/pages/blog/wireguard.md", "today": "2022-06-25", "now": "2022-06-25 12:28:31.767872", "datetime": "2022-03-12 00:00:00+00:00", "slug": "wireguard", "jinja": false, "output_html": "markout/wireguard/index.html", "long_description": "Virtual Private Networks are a big deal, and this shouldn Wireguard is an awesome peer-to-peer VPN tunnel that makes it really easy for me to get into my home network when I Wireguard can be configured as a  I use  The reason I like it is that I have", "year": 2022}, {"cover": "/static/git-worktrees-01.png", "title": "Git-Worktrees-01", "tags": ["git"], "status": "draft", "templateKey": "blog-post", "path": "pages/blog/git-worktrees-01.md", "description": "Hopefully if you write code you are using git, if not go learn the basics of  Assuming you are at least familiar with git then you probably work the same way I ", "content": "## Git \n\nHopefully if you write code you are using git, if not go learn the basics of `commit`, `pull`, `push`, and `pull request`/`merge request` like... right now.\n\nAssuming you are at least familiar with git then you probably work the same way I have since I've been using it.\n\n1. clone or initialize a repo\n2. checkout a branch, `git checkout -b my-feature`\n3. work on `my-feature` and when ready open a PR into `main`\n4. `git pull main` then `git checkout -b another-feature`\n5. etc...\n\n\nWhat if you need to switch between branches for some reason? Often I'm jumping into projects with my co-workers left and right, and I'll have changes that I'm either working on or exploring for them.\nWhen it's time to switch branches I think there's more elegant ways than this but I've always done this:\n\n1. `stash` the current changes\n2. checkout out the relevant branch \n3. helped out \n4. re-checkout my original branch\n5. `pop` the `stash`\n\nNow, that's not awful but I think `worktrees` will make this nicer for a few reasons!\n\n## Worktrees\n\nWorktrees are linked branches that have their own directories somewhere on your computer.\nTo checkout a branch you don't have to worry about stashing any changes, you just `cd` into the directory of that branch.\n\n> The branch can be literally anywhere - it doesn't have to be in the repo folder\n\n\n## Use Case\n\nI've seen ThePrimeagean argue for worktrees for several reasons, see a YT video [here](https://www.youtube.com/watch?v=2uEqYw-N8uE)\n\nI'm entirely in Python at the moment, or working with projects that dont' have that kind of requirement (ie. this website).\nMy reason for wanting worktrees is 3 fold.\n\n### Files that could have been gitignored but ain't\n\nI have a `.envrc` I put in every project, but it's not gitignored for reasons that aren't relevant right now...\nIf I switch branches I'll stash everything I have at the time, including my .envrc, but then if I forget to pop the stash and I move on and come back then my environment isn't active and I have to go find the stash, pop it, cd out, and then back in and honestly.... that sucks.\nWorktrees will let me have the .envrc in every branch, and if I checkout or switch to a new one, my personal branch is unaffected.\n\n\n### Symlinks\n\nIn my team's [Kedro](https://kedro.readthedocs.io/en/stable/01_introduction/01_introduction.html) workflow we keep a specific directory, the `conf` directory at a different spot than the Kedro team has in their templates (the why is outside the scope here).\nThe way I preserve every kedro utility for my own benefit is to symlink our `conf` to where the Kedro template expects it to be. \nBut then everytime I stash changes I lose that symlink so I either just don't have it for the time being or I recreate it which is a hassle\nWorktrees will let me have that present and persistent on all my branches at once.\n\n### Foo\n\nBecause why not!? This workflow feels future-proof, and if my toolset changes down the line then having this worktree centric workflow might be helpful and I'm just prepping for that possibility!", "date": "2022-03-11", "edit_link": "https://github.com/edit/main/pages/blog/git-worktrees-01.md", "today": "2022-06-25", "now": "2022-06-25 12:28:31.767879", "datetime": "2022-03-11 00:00:00+00:00", "slug": "git-worktrees-01", "jinja": false, "output_html": "markout/git-worktrees-01/index.html", "long_description": "Hopefully if you write code you are using git, if not go learn the basics of  Assuming you are at least familiar with git then you probably work the same way I have since I clone or initialize a repo checkout a branch,  work on  git pull main etc... ", "year": 2022}, {"cover": "/static/truenas-and-wireguard.png", "title": "Truenas-And-Wireguard", "tags": ["homelab"], "status": "published", "templateKey": "blog-post", "path": "pages/blog/truenas-and-wireguard.md", "description": "One of the most common use cases for self-hosting anything is a file share system. I don I finally have a need to put my TrueNAS box on my wireguard network in ", "content": "## NAS\n\nOne of the most common use cases for self-hosting anything is a file share system. \nI have been a fan of [TrueNAS](https://www.truenas.com/) for a while. \nI currently use TrueNAS Core at home, and plan to consider transitioning to TrueNAS Scale soon.\n\n__Blog post forthcoming on that!__ \n\n\n## VPN \n\nI don't write a ton about homelabbing yet but one of the first things to set up whether you have a massive homelab or a little raspberry pi would be a self-hosted VPN.\nI have notes on wireguard [here](\"/wireguard\").\n\nI finally have a need to put my TrueNAS box on my wireguard network in order to transfer files to other devices that are outside my LAN.\n\nThere is a handy tutorial on setting this up via the GUI [here](https://www.truenas.com/docs/core/network/wireguard/).\nThey walk you through setting up 2 tunables wireguard. One to enable the the connection and one to setup the network interface.\nNext you create a `Post Init` script which will check that the right directories exist and will copy the wireguard config that hasn't been made yet to the proper location and finally starts wireguard.\n\nThe above is just copy/paste from the tutorial but the final step, although not super tricky, isn't the same for everyone as it depends on your wireguard config and network setup.\n\nThe final step is for you create the relevant wireguard config (see my post but I just use `pivpn -a`) and send that config over to your TrueNAS box!\n\nFor me this final work flow looked like this:\n\n```bash\n\nssh user@vpn-server\n\npivpn -a\n\n<follow prompts>\n\nscp ~/configs/truenas.conf root@<truenas ip>:/root/wg0.conf\n\n```\n\n## Bug?\n\nThe script in the tutorial for starting the wireguard service is straight forward enough however my TrueNAS box didn't get the wireguard interface up and running on reboot.\n\nEasy enough solution:\n\n```bash\nssh root@<truenas ip>\n/usr/local/etc/rc.d/wireguard start\n```\n\nWe can check that the interface is now working with `ifconfig` and should see something like the following:\n\n```bash \nwg0: flags=8051<UP,POINTOPOINT,RUNNING,MULTICAST> metric 0 mtu 1420\n        options=80000<LINKSTATE>\n        inet x.x.x.x --> x.x.x.x netmask 0xffffff00\n        groups: tun\n        nd6 options=101<PERFORMNUD,NO_DAD>\n        Opened by PID 1325\n```\n\n\n## Gotcha!\n\nHere's another thing I had to navigate when setting this up.\n\nMy `pivpn` configuration sets the endpoint for my wireguard clients to `paynepride.com:<port forwarded to wireguard server>`\n\nWhat this means is that when I check the wireguard config for TrueNAS which is on my home network it resolves `paynepride.com` to the server I have running my reverse proxy.\nHowever! My reverse proxy is not responsible for my vpn traffic and so the traffic was just getting dropped - instead I needed to change the wireguard config just for my truenas box to piont to the local address of my vpn server.\nThis really threw me for a loop today but is just another reminder that if you have network problems it's probably DNS...\n\nGive her the 'ol reboot and now if I check `wg` I should see some traffic on my wireguard tunnel!\n\n![Alt text](/images/truenas-wireguard.png \"truenas-wireguard\")", "date": "2022-03-23", "edit_link": "https://github.com/edit/main/pages/blog/truenas-and-wireguard.md", "today": "2022-06-25", "now": "2022-06-25 12:28:31.767886", "datetime": "2022-03-23 00:00:00+00:00", "slug": "truenas-and-wireguard", "jinja": false, "output_html": "markout/truenas-and-wireguard/index.html", "long_description": "One of the most common use cases for self-hosting anything is a file share system. I don I finally have a need to put my TrueNAS box on my wireguard network in order to transfer files to other devices that are outside my LAN. There is a handy tutoria", "year": 2022}, {"cover": "/static/jellyfin-media-players.png", "title": "Jellyfin-Media-Players", "tags": ["homelab"], "status": "published", "templateKey": "blog-post", "path": "pages/blog/jellyfin-media-players.md", "description": "I use Jellyfin at home for serving up most of our media - movies and shows etc. My dream is to have a GPU capable of transcoding any and all of our media for sm", "content": "I use Jellyfin at home for serving up most of our media - movies and shows etc.\n\nMy dream is to have a GPU capable of transcoding any and all of our media for smooth playback on any device...\nNow, I thought I'd have that by now with my Nvidia Quadro P400 however I have issues left and right with 4k content.\n\nWhat can I do to still use Jellyfin but get smooth playback?\n\nTHe first answer is figure out why I suck with GPUs, but pausing that there's shorter solutions -> namely, use a media player that's compatabile with the encoded content!\n\n## VLC\n\nI'll keep this one short - VLC is great and if you don't need a netflix like experience, I'd recommend just using it to browse your network drives and play whatever you have\n\n## Jellyfin Web Player\n\nThis is the reason I'm writing this post... the web player is great but not everything is supported on all devices\n\n## Jellyfin MPV Shim\n\nThis cross-platform cast client is my answer now.\nYou can find the project [here](https://github.com/jellyfin/jellyfin-mpv-shim/blob/master/README.md#linux-installation)\n\nThe installation instrauctions are super straightforward for Windows, Mac OS, or Linux.\n\nI'm on Linux and so my install went like this:\n\n```bash\n\nsudo apt update\nsudo apt install mpv \n\npipx install jellyfin-mpv-shim\npipx inject jellyfin-mpv-shim pystray\n\n#profit\n\n```\n\nI used `pipx` to install the player as I prefer it for stand alone utilities over pip installing anything globally.\n\nAfer that I just start the player at the terminal with `jellyfin-mpv-shim`\nThen in the web browser I can cast my content to the player and bypass the web player (and thus solve much of my transcoding issues) trivially!", "date": "2022-04-17", "edit_link": "https://github.com/edit/main/pages/blog/jellyfin-media-players.md", "today": "2022-06-25", "now": "2022-06-25 12:28:31.767893", "datetime": "2022-04-17 00:00:00+00:00", "slug": "jellyfin-media-players", "jinja": false, "output_html": "markout/jellyfin-media-players/index.html", "long_description": "I use Jellyfin at home for serving up most of our media - movies and shows etc. My dream is to have a GPU capable of transcoding any and all of our media for smooth playback on any device... What can I do to still use Jellyfin but get smooth playback", "year": 2022}, {"cover": "", "title": "kvm-network-interface-via-nat-ubuntu-20", "tags": ["homelab", "linux"], "status": "published", "templateKey": "blog-post", "path": "pages/blog/kvm-network-interface-via-nat-ubuntu-20.md", "description": "I have started using VMs more and more in my development workflow and it First thing There was a  These commands got me up and running without even turning the ", "content": "I have started using VMs more and more in my development workflow and it's\nimpossible to work in a VM without an internet connection for me most of the\ntime. Setting up the KVM networking is kind of confusing to me and I've done it\ntwo different ways. Here is how I set it up on my home desktop using NAT.\n\n# Credit\n\nFirst thing's first: [credit to this post](https://computingforgeeks.com/managing-kvm-network-interfaces-in-linux/)\n\n# Commands\n\nThere was a `default` network already made by virt-manager but my VM couldn't connect over it at all...\n\nThese commands got me up and running without even turning the VM off\n\n> I went full on `sudo -i` for this just to make it easier - be careful\n\n## Dump an existint network config\n\n```bash\n# as root\n\nvirsh net-dumpxml default > br1.xml\n\nvim br1.xml\n\n```\n\n## Edit it\n\nI was unsure what the ip range should be so I just stuck with the original blog. \nThe `default` network had the CIDR block defined as `192.168.122.0/24` which is different from my home network so I guess it's fine?\n\n```xml\n<network>\n  <name>br1</name>\n  <forward mode='nat'>\n    <nat>\n      <port start='1024' end='65535'/>\n    </nat>\n  </forward>\n  <bridge name='br1' stp='on' delay='0'/>\n  <ip address='192.168.10.1' netmask='255.255.255.0'>\n    <dhcp>\n      <range start='192.168.10.10' end='192.168.10.100'/>\n    </dhcp>\n  </ip>\n</network>\n```\n\n## Define a network\n\n```bash\nvirsh net-define br1.xml\nvirsh net-autostart br1\n```\n\nThen to check...\n\n```bash\nvirsh net-list --all\n\n Name      State    Autostart   Persistent\n--------------------------------------------\n br1       active   yes         yes\n default   active   yes         yes\n```\n\n## UUID\n\n`virsh net-uuid br1`\n\n\n## Magic\n\n`virsh attach-interface --domain <NAME OF VM> --type bridge --source br1 --model virtio --config --live`\n\nMy VM, `ubuntu20.04` was running and immediately connected to the newly attached device!\n\n\n# Credit again\n\nVisit the original post for more details - this serves more as as a quicker set of notes for future me", "date": "2022-06-25", "edit_link": "https://github.com/edit/main/pages/blog/kvm-network-interface-via-nat-ubuntu-20.md", "today": "2022-06-25", "now": "2022-06-25 12:28:31.767900", "datetime": "2022-06-25 00:00:00+00:00", "slug": "kvm-network-interface-via-nat-ubuntu-20", "jinja": false, "output_html": "markout/kvm-network-interface-via-nat-ubuntu-20/index.html", "long_description": "I have started using VMs more and more in my development workflow and it First thing There was a  These commands got me up and running without even turning the VM off I went full on  I was unsure what the ip range should be so I just stuck with the o", "year": 2022}, {"cover": "/static/caps-lock-polybar.png", "title": "Caps-Lock-Polybar", "tags": ["linux"], "status": "draft", "templateKey": "blog-post", "path": "pages/blog/caps-lock-polybar.md", "description": "My moonlander is great, and I just recently added CAPS LOCK back to my keymapping but I Because of this though I keep turning CAPS LOCK on without meaning too b", "content": "My moonlander is great, and I just recently added CAPS LOCK back to my keymapping but I've moved it...\nAt present it is where the ESC kep usually is however I'm trying to match my general moonlander usage with a keymap that fits on a planck.\n\nBecause of this though I keep turning CAPS LOCK on without meaning too by thinking I'm hitting ESC, so while I learn the keymapping I need a way to know if I screwed up...\n\nEnter  \ndotfiles  \ue725 work  \ue79b \u00d74 \uf6c1 \u00d712 \uf21b \u00d74 via \ue235  v3.8.11(dotfiles) on \ue33d (us-east-1) proxy\n\u276f xset q | grep Caps\n    00: Caps Lock:   on     01: Num Lock:    off    02: Scroll Lock: off\n\ndotfiles  \ue725 work  \ue79b \u00d74 \uf6c1 \u00d712 \uf21b \u00d74 via \ue235  v3.8.11(dotfiles) on \ue33d (us-east-1) proxy\n\u276f xset q | grep Caps\n    00: Caps Lock:   off    01: Num Lock:    off    02: Scroll Lock: off", "date": "2022-04-15", "edit_link": "https://github.com/edit/main/pages/blog/caps-lock-polybar.md", "today": "2022-06-25", "now": "2022-06-25 12:28:31.767907", "datetime": "2022-04-15 00:00:00+00:00", "slug": "caps-lock-polybar", "jinja": false, "output_html": "markout/caps-lock-polybar/index.html", "long_description": "My moonlander is great, and I just recently added CAPS LOCK back to my keymapping but I Because of this though I keep turning CAPS LOCK on without meaning too by thinking I Enter dotfiles  \ue725 work  \ue79b \u00d74 \uf6c1 \u00d712 \uf21b \u00d74 via \ue235  v3.8.11(dotfiles) on \ue33d (us-eas", "year": 2022}, {"cover": "/static/And-vs-ampersand.png", "title": "And-vs-&", "tags": ["python"], "status": "published", "templateKey": "blog-post", "path": "pages/blog/and-vs-&.md", "description": "I often struggle to remember the correct way to do  I remember learning long long ago that  Python  However we can use  Here If we compare  Let bool(my_list) So", "content": "I often struggle to remember the correct way to do `and` type comparisons when working in pandas.\n\nI remember learning long long ago that `and` and `&` are different, the former being lazy boolean evaluation whereas the latter is a bitwise operation.\n\n__I learned a lot from [this SO post](https://stackoverflow.com/questions/22646463/and-boolean-vs-bitwise-why-difference-in-behavior-with-lists-vs-nump)__\n\n## Lists \n\nPython `list` objects can contain unlike elements - ie. `[True, 'foo', 1, '1', [1,2,3]]` is a valid list with booleans, strings, integers, and another list.\nBecause of this, we can't use `&` to compare two lists since they can't be combined in a consistent and meaningful way.\n\nHowever we can use `and` since it doesn't do bitwise operations, it just evaluates the boolean value of the list (basically if it's non-empty then `bool(my_list)` evaluates to `True`)\n\nHere's an example:\n\n```python\nsandbox NO VCS  via 3.8.11(sandbox) ipython\n\u276f my_list = [1, \"2\", \"foo\", [True], False]\n\nsandbox NO VCS  via 3.8.11(sandbox) ipython\n\u276f bool(my_list)\nTrue\n```\n\n\nIf we compare `my_list` with `another_list` using `and` then the comparision will go:\n\n```\nif bool(my_list):\n    if bool(another_list):\n       <operation> \n    else:\n       break\n```\n\nLet's see another example:\n\n```python\nsandbox NO VCS  via 3.8.11(sandbox) ipython\n\u276f another_list = [False, False]\n\nsandbox NO VCS  via 3.8.11(sandbox) ipython\n\u276f my_list and another_list\n[False, False]\n```\n\n\n`bool(my_list)` evaluated to `True`, and `bool(another_list)` _also_ evaluated to `True` even though it's full of `False` values because the object is non-empty.\n\n\n```python\nsandbox NO VCS  via 3.8.11(sandbox) ipython\n\u276f if my_list and another_list:\n...:     print(\"foo\")\nfoo\n```\n\nSo using `and` in this case results in a `True` conditional, so the `print` statement is executed.\n\nFeels kind of counter-intuitive at first glance, to me anyways...\n\nHowever, we can't use `&` because there isn't a meaningful to do bitwise operations over these two lists:\n\n```python\nsandbox NO VCS  via 3.8.11(sandbox) ipython\n\u276f my_list & another_list\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 <ipython-input-19-a2a16cebb3da>:1 in <cell line: 1>                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\nTypeError: unsupported operand type(s) for &: 'list' and 'list'\n\n```\n\n## Numpy\n\n`numpy` arrays are special and they have a lot of fancy vectorization utilities built-in which make them great and fast for mathematical operations but now our logical comparisons need to be handled with a different kind of care.\n\nFirst thing though - without some trickery they do not hold mixed data types like a `list` does (necessary, I think, for the vectorized optimization that numpy is built on top of)\n\nWith that out of the way here's the main thing for this post, we can't just evaluate the `bool` of an array - numpy says no no no.\n\n```python\n\nsandbox NO VCS  via 3.8.11(sandbox) ipython\n\u276f arr = np.array([\"1\", 2, True, False])\n\nsandbox NO VCS  via 3.8.11(sandbox) ipython\n\u276f arr\narray(['1', '2', 'True', 'False'], dtype='<U21')\n\nsandbox NO VCS  via 3.8.11(sandbox) ipython\n\u276f bool(arr)\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 <ipython-input-25-4e8c5dd85b93>:1 in <cell line: 1>                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n\n```\n\n> This means that using `and` with `numpy` arrays doesn't really make sense because we probably care about the truth value of each element (bitwise), not the truth value of the array.\n\nNotice that when I print `arr` all the elements are a string - and the `dtype` is `<U21` for all elements.\n\nThis is not how I instantiated the array so be aware of that behavior with numpy.\n\n> `<U21` is a dtype expressing the values are 'Little Endian', Unicode, 12 characters. See [here](https://numpy.org/doc/stable/reference/arrays.dtypes.html#specifying-and-constructing-data-types) for docs for docs\n\nSo for logical comparisions we should look at the error message then...\nOur handy error message says to try `any` or `all`\n\nBecause the datatypes in this example are basically strings, using `arr.any()` will result in an error that I do not fully understand, but `any(arr)` and `all(arr)` work...\n\n```python\nsandbox NO VCS  via 3.8.11(sandbox) ipython\n\u276f if arr.any():\n...:     print(\"foo\")\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 <ipython-input-48-25ecac52db96>:1 in <cell line: 1>                                              \u2502\n\u2502 /home/u_paynen3/personal/sandbox/.venv/sandbox/lib/python3.8/site-packages/numpy/core/_methods.p \u2502\n\u2502 y:57 in _any                                                                                     \u2502\n\u2502                                                                                                  \u2502\n\u2502    54 def _any(a, axis=None, dtype=None, out=None, keepdims=False, *, where=True):               \u2502\n\u2502    55 \u2502   # Parsing keyword arguments is currently fairly slow, so avoid it for now              \u2502\n\u2502    56 \u2502   if where is True:                                                                      \u2502\n\u2502 \u2771  57 \u2502   \u2502   return umr_any(a, axis, dtype, out, keepdims)                                      \u2502\n\u2502    58 \u2502   return umr_any(a, axis, dtype, out, keepdims, where=where)                             \u2502\n\u2502    59                                                                                            \u2502\n\u2502    60 def _all(a, axis=None, dtype=None, out=None, keepdims=False, *, where=True):               \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\nUFuncTypeError: ufunc 'logical_or' did not contain a loop with signature matching types (None, <class 'numpy.dtype[str_]'>) -> None\n\nsandbox NO VCS  via 3.8.11(sandbox) ipython\n\n\u276f if all(arr):\n...:     print(\"foo\")\nfoo\n\nsandbox NO VCS  via 3.8.11(sandbox) ipython\n\u276f if any(arr):\n...:     print(\"foo\")\nfoo\n```\n\nLet's change the example to just use integers and see what happens:\n\n```python\nsandbox NO VCS  via 3.8.11(sandbox) ipython\n\u276f arr2 = np.array([1, True, False])\n\nsandbox NO VCS  via 3.8.11(sandbox) ipython\n\u276f arr2\narray([1, 1, 0])\n\nsandbox NO VCS  via 3.8.11(sandbox) ipython\n\u276f if arr2.any():\n...:     print(\"foo\")\nfoo\n\nsandbox NO VCS  via 3.8.11(sandbox) ipython\n\u276f if arr2.all():\n...:     print(\"foo\")\n\n```\n\nAh, now some sanity...\nFirst, the booleans are stored as integers, which based on this discussion makes sense.\nNext we check if `any` values (this is a bitwise operation) are `True`, which we see they are so the conditional evaluates to `True`.\nHowver, if we check that `all` values are `True` we see they aren't, the last value is `False` or `0` so the conditional fails.\n\nThis is a different way to evaluate logical conditions than with lists and it's because of the special nature of numpy arrays that allows them to be compared bitwise but on the flip side, there isn't a meaningful way to evaluate the `truth value` of an array.\n\n\n## Pandas\n\nNow for `pandas`, which under the hood is a lot of `numpy` but not fully. \n`pandas.Series` objects can hold mixed data types like lists, however to logically evaluate truth values we have to treat them like numpy arrays.\n\n```python\n\nsandbox NO VCS  via 3.8.11(sandbox) ipython\n\u276f s = pd.Series([1, \"foo\", True, False])\n\nsandbox NO VCS  via 3.8.11(sandbox) ipython\n\u276f s\n\n0        1\n1      foo\n2     True\n3    False\ndtype: object\n\nsandbox NO VCS  via 3.8.11(sandbox) ipython\n\u276f bool(s)\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 <ipython-input-60-68e48e81da14>:1 in <cell line: 1>                                              \u2502\n\u2502 /home/u_paynen3/personal/sandbox/.venv/sandbox/lib/python3.8/site-packages/pandas/core/generic.p \u2502\n\u2502 y:1527 in __nonzero__                                                                            \u2502\n\u2502                                                                                                  \u2502\n\u2502    1524 \u2502                                                                                        \u2502\n\u2502    1525 \u2502   @final                                                                               \u2502\n\u2502    1526 \u2502   def __nonzero__(self):                                                               \u2502\n\u2502 \u2771  1527 \u2502   \u2502   raise ValueError(                                                                \u2502\n\u2502    1528 \u2502   \u2502   \u2502   f\"The truth value of a {type(self).__name__} is ambiguous. \"                 \u2502\n\u2502    1529 \u2502   \u2502   \u2502   \"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"                       \u2502\n\u2502    1530 \u2502   \u2502   )                                                                                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\nValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n\n```\n\nJust like with numpy, we can't evaluate the truth value of the series in a meaningful way, but bitwise operations make perfect sense...\n\n```python\n\n\u276f if s.any():\n...:     print(\"foo\")\nfoo\n\nsandbox NO VCS  via 3.8.11(sandbox) ipython\n\u276f if s.all():\n...:     print(\"foo\")\n\n```\n\n__I thought this was about `and` and `&`...__\n\nRight, so recall that `and` is a lazy boolean evaluation (ie. it evaluates the 'truth value' an object) whereas `&` does bitwise comparison.\n\nWhat we see then with `pandas` and `numpy` is that if we want to do logical comparisons, we need to do them bitwise, ie. use `&`.\n\nKeep in mind though that the data types make a big deal - we can't use `&` with strings  because the bitwise operation isn't supported, for strings we need to use the boolean evaluation.\n\n\n## The Original Point\n\nMy main use case for this is finding elements in a dataframe/series based on 2 or more columns aligning row values...\n\n\nSay I have a dataframe like this:\n```python\n\nsandbox NO VCS  via 3.8.11(sandbox) ipython\n\u276f df\n\n   s s2   s3\n0  1  0  foo\n1  1  a  bar\n2  1  b  baz\n3  2  a  fee\n4  2  0   fi\n```\n\nExample use case is I want to get the values in `s3` where `s` is 1 and `s2` is 'a'. ie. I'm just after `bar` for now...\n\nUp until now I've always just tried `df.s3[(df.s == 1) and (df.s2 == \"a\")]` the first time and every single time I've gotten this error that I just haven't ever fully understood:\n\n```python\nValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n```\n\nBut after this deep dive I think I've grasped that `and` doesn't actually do what I want here, and in order to do the bitwise comparision I need to use `&`\n\n```python\nsandbox NO VCS  via 3.8.11(sandbox) ipython\n\u276f df.s3[(df.s == 1) & (df.s2 == \"a\")]\n\n1    bar\nName: s3, dtype: object\n```\n\n## End\n\nHopefully this set of ramblings brings some clarity to `and` and `&` and you can Google one less error in the future in your logical comparison workflows \ud83d\ude04", "date": "2022-04-06", "edit_link": "https://github.com/edit/main/pages/blog/and-vs-&.md", "today": "2022-06-25", "now": "2022-06-25 12:28:31.767914", "datetime": "2022-04-06 00:00:00+00:00", "slug": "and-vs-&", "jinja": false, "output_html": "markout/and-vs-&/index.html", "long_description": "I often struggle to remember the correct way to do  I remember learning long long ago that  Python  However we can use  Here If we compare  Let bool(my_list) So using  Feels kind of counter-intuitive at first glance, to me anyways... However, we can ", "year": 2022}, {"cover": "/static/home-server-refactor.png", "title": "Home-Server-Refactor", "tags": ["blog"], "status": "draft", "templateKey": "blog-post", "path": "pages/blog/home-server-refactor.md", "description": "My current homelab setup is not great but it works... I boot off an SD card and have 1 SSD and 5 HDDs configured as a JBOD array using a Dell H700 SAS controlle", "content": "My current homelab setup is not great but it works...\n\n# Proxmox on PowerEdge R610\n\nI boot off an SD card and have 1 SSD and 5 HDDs configured as a JBOD array using a Dell H700 SAS controller.\nI cannot boot from a disk using this controller and I can't get the firmware configured in a way to allow it.\nSo I have 1 SSD as a ZFS array that I've been putting my VM images on, and the 5 HDDs are passed through to a TrueNAS VM where I handle all the ZFS stuff there... kind of meta because I then attached those drives to Proxmox as a CIFS share.\n\n# TrueNAS on dedicated box\n\nI have an on-prem backup that is just an old desktop running TrueNAS\nI regularly backup the 5 disk RAIDZ2 array from my Proxmox host (managed by a TrueNAS VM) to this backup box\n\nCurrently there is nothing else running on this machine since it's my \"backup\"\n\n# Jellyfin\n\nI was HWA for Jellyfin, but hardware passthrough on the R610 is finicky or broken so Jellyfin is running on an Ubuntu host.\n\nI could put UBuntu on the R610 and give up \"true virtualization\". Then I'd manage the SMB share myself.\nIf I do that then I would get rid of \"users\" I think, ie. basically forgo least-priviledges since I'm not sure how hard that is to manage.\n\nOn the other hand, direct access to the smb config might make it easier?\n\nI have the media array on Jellyfin box setup as NFS which was really easy with ZFS... I think SMB would be just as easy.\n\n# Plan of attack...\n\n1. Move all vm disks to individual datasets on the NAS \n2. Backup docker data... not sure how well this will work, maybe just start over?\n3. Clean up Ansible playbooks on the user side of things - stick with neville vs just using my own name?\n4. Install Ubuntu 20 or 22 on a 2.5\" drive that I'll toss in this SSD enclosure (or a usb thumb stick?)\n5. Re-deploy everything with ansible-playbook and configure...\n\n## Configuration...\n\n0. THE FREAKING NAS -> just import zfs array and configure SMB?\n\n1.~~ Nextcloud users and connections.. might be able to just copy the data folder? not sure about the database... try spinning it up in the sandbox vm and see if stuff is there ~~\n2.~~ *arr suite, media profiles and connections to transmission... nothing major~~\n3. transmission - should be deploy and go\n4. ombi and jackett should also just work after some config again\n5. ~~traefik should just work~~\n6. ~~try to bring up pi-hole from the vm that's already running~~\n7. ~~heimdall will hopefully just be copying the data folder from the existind docker one'~~\n8.~~ booksonic can be reconfigured easily~~\n9. ~~portainer... hopefully just copying data folder over?~~\n10. ~~littlelink~~, small-group-notes, and blog (at home) will need manually re-deployed once Ubuntu is installed bare-metal\n\n## BIG BIG BIG TODOS\n1. Sanoid/syncoid! Get snapshots going and backups configured with on prem TrueNAS\n2. Wireguard setup on DA.\n3. network share on printer for paperless\n~~4. update peperless in ansible-nas~~\n4. ~~Just deploy paperless manually... monitor/manage with portainer~~\n5. booksonic not seeing audiobooks/podcasts\n\n1. need a smb user to map nas/documents to the printer for paperless\n3. wireguard setup now on kps phone, desktop, server (and backup truenas?), and dad's pi\n4. ~~verify lan services work~~\n5. ~~Tdar so Jellyfin can work better~~\n\nSnapshot business might be cause of all the docker containers and docker using\nZFS backend... take everything down and try removing\n\n1. file browser - currently I just one-clicked in portainer, I want to make a stack with my own config file which I'll rip from techno tip and then add my traefik lables too\n\nForget filebrowser - going to just use Nextcloud for how it's supposed to be used.\n3. Need to organize those files in nextcloud\n## CHECK THIS ->  ran as 'cp' utility in tmux window, no progress bars or anything. it's in ansible-nas session -> window 3\nOlivet bible stuff going to /tmp/olivet/ -> will move this to nextcloud, ideally by the app via appimage so that the db updates and I don't have to run that occ script\nI wnat to organize \"home\" still in nextcloud \n\nsetup Sanoid\n\n\nclean up bitwarden \nlearn nextcloud sharing -> maybe just give a link to grandma?\nrest of todos -> document db and sanoid + zfs.rent\n\nCheck on mom's will\ndo media thing for church - split vocals on mp3/4\n\npermission-data playbook changes everything to ansible-nas:ansible-nas but then samba task will re-permission some stuff to root:users... this looks fine\nI had to add `group` to the samba config in my playbook to get user auth to work with samba\nThis isn't fully working... it works from cli but my python process can't write to a folder in dump after 777.... need to learn more?\nSo I can make a file after adding the ansible-nas group to config, but I still cannot make a directory on the smb mount...\n\nADDING `inherit permission = yes` under `[global]` in the smb.conf worked!\n\nstill not working from printer...\nI think what I want is to setup 2 scan options - single docs right to paperless, or combined scans to dump, then manually split and send to paperless", "date": "2022-04-10", "edit_link": "https://github.com/edit/main/pages/blog/home-server-refactor.md", "today": "2022-06-25", "now": "2022-06-25 12:28:31.767921", "datetime": "2022-04-10 00:00:00+00:00", "slug": "home-server-refactor", "jinja": false, "output_html": "markout/home-server-refactor/index.html", "long_description": "My current homelab setup is not great but it works... I boot off an SD card and have 1 SSD and 5 HDDs configured as a JBOD array using a Dell H700 SAS controller. I have an on-prem backup that is just an old desktop running TrueNAS Currently there is", "year": 2022}, {"cover": "/static/dynamic-form-values-with-jinja-and-fastapi.png", "title": "Dynamic-Form-Values-With-Jinja-And-Fastapi", "tags": ["python"], "status": "draft", "templateKey": "blog-post", "path": "pages/blog/dynamic-form-values-with-jinja-and-fastapi.md", "description": "I FastAPI is an amazing framework for quickly building APIs with Python. I will have a slightly longer post about my brief experience with it coming later... On", "content": "I'm currently working on a self-hostable wish list app using FastAPI so we can\nfinally drop Amazon forever. (The lists funcionality has been super handy for\nsharing holiday gift ideas with the famj!)\n\n# FastAPI\n\nFastAPI is an amazing framework for quickly building APIs with Python. I will have a slightly longer post about my brief experience with it coming later...\n\n# Jinja, Forms, and FastAPI\n\nOne of the last things I needed to figure out in my app was how to generate a\nform in a Jinja template with a dynamic number of inputs and then pass all the\ninputs to the backend to perform a database operation (my exact case was\nremoving rows from a table).\n\n## Explicit Values\n\nThe way to pass back explicit variables is really easy...\n\nOur form would look like this (I'm using bootstrap CSS)\n\n```jinja\n<form method=\"post\">\n    <div class=\"form-check \">\n        <input class=\"form-check-input\"  name=\"item_1\" id=\"itemOne\" value=\"1\" type=\"checkbox\">\n        <label class=\"form-check-label\" for=\"itemOne\" > A label for this item </label>\n    </div>\n    <div class=\"form-check \">\n        <input class=\"form-check-input\"  name=\"item_2\" id=\"itemTwo\" value=\"2\" type=\"checkbox\">\n        <label class=\"form-check-label\" for=\"itemTwo\" > A label for item 2 </label>\n    </div>\n\n<button type=\"submit\" class=\"submit btn btn-xl\" >Submit</button>\n</form>\n```\n\nSo what is this? This form will have 2 rows with the lables you see in `<label>\n</label>` and checkboxes that when checked would have the value `value` in each\n`<input>` line.\n\nSo our backend might looks something like this...\n\n__I'm keeping all the imports and stuff here to show where they come from but I won't discuss it all here - that'll be in a future post__\n\n```python\nimport starlette.status as status\nfrom fastapi import APIRouter, Depends, Form, Request\nfrom fastapi.encoders import jsonable_encoder\nfrom fastapi.responses import HTMLResponse, RedirectResponse\nfrom fastapi.templating import Jinja2Templates\nfrom sqlalchemy.orm import Session\n\nfrom app.session.session import create_get_session\n\nrouter = APIRouter()\ntemplates = Jinja2Templates(directory=\"templates/\")\n\n@router.post(\"/my_route/do_something_with_form\", response_class=HTMLResponse)\nasync def delete_rows(\n    request: Request,\n    item_1: int = Form(...),\n    item_2: int = Form(...)\n    db: Session = Depends(create_get_session),\n):\n    print(item_1)  # will just print 1 to the console where fastapi is running if the checkbox was checked\n    print(item_2)  # will just print 1 to the console where fastapi is running if the checkbox was checked\n    return RedirectResponse(\"/\", status_code=status.HTTP_302_FOUND)\n```\n\n\n## Dynamic values\n\nThat's all pretty simple... pass back values by the name in the form...\n\nWhat about a form that's generated dynamically? This is my case since I display a row/checkbox for every row in my table so my form looks like this...\n\n> data is the result of a database query, and item is each row, so the dot notation is the value of each column basically in that row\n\n```jinja\n<form method=\"post\">\n  \n\n<button type=\"submit\" class=\"submit btn btn-xl btn-outline-danger\" >Remove</button>\n</form>\n\n```\n\nThis form generates a row with a checkbox for every `item` in `data` (in my\ncase each `item` is an existing row in my table). Now I started scratching my\nhead on how to pass an unknown number of inputs to my backend of FastAPI wants\neach input explicitly defined and typed... I can't just pass the form back\nbecuase that's not a thing so what's the way to do it?\n\n\n```python\n# same stuff as above, only showing post method here\n@router.post(\"/my_route/do_something_with_form\", response_class=HTMLResponse)\nasync def delete_rows(\n    request: Request,\n    db: Session = Depends(create_get_session),\n):\n    form_data = await request.get_form()\n    data = jsonable_encoder(form_data)\n    # data = {\"item_1\": 1, \"item_2\": 2, ... \"item_N\": N}\n    return RedirectResponse(\"/\", status_code=status.HTTP_302_FOUND)\n```\n\nWe `await request.get_form()` and after encoding the data we get a dictionary with key/value pairs of the name/value from the form!\n\nThis took me quite a long time to figure out in part because most of the Google-able resources are still on Flask...\n\nI look forward to my wish list app maturing and I hope this helps someone working with FastAPI!", "date": "2022-05-15", "edit_link": "https://github.com/edit/main/pages/blog/dynamic-form-values-with-jinja-and-fastapi.md", "today": "2022-06-25", "now": "2022-06-25 12:28:31.767928", "datetime": "2022-05-15 00:00:00+00:00", "slug": "dynamic-form-values-with-jinja-and-fastapi", "jinja": false, "output_html": "markout/dynamic-form-values-with-jinja-and-fastapi/index.html", "long_description": "I FastAPI is an amazing framework for quickly building APIs with Python. I will have a slightly longer post about my brief experience with it coming later... One of the last things I needed to figure out in my app was how to generate a The way to pas", "year": 2022}, {"cover": "/static/pandas-select-dtypes.png", "title": "Pandas-Select-Dtypes", "tags": ["python"], "status": "published", "templateKey": "til", "path": "pages/til/pandas-select-dtypes.md", "description": "On my team we often have to change data types of columns in a  A handy way to do this is by using  Here is an example of finding columns read in as  Here is the", "content": "On my team we often have to change data types of columns in a `pandas.DataFrame` for a variety of reasons.\nThe main one is it tends to be an artifact of EDA whereby a file is read in via `pandas` but the data types are somewhat wonky (ie. dates show up as strings, or a column that *should* be a integer comes in as float, etc.).\nThe best solution I think is to leverage the `dtypes` keyword argument in which `pd.read_X` method is used. \nHowever there is another way which is to coerce the data types at runtime instead of loadtime.\n\nA handy way to do this is by using `pandas.DataFrame.select_dtypes`...\n\nHere is an example of finding columns read in as `datetime64` and the developer would prefer to use pandas datetimes.\n\n```python\ndf = pd.read_csv(\"./file-with-confusing-dtypes.csv\")\nfor c in df.columns:\n    if df[c].dtype == \"datetime64\":\n        df[c] = pd.to_datetime(df.c)\n\n```\n\nHere is the difference in code flow between `select_dtypes` and manually finding the `datetype64` columns:\n\n```python\ndf = pd.read_csv(\"./file-with-confusing-dtypes.csv\")\nfor c in df.select_dtypes('datetime64'):\n    df[c] = pd.to_datetime(df.c)\n\n```\n\n\nThe difference isn't huge but it's the little steps in leveling up that turn script-kitty scripts into clean looking functions.", "date": "2022-03-05", "edit_link": "https://github.com/edit/main/pages/til/pandas-select-dtypes.md", "today": "2022-06-25", "now": "2022-06-25 12:28:31.767935", "datetime": "2022-03-05 00:00:00+00:00", "slug": "til/pandas-select-dtypes", "jinja": false, "output_html": "markout/til/pandas-select-dtypes/index.html", "long_description": "On my team we often have to change data types of columns in a  A handy way to do this is by using  Here is an example of finding columns read in as  Here is the difference in code flow between  The difference isn", "year": 2022}, {"cover": "", "title": "Reindex Nextcloud After Adding Data via CLI", "tags": ["homelab", "linux"], "status": "published", "templateKey": "til", "path": "pages/til/reindex-nextcloud-after-adding-data-via-cli.md", "description": "I wrote  Because I had a ton of photos on the NAS anyways that I wanted moved over to As www-data I needed to ", "content": "# My Nextcloud woes\n\nI wrote [here](\"nextcloud-permissions-with-zfs-and-ansible-nas\") about setting\nup `www-data` as the owner of any directories you want nextcloud to manage.\nHowever, I regularly struggle wtih permissions issues on my NAS because of the\nexternal storage app anyways so I've decided to just put our photos in the spot\nNextcloud would otherwise put them, and use this as healthy pressure on our\nfamily to organize our photos and put the ones we care about with the rest of\nour family media.\n\n# Migration\n\nBecause I had a ton of photos on the NAS anyways that I wanted moved over to\nNextcloud I just rsync'd the photos directory on my NAS to the user's photos\ndirectory in nextcloud but they weren't showing up in the web UI!\n\n# The Fix?\n\nAs www-data I needed to `php /var/www/nextcloud/occ files:scan --all` inside my\nnextcloud docker container AFTER moving a bunch of photos off my \"NAS\" into the\nfolder mounted to the nextcloud container as its data folder! Before I did this\nthey weren't showing up in the web UI/", "date": "2022-05-29", "edit_link": "https://github.com/edit/main/pages/til/reindex-nextcloud-after-adding-data-via-cli.md", "today": "2022-06-25", "now": "2022-06-25 12:28:31.767942", "datetime": "2022-05-29 00:00:00+00:00", "slug": "til/reindex-nextcloud-after-adding-data-via-cli", "jinja": false, "output_html": "markout/til/reindex-nextcloud-after-adding-data-via-cli/index.html", "long_description": "I wrote  Because I had a ton of photos on the NAS anyways that I wanted moved over to As www-data I needed to ", "year": 2022}, {"cover": "/static/pandas-string-contains.png", "title": "Pandas-String-Contains", "tags": ["python"], "status": "published", "templateKey": "til", "path": "pages/til/pandas-string-contains.md", "description": "pandas.Series.str.contains We often need to filter pandas DataFrames based on several string values in a Series. Notice that sweet pyflyby import \ud83d\ude01 And this isn", "content": "# TL;DR \n\n`pandas.Series.str.contains` accepts regular expressions and this is turned on by __default__!\n\n# Use case\n\nWe often need to filter pandas DataFrames based on several string values in a Series.\n\n> Notice that sweet pyflyby import \ud83d\ude01!\n\n```python\nsandbox \uf7a1  main via 3.8.11(sandbox) ipython\n\u276f df = pd.DataFrame({\"A\": [\"string1\", \"string2\", \"string3\"]})\n[PYFLYBY] import pandas as pd\n\nsandbox \uf7a1  main via 3.8.11(sandbox) ipython\n\u276f df\n\n         A\n0  string1\n1  string2\n2  string3\n\nsandbox \uf7a1  main via 3.8.11(sandbox) ipython\n\u276f df[df.A.str.contains('1') | df.A.str.contains('2')]\n\n         A\n0  string1\n1  string2\n\n```\n\nAnd this isn't the worst thing in the world, especially for such a tiny example...\n\nBut what if we had dozens or more values to filter on?\n\nThen it looks so much nicer to create an iterable of the values we want to filter on and join them with an apropriate regex operator (in this case `|` for _inclusive or_)\n\n```python\n\nsandbox \uf7a1  main via 3.8.11(sandbox) ipython\n\u276f vals = [\"1\", \"2\"]  # iterable with whatever is appropriate for your use case\n\nsandbox \uf7a1  main via 3.8.11(sandbox) ipython\n\u276f df[df.A.str.contains(\"|\".join(vals), regex=True)]\n\n         A\n0  string1\n1  string2\n\n```\n\n# Fin\n\nThis is a super nice and concise way to do the kind of filtering my team does on a daily basis!", "date": "2022-05-02", "edit_link": "https://github.com/edit/main/pages/til/pandas-string-contains.md", "today": "2022-06-25", "now": "2022-06-25 12:28:31.767949", "datetime": "2022-05-02 00:00:00+00:00", "slug": "til/pandas-string-contains", "jinja": false, "output_html": "markout/til/pandas-string-contains/index.html", "long_description": "pandas.Series.str.contains We often need to filter pandas DataFrames based on several string values in a Series. Notice that sweet pyflyby import \ud83d\ude01 And this isn But what if we had dozens or more values to filter on? Then it looks so much nicer to cre", "year": 2022}, {"cover": "/static/unpack-anywhere-with-star.png", "title": "Unpack-Anywhere-With-Star", "tags": ["python"], "status": "published", "templateKey": "til", "path": "pages/til/unpack-anywhere-with-star.md", "description": "Unpacking iterables in python with  But  I", "content": "Unpacking iterables in python with `*` is a pretty handy trick for writing code that is just a tiny bit more pythonic than not.\n\n```python\narr: Tuple[Union[int, str]] = (1, 2, 3, 'a', 'b', 'c')\n\n\nprint(arr)\n>>> (1, 2, 3, 'a', 'b', 'c')\n\n# the * unpacks the tuple into the individual elements\nprint(*arr)\n>>> 1, 2, 3, 'a', 'b', 'c'\n\nx, y, z, *alphas = arr\n\n# x = 1, y = 2, z = 3\n# alphas = [ 'a', 'b', 'c' ]\n\n```\n\nBut [@Ned Batchelder](https://twitter.com/nedbat) showed me via Twitter than you can arbitrarily unpack arguments based on position - it doesn't have to be done at the beginning or the end!\n\n```python\nx, y, *mixed, alpha = arr\n\n# x = 1, y = 2\n# mixed = [3, 'a', 'b']\n# alpha = 'c'\n```\n\nI'm not entirely sure when I'll need this but it definitley shows me another example of how flexible python is!", "date": "2022-04-24", "edit_link": "https://github.com/edit/main/pages/til/unpack-anywhere-with-star.md", "today": "2022-06-25", "now": "2022-06-25 12:28:31.767955", "datetime": "2022-04-24 00:00:00+00:00", "slug": "til/unpack-anywhere-with-star", "jinja": false, "output_html": "markout/til/unpack-anywhere-with-star/index.html", "long_description": "Unpacking iterables in python with  But  I", "year": 2022}, {"cover": "", "title": "Add space to your LVM on Ubuntu", "tags": ["homelab", "linux"], "status": "published", "templateKey": "til", "path": "pages/til/add-space-to-your-lvm-on-ubuntu.md", "description": "I ran out of space on the SSD in my server when doing some file transfers but only 100GB was used of a 256 GB SSD? When installing Ubuntu live server the defaul", "content": "I ran out of space on the SSD in my server when doing some file transfers but only 100GB was used of a 256 GB SSD?\n\n# LVM \n\nWhen installing Ubuntu live server the default option for how to partition the\ndisk (in my experience) has been to setup an LVM group that defaults to less\nthan the available space. Most recently I put Ubuntu server on a 256 GB SSD but\nthe main partition was formatted as an LVM group with 100GB of storage... I\ndidn't think anything of this even though I'm mostly used to EXT4.\n\nI think the reason for LVMs is performance, but in hindsight, I don't really\ncare much about the performance differences, I really just want all my storage\nthat's fast enough\n\n# Extending the LVM\n\n A moment of googling brought me to Ubuntu's wiki and I\nlearned that I can expand my LVM to the space I need...\n\n`sudo lvdisplay` and `sudo pvdisplay` show detailed views of the logical volumes and physical volumes respectively.\n\nTake a look at those and find the volume you need to extend. For me I found this:\n\n```bash\n  --- Logical volume ---\n  LV Path                /dev/ubuntu-vg/ubuntu-lv\n  LV Name                ubuntu-lv\n  VG Name                ubuntu-vg\n  LV Write Access        read/write\n  LV Status              available\n  ...\n```\n\nThere's more that you'll see but this is what's relevant - I need to extend the\n`ubuntu-lv` logical volume in the `ubuntu-vg` volume group.\n\n`sudo lvextend -L +50g ubuntu-vg/ubuntu-lv` gives me 50 more GB of storage which should be enough for at least tonight \ud83e\udd13\n\n[RTFM](https://wiki.ubuntu.com/Lvm)", "date": "2022-05-26", "edit_link": "https://github.com/edit/main/pages/til/add-space-to-your-lvm-on-ubuntu.md", "today": "2022-06-25", "now": "2022-06-25 12:28:31.767962", "datetime": "2022-05-26 00:00:00+00:00", "slug": "til/add-space-to-your-lvm-on-ubuntu", "jinja": false, "output_html": "markout/til/add-space-to-your-lvm-on-ubuntu/index.html", "long_description": "I ran out of space on the SSD in my server when doing some file transfers but only 100GB was used of a 256 GB SSD? When installing Ubuntu live server the default option for how to partition the I think the reason for LVMs is performance, but in hinds", "year": 2022}, {"cover": "/static/dataframe-to-markdown.png", "title": "Dataframe-To-Markdown", "tags": ["python"], "status": "published", "templateKey": "til", "path": "pages/til/dataframe-to-markdown.md", "description": "pandas.DataFrame I do a lot of work with tabular data and one thing I have incorporated into some of that work is automatic data summary reports by throwing the", "content": "# Pandas\n\n`pandas.DataFrame`s are pretty sweet data structures in Python.\n\nI do a lot of work with tabular data and one thing I have incorporated into some of that work is automatic data summary reports by throwing the first few, or several relevant, rows of a dataframe at a point in a pipeline into a markdown file.\n\nPandas has a method on DataFrames that makes this 100% trivial!\n\n# The Method\n\nSay we have a dataframe, `df`... then it's literally just: `df.to_markdown()`\n\n```python\n\u276f df.head()\n\n          Unnamed: 0   mpg  cyl   disp   hp  drat     wt   qsec  vs  am  gear  carb\n0          Mazda RX4  21.0    6  160.0  110  3.90  2.620  16.46   0   1     4     4\n1      Mazda RX4 Wag  21.0    6  160.0  110  3.90  2.875  17.02   0   1     4     4\n2         Datsun 710  22.8    4  108.0   93  3.85  2.320  18.61   1   1     4     1\n3     Hornet 4 Drive  21.4    6  258.0  110  3.08  3.215  19.44   1   0     3     1\n4  Hornet Sportabout  18.7    8  360.0  175  3.15  3.440  17.02   0   0     3     2\n\n```\n\nIn ipython I can call the method and get a markdown table back as a string\n\n```python\n\nmental-data-lake \uf7a1  new-posts via 3.8.11(mental-data-lake) ipython\n\u276f df.head().to_markdown()\n'|    | Unnamed: 0        |   mpg |   cyl |   disp |   hp |   drat |    wt |   qsec |   vs |   am |   gear |   carb |\\n|---:|:------------------|------:|------:|-------:|-----:|-------:|------:|-------:|-----:|-----:|-------:|-------:|\\n|  0 | Mazda RX4         |  21   |     6 |    160 |  110 |   3.9  | 2.62  |  16.46 |    0 |    1 |      4 |      4 |\\n|  1 | Mazda RX4 Wag     |  21   |     6 |    160 |  110 |   3.9  | 2.875 |  17.02 |    0 |    1 |      4 |      4 |\\n|  2 | Datsun 710        |  22.8 |     4 |    108 |   93 |   3.85 | 2.32  |  18.61 |    1 |    1 |      4 |      1 |\\n|  3 | Hornet 4 Drive    |  21.4 |     6 |    258 |  110 |   3.08 | 3.215 |  19.44 |    1 |    0 |      3 |      1 |\\n|  4 | Hornet Sportabout |  18.7 |     8 |    360 |  175 |   3.15 | 3.44  |  17.02 |    0 |    0 |      3 |      2 |'\n\n```\n\nYou can drop that string into a markdown file and using any reader that supports the rendering you'll have a nicely formated table of example data in whatever report you're making!\n\n# Bonus method\n\nJust like markdown, you can export a dataframe to html with `df.to_html()` and use that if it's more appropriate for your use case:\n\n```text\n\n'<table border=\"1\" class=\"dataframe\">\\n  <thead>\\n    <tr style=\"text-align: right;\">\\n      <th></th>\\n      <th>Unnamed: 0</th>\\n      <th>mpg</th>\\n      <th>cyl</th>\\n      <th>disp</th>\\n      <th>hp</th>\\n      <th>drat</th>\\n      <th>wt</th>\\n      <th>qsec</th>\\n      <th>vs</th>\\n      <th>am</th>\\n      <th>gear</th>\\n      <th>carb</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <th>0</th>\\n      <td>Mazda RX4</td>\\n      <td>21.0</td>\\n      <td>6</td>\\n      <td>160.0</td>\\n      <td>110</td>\\n      <td>3.90</td>\\n      <td>2.620</td>\\n      <td>16.46</td>\\n      <td>0</td>\\n      <td>1</td>\\n      <td>4</td>\\n      <td>4</td>\\n    </tr>\\n    <tr>\\n      <th>1</th>\\n      <td>Mazda RX4 Wag</td>\\n      <td>21.0</td>\\n      <td>6</td>\\n      <td>160.0</td>\\n      <td>110</td>\\n      <td>3.90</td>\\n      <td>2.875</td>\\n      <td>17.02</td>\\n      <td>0</td>\\n      <td>1</td>\\n      <td>4</td>\\n      <td>4</td>\\n    </tr>\\n    <tr>\\n      <th>2</th>\\n      <td>Datsun 710</td>\\n      <td>22.8</td>\\n      <td>4</td>\\n      <td>108.0</td>\\n      <td>93</td>\\n      <td>3.85</td>\\n      <td>2.320</td>\\n      <td>18.61</td>\\n      <td>1</td>\\n      <td>1</td>\\n      <td>4</td>\\n      <td>1</td>\\n    </tr>\\n    <tr>\\n      <th>3</th>\\n      <td>Hornet 4 Drive</td>\\n      <td>21.4</td>\\n      <td>6</td>\\n      <td>258.0</td>\\n      <td>110</td>\\n      <td>3.08</td>\\n      <td>3.215</td>\\n      <td>19.44</td>\\n      <td>1</td>\\n      <td>0</td>\\n      <td>3</td>\\n      <td>1</td>\\n    </tr>\\n    <tr>\\n      <th>4</th>\\n      <td>Hornet Sportabout</td>\\n      <td>18.7</td>\\n      <td>8</td>\\n      <td>360.0</td>\\n      <td>175</td>\\n      <td>3.15</td>\\n      <td>3.440</td>\\n      <td>17.02</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>3</td>\\n      <td>2</td>\\n    </tr>\\n  </tbody>\\n</table>'\n\n```\n\nMy blog will render that html into a nice table! (After removing new line characters)\n\n<table border=\"1\" class=\"dataframe\">  <thead>    <tr style=\"text-align: right;\">      <th>Unnamed: 0</th>      <th>mpg</th>      <th>cyl</th>      <th>disp</th>      <th>hp</th>      <th>drat</th>      <th>wt</th>      <th>qsec</th>      <th>vs</th>      <th>am</th>      <th>gear</th>      <th>carb</th>    </tr>  </thead>  <tbody>    <tr>      <td>Mazda RX4</td>      <td>21.0</td>      <td>6</td>      <td>160.0</td>      <td>110</td>      <td>3.90</td>      <td>2.620</td>      <td>16.46</td>      <td>0</td>      <td>1</td>      <td>4</td>      <td>4</td>    </tr>    <tr>      <td>Mazda RX4 Wag</td>      <td>21.0</td>      <td>6</td>      <td>160.0</td>      <td>110</td>      <td>3.90</td>      <td>2.875</td>      <td>17.02</td>      <td>0</td>      <td>1</td>      <td>4</td>      <td>4</td>    </tr>    <tr>      <td>Datsun 710</td>      <td>22.8</td>      <td>4</td>      <td>108.0</td>      <td>93</td>      <td>3.85</td>      <td>2.320</td>      <td>18.61</td>      <td>1</td>      <td>1</td>      <td>4</td>      <td>1</td>    </tr>    <tr>      <td>Hornet 4 Drive</td>      <td>21.4</td>      <td>6</td>      <td>258.0</td>      <td>110</td>      <td>3.08</td>      <td>3.215</td>      <td>19.44</td>      <td>1</td>      <td>0</td>      <td>3</td>      <td>1</td>    </tr>    <tr>      <td>Hornet Sportabout</td>      <td>18.7</td>      <td>8</td>      <td>360.0</td>      <td>175</td>      <td>3.15</td>      <td>3.440</td>      <td>17.02</td>      <td>0</td>      <td>0</td>      <td>3</td>      <td>2</td>    </tr>  </tbody></table>", "date": "2022-05-07", "edit_link": "https://github.com/edit/main/pages/til/dataframe-to-markdown.md", "today": "2022-06-25", "now": "2022-06-25 12:28:31.767969", "datetime": "2022-05-07 00:00:00+00:00", "slug": "til/dataframe-to-markdown", "jinja": false, "output_html": "markout/til/dataframe-to-markdown/index.html", "long_description": "pandas.DataFrame I do a lot of work with tabular data and one thing I have incorporated into some of that work is automatic data summary reports by throwing the first few, or several relevant, rows of a dataframe at a point in a pipeline into a markd", "year": 2022}, {"cover": "/static/stow.png", "title": "Stow", "tags": ["bash", "linux"], "status": "published", "templateKey": "til", "path": "pages/til/stow.md", "description": "Stow is a great tool for managing dotfiles. My usage looks like cloning my dotfiles to my home directory, setting some environment variables via a script, then ", "content": "Stow is a great tool for managing dotfiles. My usage looks like cloning my dotfiles to my home directory, setting some environment variables via a script, then stowing relevant packages and boom my config is good to go...\n\n```bash\ncd ~\ngit clone <my dotfiles repo>\ncd dotfiles\n# env variable stuff ignored here\nstow zsh  # This will symlink my .zshrc file which is in ~/dotfiles/zsh to ~/.zshrc\n```\nBy default stow will stow packages up one directory from the root directory. \nIn this example the root directory is `~/dotfiles` and the package is `zsh`.\nSo the files in the `zsh` package will symlinked into `~/`.\n\n`stow` makes it easy to share dotfiles across machines, or safely experiment with config changes while always being protected by `git` since your dotfiles are in a git repo!\n...They are in a git repo... right?", "date": "2022-03-04", "edit_link": "https://github.com/edit/main/pages/til/stow.md", "today": "2022-06-25", "now": "2022-06-25 12:28:31.767976", "datetime": "2022-03-04 00:00:00+00:00", "slug": "til/stow", "jinja": false, "output_html": "markout/til/stow/index.html", "long_description": "Stow is a great tool for managing dotfiles. My usage looks like cloning my dotfiles to my home directory, setting some environment variables via a script, then stowing relevant packages and boom my config is good to go... By default stow will stow pa", "year": 2022}, {"cover": "/static/python-builtin-calendar.png", "title": "Python-Builtin-Calendar", "tags": ["python"], "status": "published", "templateKey": "til", "path": "pages/til/python-builtin-calendar.md", "description": "I almost exclusively use Python for my job and have been eye-balls deep in it for almost 5 years but I really lack in-depth knowledge of builtins. I only needed", "content": "## Being lazy\n\nI almost exclusively use Python for my job and have been eye-balls deep in it for almost 5 years but I really lack in-depth knowledge of builtins.\nI recently learned of an awesome builtin called `calendar`  that has way more than I know about for sure but I'm glad I know it's here now!\n\nI only needed it because I was too lazy to hard code the 7 weekdays into my module but it turns out there's a lot of useful things like `calendar.isleap()`!\n\n\n![Alt text](/images/builtin-calendar.png \"calendar\")\n\n## Future use\n\nI'm not exactly sure what will come my way where `calendar` will be super relevant but like anything, I'm just glad to know it exists for when the time arises!", "date": "2022-03-08", "edit_link": "https://github.com/edit/main/pages/til/python-builtin-calendar.md", "today": "2022-06-25", "now": "2022-06-25 12:28:31.767983", "datetime": "2022-03-08 00:00:00+00:00", "slug": "til/python-builtin-calendar", "jinja": false, "output_html": "markout/til/python-builtin-calendar/index.html", "long_description": "I almost exclusively use Python for my job and have been eye-balls deep in it for almost 5 years but I really lack in-depth knowledge of builtins. I only needed it because I was too lazy to hard code the 7 weekdays into my module but it turns out the", "year": 2022}, {"cover": "/static/vim-auto-space.png", "title": "Vim-Auto-Space", "tags": ["vim"], "status": "published", "templateKey": "til", "path": "pages/til/vim-auto-space.md", "description": "I ran into an issue where I had some copy-pasta markdown tables in a docstring but the generator I used to make the table gave me tabs instead of spaces in odd ", "content": "I ran into an issue where I had some copy-pasta markdown tables in a docstring but the generator I used to make the table gave me tabs instead of spaces in odd places which caused `black` to throw a fit.\nInstead of manually changing all tabs to spaes, or trying some goofy `:%s/<magic tab character>/<%20 maybe?>/g` I learned that Vim has my back...\n\n```\n:retab\n```", "date": "2022-03-04", "edit_link": "https://github.com/edit/main/pages/til/vim-auto-space.md", "today": "2022-06-25", "now": "2022-06-25 12:28:31.767990", "datetime": "2022-03-04 00:00:00+00:00", "slug": "til/vim-auto-space", "jinja": false, "output_html": "markout/til/vim-auto-space/index.html", "long_description": "I ran into an issue where I had some copy-pasta markdown tables in a docstring but the generator I used to make the table gave me tabs instead of spaces in odd places which caused ", "year": 2022}, {"cover": "", "title": "cheat on your man", "tags": ["linux", "cli"], "status": "published", "templateKey": "til", "path": "pages/til/cheat-on-your-man.md", "description": "man man man cheat man You get tiny examples to remind you of what you ", "content": "`man` can be a pain to read... and there's lots of alternatives out there and one I've just started playing with is [cheat](https://github.com/cheat/cheat)\n\n\n`man man` will give you this plus a billion more lines of docs, which is useful when you need it...\n\n```bash\nMAN(1)                                                                                                                       Manual pager utils                                                                                                                      MAN(1)\n\nNAME\n       man - an interface to the on-line reference manuals\n\nSYNOPSIS\n       man  [-C  file]  [-d] [-D] [--warnings[=warnings]] [-R encoding] [-L locale] [-m system[,...]] [-M path] [-S list] [-e extension] [-i|-I] [--regex|--wildcard] [--names-only] [-a] [-u] [--no-subpages] [-P pager] [-r prompt] [-7] [-E encoding] [--no-hyphenation]\n       [--no-justification] [-p string] [-t] [-T[device]] [-H[browser]] [-X[dpi]] [-Z] [[section] page[.section] ...] ...\n       man -k [apropos options] regexp ...\n       man -K [-w|-W] [-S list] [-i|-I] [--regex] [section] term ...\n       man -f [whatis options] page ...\n       man -l [-C file] [-d] [-D] [--warnings[=warnings]] [-R encoding] [-L locale] [-P pager] [-r prompt] [-7] [-E encoding] [-p string] [-t] [-T[device]] [-H[browser]] [-X[dpi]] [-Z] file ...\n       man -w|-W [-C file] [-d] [-D] page ...\n       man -c [-C file] [-d] [-D] page ...\n       man [-?V]\n\nDESCRIPTION\n       man is the system's manual pager.  Each page argument given to man is normally the name of a program, utility or function.  The manual page associated with each of these arguments is then found and displayed.  A section, if provided, will direct  man  to  look\n       only  in  that  section of the manual.  The default action is to search in all of the available sections following a pre-defined order (\"1 n l 8 3 2 3posix 3pm 3perl 3am 5 4 9 6 7\" by default, unless overridden by the SECTION directive in /etc/manpath.config),\n       and to show only the first page found, even if page exists in several sections.\n\n       The table below shows the section numbers of the manual followed by the types of pages they contain.\n\n       1   Executable programs or shell commands\n       2   System calls (functions provided by the kernel)\n       3   Library calls (functions within program libraries)\n       4   Special files (usually found in /dev)\n       5   File formats and conventions eg /etc/passwd\n       6   Games\n       7   Miscellaneous (including macro packages and conventions), e.g. man(7), groff(7)\n       8   System administration commands (usually only for root)\n       9   Kernel routines [Non standard]\n\n       A manual page consists of several sections.\n\n       Conventional section names include NAME, SYNOPSIS, CONFIGURATION, DESCRIPTION, OPTIONS, EXIT STATUS, RETURN VALUE, ERRORS, ENVIRONMENT, FILES, VERSIONS, CONFORMING TO, NOTES, BUGS, EXAMPLE, AUTHORS, and SEE ALSO.\n\n       The following conventions apply to the SYNOPSIS section and can be used as a guide in other sections.\n\n       bold text          type exactly as shown.\n       italic text        replace with appropriate argument.\n       [-abc]             any or all arguments within [ ] are optional.\n       -a|-b              options delimited by | cannot be used together.\n       argument ...       argument is repeatable.\n       [expression] ...   entire expression within [ ] is repeatable.\n\n       Exact rendering may vary depending on the output device.  For instance, man will usually not be able to render italics when running in a terminal, and will typically use underlined or coloured text instead.\n\n       The command or function illustration is a pattern that should match all possible invocations.  In some cases it is advisable to illustrate several exclusive invocations as is shown in the SYNOPSIS section of this manual page.\n\nEXAMPLES\n       man ls\n           Display the manual page for the item (program) ls.\n\n       man man.7\n           Display the manual page for macro package man from section 7.\n```\n\n\n## But what if you don't?\n\n`cheat man`\n\n```bash\n# To convert a man page to pdf:\nman -t bash | ps2pdf - bash.pdf\n\n# To view the ascii chart:\nman 7 ascii\n```\n\nYou get tiny examples to remind you of what you **probably** are trying to do!", "date": "2022-06-23", "edit_link": "https://github.com/edit/main/pages/til/cheat-on-your-man.md", "today": "2022-06-25", "now": "2022-06-25 12:28:31.767997", "datetime": "2022-06-23 00:00:00+00:00", "slug": "til/cheat-on-your-man", "jinja": false, "output_html": "markout/til/cheat-on-your-man/index.html", "long_description": "man man man cheat man You get tiny examples to remind you of what you ", "year": 2022}, {"cover": "/static/forms-with-fast-api-and-jinja.png", "title": "Forms with FastAPI and Jinja", "tags": ["python"], "status": "published", "templateKey": "til", "path": "pages/til/fastapi-jinja-forms.md", "description": "I just started using FastAPI for a home project and needed to pass back a The jinja templating for rendering HTML based on something like a python iterable is n", "content": "I just started using FastAPI for a home project and needed to pass back a\ndynamic number of values from a form rendered with jinja...\n\n\n# Dynamic Values \n\nThe jinja templating for rendering HTML based on something like a python iterable is nice and easy\n\n> data is the result of a database query, and item is each row, so the dot notation is the value of each column basically in that row\n\n```jinja\n<form method=\"post\">\n  \n\n<button type=\"submit\" class=\"submit btn btn-xl btn-outline-danger\" >Remove</button>\n</form>\n\n```\n\nThis form generates a row with a checkbox for every `item` in `data` (in my\ncase each `item` is an existing row in my table). it?\n\nThe way to pass back all those values is pretty straight forward (after hours of messing around that is!)\n\n```python\n# I hate it when tutorials don't show ALL relevant pieces to the blurb\nimport starlette.status as status\nfrom fastapi import APIRouter, Depends, Form, Request\nfrom fastapi.encoders import jsonable_encoder\nfrom fastapi.responses import HTMLResponse, RedirectResponse\nfrom fastapi.templating import Jinja2Templates\nfrom sqlalchemy.orm import Session\n\nfrom app.session.session import create_get_session\n\nrouter = APIRouter()\ntemplates = Jinja2Templates(directory=\"templates/\")\n\n@router.post(\"/my_route/do_something_with_form\", response_class=HTMLResponse)\nasync def delete_rows(\n    request: Request,\n    db: Session = Depends(create_get_session),\n):\n    form_data = await request.get_form()\n    data = jsonable_encoder(form_data)\n    # data = {\"item_1\": 1, \"item_2\": 2, ... \"item_N\": N}\n    return RedirectResponse(\"/\", status_code=status.HTTP_302_FOUND)\n```\n\nWe `await request.get_form()` and after encoding the data we get a dictionary with key/value pairs of the name/value from the form!\n\nThis took me quite a long time to figure out in part because most of the Google-able resources are still on Flask...", "date": "2022-05-15", "edit_link": "https://github.com/edit/main/pages/til/fastapi-jinja-forms.md", "today": "2022-06-25", "now": "2022-06-25 12:28:31.768004", "datetime": "2022-05-15 00:00:00+00:00", "slug": "til/fastapi-jinja-forms", "jinja": false, "output_html": "markout/til/fastapi-jinja-forms/index.html", "long_description": "I just started using FastAPI for a home project and needed to pass back a The jinja templating for rendering HTML based on something like a python iterable is nice and easy data is the result of a database query, and item is each row, so the dot nota", "year": 2022}, {"cover": "/static/tree.png", "title": "Tree", "tags": ["linux"], "status": "published", "templateKey": "til", "path": "pages/til/tree.md", "description": "I wanted a quick way to generate an  tree Say I have a file structure like this: To generate a barebones simple  tree ./html-files -H \".\" -L 1 -P \"*.html\" and g", "content": "I wanted a quick way to generate an `index.html` for a directory of html files that grows by 1 or 2 files a week.\nI don't know any html (the files are exports from my [tiddlywiki](/tiddly-wiki))...\n\n`tree` is just the answer.\n\nSay I have a file structure like this:\n\n```\n./html-files\n\u251c\u2500\u2500 file1.html\n\u2514\u2500\u2500 file2.html\n```\n\nTo generate a barebones simple `index.html` we can use tree as follows:\n\n`tree ./html-files -H \".\" -L 1 -P \"*.html\"`\n\nand get the following:\n\n```html\n\n<!DOCTYPE html>\n<html>\n<head>\n <meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\">\n <meta name=\"Author\" content=\"Made by 'tree'\">\n <meta name=\"GENERATOR\" content=\"$Version: $ tree v1.8.0 (c) 1996 - 2018 by Steve Baker, Thomas Moore, Francesc Rocher, Florian Sesser, Kyosuke Tokoro $\">\n <title>Directory Tree</title>\n <style type=\"text/css\">\n  <!--\n  BODY { font-family : ariel, monospace, sans-serif; }\n  P { font-weight: normal; font-family : ariel, monospace, sans-serif; color: black; background-color: transparent;}\n  B { font-weight: normal; color: black; background-color: transparent;}\n  A:visited { font-weight : normal; text-decoration : none; background-color : transparent; margin : 0px 0px 0px 0px; padding : 0px 0px 0px 0px; display: inline; }\n  A:link    { font-weight : normal; text-decoration : none; margin : 0px 0px 0px 0px; padding : 0px 0px 0px 0px; display: inline; }\n  A:hover   { color : #000000; font-weight : normal; text-decoration : underline; background-color : yellow; margin : 0px 0px 0px 0px; padding : 0px 0px 0px 0px; display: inline; }\n  A:active  { color : #000000; font-weight: normal; background-color : transparent; margin : 0px 0px 0px 0px; padding : 0px 0px 0px 0px; display: inline; }\n  .VERSION { font-size: small; font-family : arial, sans-serif; }\n  .NORM  { color: black;  background-color: transparent;}\n  .FIFO  { color: purple; background-color: transparent;}\n  .CHAR  { color: yellow; background-color: transparent;}\n  .DIR   { color: blue;   background-color: transparent;}\n  .BLOCK { color: yellow; background-color: transparent;}\n  .LINK  { color: aqua;   background-color: transparent;}\n  .SOCK  { color: fuchsia;background-color: transparent;}\n  .EXEC  { color: green;  background-color: transparent;}\n  -->\n </style>\n</head>\n<body>\n        <h1>Directory Tree</h1><p>\n        <a href=\".\">.</a><br>\n        \u251c\u2500\u2500 <a href=\"./file1.html\">file1.html</a><br>\n        \u2514\u2500\u2500 <a href=\"./file2.html\">file2.html</a><br>\n        <br><br>\n        </p>\n        <p>\n\n0 directories, 2 files\n        <br><br>\n        </p>\n        <hr>\n        <p class=\"VERSION\">\n                 tree v1.8.0 \u00a9 1996 - 2018 by Steve Baker and Thomas Moore <br>\n                 HTML output hacked and copyleft \u00a9 1998 by Francesc Rocher <br>\n                 JSON output hacked and copyleft \u00a9 2014 by Florian Sesser <br>\n                 Charsets / OS/2 support \u00a9 2001 by Kyosuke Tokoro\n        </p>\n</body>\n</html>\n\n```\n\n\nwhich [looks like this](/tree-index-example.html) when you serve it up with `python -m http.server`", "date": "2022-03-06", "edit_link": "https://github.com/edit/main/pages/til/tree.md", "today": "2022-06-25", "now": "2022-06-25 12:28:31.768011", "datetime": "2022-03-06 00:00:00+00:00", "slug": "til/tree", "jinja": false, "output_html": "markout/til/tree/index.html", "long_description": "I wanted a quick way to generate an  tree Say I have a file structure like this: To generate a barebones simple  tree ./html-files -H \".\" -L 1 -P \"*.html\" and get the following: which ", "year": 2022}, {"cover": "", "title": "Netplan change from Focal to Jammy", "tags": ["homelab", "linux"], "status": "published", "templateKey": "til", "path": "pages/til/netplan-change-from-focal-to-jammy.md", "description": "I am revamping my home server and bumped myself early up to Jammy Jellyfish... Turns out Netplan got a little change in how to express the  Old Ubuntu 20.04 way", "content": "I am revamping my home server and bumped myself early up to Jammy Jellyfish...\nhowever to my peril I reused my netplan config and after hitting my server with\nthe 'ol `netplan apply` I lost connection...\nDNS still seemed to kinda work externally, but internally nothing was up... \n\nTurns out Netplan got a little change in how to express the `gateway` key in the netplan config!\n\nOld Ubuntu 20.04 way\n\n```yaml\nnetwork:\n  version: 2\n  ethernets:\n    enp0s4:\n      addresses: [192.168.1.{Static IP}/24]\n      gateway4: 192.168.1.1  # <-- This changes!\n      nameservers:\n        addresses: [192.168.1.1, 1.1.1.1]\n```\n\nNew jammin way for Jammy Jellyfish (__at least that worked for me__)\n```yaml\nnetwork:\n  version: 2\n  ethernets:\n    enp0s4:\n      addresses: [192.168.1.{Static IP}/24]\n      routes:\n        - to: default\n          via: 192.168.1.1 \n      nameservers:\n        addresses: [192.168.1.1, 1.1.1.1]\n```", "date": "2022-05-22", "edit_link": "https://github.com/edit/main/pages/til/netplan-change-from-focal-to-jammy.md", "today": "2022-06-25", "now": "2022-06-25 12:28:31.768018", "datetime": "2022-05-22 00:00:00+00:00", "slug": "til/netplan-change-from-focal-to-jammy", "jinja": false, "output_html": "markout/til/netplan-change-from-focal-to-jammy/index.html", "long_description": "I am revamping my home server and bumped myself early up to Jammy Jellyfish... Turns out Netplan got a little change in how to express the  Old Ubuntu 20.04 way New jammin way for Jammy Jellyfish (", "year": 2022}, {"cover": "/static/pyclean.png", "title": "Pyclean", "tags": ["python"], "status": "published", "templateKey": "til", "path": "pages/til/pyclean.md", "description": "I like to keep my workspace clean and one thing that I don The easiest way ( You could accomplish something similar with ", "content": "I like to keep my workspace clean and one thing that I don't personally love looking at is the `__pycache__` directory that pops up after running some code.\nThe `*.pyc` files that show up there are python bytecode and they are cached to make subsequent runs a tad faster. \nMy stuff never really needs this bonus speed boost and so I came across a neat tool called `pyclean`!\n\n## Pyclean\n\nThe easiest way (**in my opinion**) to run `pyclean` is to just use `pipx run`.\n\n```bash\nsandbox/src  \ud83c\udf31 main \ud83d\uddd1\ufe0f  \u00d73\ud83d\udee4\ufe0f  \u00d72via \ud83d\udc0d v3.8.11 (sandbox)  took 9s\n\u276f ls\nabcmeta.py  __pycache__  python-print-align.py  system-monitor-psutils.py\n\nsandbox/src  \ud83c\udf31 main \ud83d\uddd1\ufe0f  \u00d73\ud83d\udee4\ufe0f  \u00d72via \ud83d\udc0d v3.8.11 (sandbox)\n\u276f pipx run pyclean .\n\u26a0\ufe0f  pyclean is already on your PATH and installed at /usr/bin/pyclean. Downloading and running anyway.\nCleaning directory .\nTotal 1 files, 1 directories removed.\n\nsandbox/src  \ud83c\udf31 main \ud83d\uddd1\ufe0f  \u00d73\ud83d\udee4\ufe0f  \u00d72via \ud83d\udc0d v3.8.11 (sandbox)\n\u276f ls\nabcmeta.py  python-print-align.py  system-monitor-psutils.py\n\n```\n\n## Why not bash?\n\nYou could accomplish something similar with `rm **/*.pyc` or `find -n '*.py?' -delete` but there's a chance you'll find something you don't love gone.\nAlso this won't help our poor Windows friends out there!\n`pyclean` is fully python so it's OS independent.\n\n## Credits!\n\n[repo](https://github.com/bittner/pyclean)", "date": "2022-03-22", "edit_link": "https://github.com/edit/main/pages/til/pyclean.md", "today": "2022-06-25", "now": "2022-06-25 12:28:31.768025", "datetime": "2022-03-22 00:00:00+00:00", "slug": "til/pyclean", "jinja": false, "output_html": "markout/til/pyclean/index.html", "long_description": "I like to keep my workspace clean and one thing that I don The easiest way ( You could accomplish something similar with ", "year": 2022}, {"cover": "/static/mu.png", "title": "Mu", "tags": ["python", "git"], "status": "published", "templateKey": "til", "path": "pages/til/mu.md", "description": "If you work with a template for several projects then you might sometimes need to do the same action across all repos. mu status mu sh mu stash There mu As you ", "content": "If you work with a template for several projects then you might sometimes need to do the same action across all repos.\nA good example of this is updating a package in `requirements.txt` in every project, or refactoring a common module.\nIf you have several repos to do this across then it can be time consuming... enter `mu-repo`\n\n\n## Mu\n\n[mu-repo](https://fabioz.github.io/mu-repo/) is an awesome cli tool for working with multiple git repositories at the same time. \nThere are several things you can do:\n\n1. `mu status` will give you the `git status` of every registered repo (see below)\n2. `mu sh` will let you execute system level commands in every repo\n3. `mu stash` will stash all changes across all registered repos\n4. There's literally a ton more but these are some handy ones\n\n\n## Registration\n\n`mu` tracks its own `groups`, and there is a default group when no particular one is active.\nIt's as simple as `mu register proj1 prog2 ...` to get repos registered\n\n```bash \n\n\u276f mu register proj1 proj2\nRepository: proj1 registered\nRepository: proj2 registered\n\n\u276f mu status\n\n  proj1 : git status\n    On branch main\n\n    No commits yet\n\n    Untracked files:\n    (use \"git add <file>...\" to include in what will be committed)\n    requirements.txt\n\n    nothing added to commit but untracked files present (use \"git add\" to track)\n\n  proj2 : git status\n    On branch main\n\n    No commits yet\n\n    Changes to be committed:\n    (use \"git rm --cached <file>...\" to unstage)\n    new file:   requirements.txt\n\n\n```\n\n## Working with mu\n\nAs you can see above I have two projects each with a `requirements.txt` added but not committed yet.\nUsing `mu` I can stage this change across both repos at once.\n\n```bash  \n\n\u276f mu add requirements.txt\n\n  proj1 : git add requirements.txt\n\n  proj2 : git add requirements.txt\n```\n\nThen as you might imagine, I can make the commit in each repo\n\n\n```bash\n\n\u276f mu commit -m \"Add requirements.txts\"\n\n  proj1 : git commit -m Add requirements.txts\n    [main (root-commit) 18376d7] Add requirements.txts\n    1 file changed, 1 insertion(+)\n    create mode 100644 requirements.txt\n\n  proj2 : git commit -m Add requirements.txts\n    [main (root-commit) 18376d7] Add requirements.txts\n    1 file changed, 1 insertion(+)\n    create mode 100644 requirements.txt\n```\n\n## mu groups\n\nThe other thing I got a lot of use out of recently was `mu`'s groups.\nAt work I have about 40 repos cloned that are all based on the same kedro pipeline template.\nSome of these projects have been deprecated.\nI also have several more repos that are not kedro template - custom libraries or something.\n`group` let me utilize `mu` across different groups of repos.\n\nSay `proj2` is a deprecated project that I don't need to worry about making changes to anymore.\nI don't just have to unregister it, instead I can make a group called \"active\" and register `proj1` in that group\n\n```bash\n\n\u276f mu group add active --empty\n\n~/personal\n\u276f mu group add deprecated --empty\n\n~/personal\n\u276f mu group\n  active\n* deprecated\n\n```\n\n\nThe `*` tells me which group is active. \nThe `--empty` flag tells `mu` to not add all registered repos to that group.\nIf I don't want to use any groups then `mu group reset` will go back to the default group with all registered repos.\n\nWith groups I can register only the repos that I want to be working across in their own group and not worry about affecting other repos with my batch changes!", "date": "2022-03-15", "edit_link": "https://github.com/edit/main/pages/til/mu.md", "today": "2022-06-25", "now": "2022-06-25 12:28:31.768033", "datetime": "2022-03-15 00:00:00+00:00", "slug": "til/mu", "jinja": false, "output_html": "markout/til/mu/index.html", "long_description": "If you work with a template for several projects then you might sometimes need to do the same action across all repos. mu status mu sh mu stash There mu As you can see above I have two projects each with a  Then as you might imagine, I can make the c", "year": 2022}, {"cover": "/static/stow-target.png", "title": "Stow-Target", "tags": ["bash", "linux"], "status": "published", "templateKey": "til", "path": "pages/til/stow-target.md", "description": "Check out  What if I want to stow a package somewhere else? Maybe I don", "content": "Check out [stow](/stow) for a brief introduction to `stow`\n\nWhat if I want to stow a package somewhere else?\nBoom, that's where `-t` comes in...\n\nMaybe I don't like having my `dotfiles` repo at `$HOME` and instead I want it in `~/git` or `~/personal` just to stay organized...\nWell then I could have the same workflow except the `stow` command looks like this:\n\n```bash\nstow zsh -t ~/\n#or\nstow zsh -t $HOME\n```", "date": "2022-03-04", "edit_link": "https://github.com/edit/main/pages/til/stow-target.md", "today": "2022-06-25", "now": "2022-06-25 12:28:31.768041", "datetime": "2022-03-04 00:00:00+00:00", "slug": "til/stow-target", "jinja": false, "output_html": "markout/til/stow-target/index.html", "long_description": "Check out  What if I want to stow a package somewhere else? Maybe I don", "year": 2022}, {"cover": "/static/ubuntu-static-ip.png", "title": "Ubuntu-Static-Ip", "tags": ["linux"], "status": "published", "templateKey": "til", "path": "pages/til/ubuntu-static-ip.md", "description": "Sometimes I need to manually set a static IP of a Linux machine. I generally run the latest version of Ubuntu server in my VMs at home. In Ubuntu 20 I gateway4 ", "content": "Sometimes I need to manually set a static IP of a Linux machine. I generally run the latest version of Ubuntu server in my VMs at home.\n\nIn Ubuntu 20 I'm able to change up `/etc/netplan/<something>.yml`\n\n```yaml\nnetwork:\n  version: 2\n  ethernets:\n    enp0s4:\n      addresses: [192.168.1.{Static IP}/24]\n      gateway4: 192.168.1.1\n      nameservers:\n        addresses: [192.168.1.1, 1.1.1.1]\n```\n\n`gateway4` is your router address\n`nameservers` is a list of desired DNS servers for that machine to use. I  usually use my router which is configured to use my pi-hole as my primary DNS, then set  `1.1.1.1` (CloudFlare) as a backup\n\nHit it with the `sudo netplan apply` and you should be good to go!", "date": "2022-03-03", "edit_link": "https://github.com/edit/main/pages/til/ubuntu-static-ip.md", "today": "2022-06-25", "now": "2022-06-25 12:28:31.768048", "datetime": "2022-03-03 00:00:00+00:00", "slug": "til/ubuntu-static-ip", "jinja": false, "output_html": "markout/til/ubuntu-static-ip/index.html", "long_description": "Sometimes I need to manually set a static IP of a Linux machine. I generally run the latest version of Ubuntu server in my VMs at home. In Ubuntu 20 I gateway4 Hit it with the ", "year": 2022}, {"cover": "/static/vim-spell-check.png", "title": "Vim-Spell-Check", "tags": ["vim"], "status": "published", "templateKey": "til", "path": "pages/til/vim-spell-check.md", "description": "set: spell spelllang=en_us Sometimes there Common example: package names plotly You can easily add these to your vim config by hitting ", "content": "__Did you know you can spell check in Vim?!__\n\n\n<!DOCTYPE html>\n<html>\n   <head>\n      <title>Vim Spell check</title>\n   </head>\n\n   <body>\n      <h3>Without...</h3>\n      <p>Here is a missspelled word.</p>\n\n      <h3>With!</h3>\n      <p>Here is a <u>missspelled</u> word.</p>\n\n   </body>\n</html>\n\n## What is this magic???\n\n`set: spell spelllang=en_us`\n\n\n## Custom words?\n\nSometimes there's things that are words to you but not the default spell checker...\n\nCommon example: package names!\n\n`plotly`, `streamlit`, `psutil`, etc etc...\n\n\nYou can easily add these to your vim config by hitting `zw` ontop of the word!", "date": "2022-04-01", "edit_link": "https://github.com/edit/main/pages/til/vim-spell-check.md", "today": "2022-06-25", "now": "2022-06-25 12:28:31.768054", "datetime": "2022-04-01 00:00:00+00:00", "slug": "til/vim-spell-check", "jinja": false, "output_html": "markout/til/vim-spell-check/index.html", "long_description": "set: spell spelllang=en_us Sometimes there Common example: package names plotly You can easily add these to your vim config by hitting ", "year": 2022}, {"cover": "/static/dataframe-memory-usage.png", "title": "Dataframe-Memory-Usage", "tags": ["python"], "status": "published", "templateKey": "til", "path": "pages/til/dataframe-memory-usage.md", "description": "I have often wanted to dive into memory usage for pandas DataFrames when it comes to cloud deployment. I didn", "content": "I have often wanted to dive into memory usage for pandas DataFrames when it comes to cloud deployment.\nIf I have a python process running on a server at home I can use `glances` or a number of other tools to diagnose a memory issue...\nHowever at work I normally deploy dockerized processes on AWS Batch and it's much more challenging to get info on the dockerized process without more AWS integration that my team isn't quite ready for.\nSo TIL that I can get some of the info I want from pandas directly!\n\n# DataFrame.info()\n\nI didn't realize that `df.info()` was able to give me more info than just dtypes and some summary stats...\nThere is a kwarg `memory_usage` that can configure what you need to get back, so `df.memory_usage=\"deep\"` will give you how much RAM any given DataFrame is using!\nAmazing tool for finding issues with joins or renegade source data files.\n\n```python\ndf = pd.read_csv(\"cars.csv\")\n\ndf.info(memory_usage=\"deep\")\n```\n\n![Alt text](/images/df-memory-usage.png \"DF memory\")", "date": "2022-03-07", "edit_link": "https://github.com/edit/main/pages/til/dataframe-memory-usage.md", "today": "2022-06-25", "now": "2022-06-25 12:28:31.768061", "datetime": "2022-03-07 00:00:00+00:00", "slug": "til/dataframe-memory-usage", "jinja": false, "output_html": "markout/til/dataframe-memory-usage/index.html", "long_description": "I have often wanted to dive into memory usage for pandas DataFrames when it comes to cloud deployment. I didn", "year": 2022}, {"cover": "/static/htop.png", "title": "Htop", "tags": ["til"], "status": "published", "templateKey": "til", "path": "pages/til/htop.md", "description": "htop I Just hit ", "content": "`htop` is a common command line tool for seeing interactive output of your system resource utilization, running processes, etc.\n\nI've always been super confused about htop showing seemingly the same process several times though...\n\n## The Fix...\n\nJust hit `H`.... makes the view a lot nicer \ud83d\ude00", "date": "2022-04-24", "edit_link": "https://github.com/edit/main/pages/til/htop.md", "today": "2022-06-25", "now": "2022-06-25 12:28:31.768068", "datetime": "2022-04-24 00:00:00+00:00", "slug": "til/htop", "jinja": false, "output_html": "markout/til/htop/index.html", "long_description": "htop I Just hit ", "year": 2022}, {"cover": "", "title": "Subset a list based on values in another list with itertools.compress", "tags": ["python", "python"], "status": "published", "templateKey": "til", "path": "pages/til/subset-a-list-based-on-values-in-another-list-with-itertools.compress.md", "description": "I have list ", "content": "I have list [True, False, False, True] and another list [1, 2, 3, 4] and a use case where I want to filter list 2 based on list 1 to remove values that line up with the element False in list 1.... so the outcome will be [1, 4]. list(compress(list2, list1)) will do it. As long as you can create a mask for the filter than itertool.compress will be your friend!", "date": "2022-05-19", "edit_link": "https://github.com/edit/main/pages/til/subset-a-list-based-on-values-in-another-list-with-itertools.compress.md", "today": "2022-06-25", "now": "2022-06-25 12:28:31.768075", "datetime": "2022-05-19 00:00:00+00:00", "slug": "til/subset-a-list-based-on-values-in-another-list-with-itertools.compress", "jinja": false, "output_html": "markout/til/subset-a-list-based-on-values-in-another-list-with-itertools.compress/index.html", "long_description": "I have list ", "year": 2022}, {"cover": "", "title": "Tdarr worker nodes share the cache!", "tags": ["homelab", "homelab"], "status": "published", "templateKey": "til", "path": "pages/til/tdarr-worker-nodes-share-the-cache.md", "description": "When working with tdarr remote nodes, they need to have access not only to the To explain I I have an old Dell PowerEdge R610 as my main server running a live s", "content": "When working with tdarr remote nodes, they need to have access not only to the\nsame libraries but also the same transcode cache as the server otherwise the\ntranscodes will fail...\n\n# Network Setup\n\nTo explain I'll give a brief overview of my home setup  \n\nI have an old Dell PowerEdge R610 as my main server running a live server distribution of Ubuntu.\nI use ZFS for my NAS file system, and most of my datasets are accesible over my home network.\nI have a Tdarr server running in a docker container on the R610.\n\nIn my office I dailyi drive a gaming desktop with an Nvidia 2060 Super running Ubuntu as well.\nOn that desktop I am running a Tdarr node in a docker container. \nThe container has access to the network folders with my media. \n\n# Initial Magic \n\nWhen I spun up  a tdarr node on my desktop, the tdarr server running on my R610 automatically registered the node, which was freaking amazing.\nThat magic though spoiled me and I thought that I didn't need to read the rest of the docs...\n\n# Initial Fail \n\nI setup a transcode cache directory on the R610 locally and a separate transcode cache on my desktop that the remote tdarr node would use.\nHaving them separated led to 2 main issues:\n1. Transcodes were not being migrated back to my library properly\n2. I was running out of disk space on my desktop because tdarr wasn't deleting the completed cache files properly.\n\n# The Fix\n\nMake a transcode cache directory accessible on my network, and give access to that directory to the docker container running my remote tdarr node.", "date": "2022-05-25", "edit_link": "https://github.com/edit/main/pages/til/tdarr-worker-nodes-share-the-cache.md", "today": "2022-06-25", "now": "2022-06-25 12:28:31.768082", "datetime": "2022-05-25 00:00:00+00:00", "slug": "til/tdarr-worker-nodes-share-the-cache", "jinja": false, "output_html": "markout/til/tdarr-worker-nodes-share-the-cache/index.html", "long_description": "When working with tdarr remote nodes, they need to have access not only to the To explain I I have an old Dell PowerEdge R610 as my main server running a live server distribution of Ubuntu. In my office I dailyi drive a gaming desktop with an Nvidia ", "year": 2022}, {"cover": "/static/python-eval.png", "title": "Python-Eval", "tags": [], "status": "published", "templateKey": "til", "path": "pages/til/python-eval.md", "description": "", "content": "# TODO\n\n```python\ntitle = \"my Title\"\neval('\"my\" in title')\n\n>>> True\n\n```", "date": "2022-05-12", "edit_link": "https://github.com/edit/main/pages/til/python-eval.md", "today": "2022-06-25", "now": "2022-06-25 12:28:31.768089", "datetime": "2022-05-12 00:00:00+00:00", "slug": "til/python-eval", "jinja": false, "output_html": "markout/til/python-eval/index.html", "long_description": "", "year": 2022}, {"cover": "/static/file-length.png", "title": "File-Length", "tags": ["linux"], "status": "published", "templateKey": "til", "path": "pages/til/file-length.md", "description": "I have a specific need for counting the number of lines in a file quickly. To get that list I run an internal tool like this: This simply parses our internal li", "content": "I have a specific need for counting the number of lines in a file quickly.\nAt work we use S3 for data storage during our Kedro pipeline development, and in the development process we may end up orphaning several datasets.\nIn order to keep our workspace clean I have a short utility that compares the datasets in a Kedro DataCatalog with the files in the relevant S3 location.\n\nTo get that list I run an internal tool like this:\n\n```bash\nkedro our-liter | grep s3 >> orphaned_datasets.txt\n```\n\nThis simply parses our internal linter for the lines releated to my s3 linter utility and pipes those lines to a file.\n\nTo get a quick idea of how out of wack a pipeline is I could open the text file in vim, git it with the `G` and see what line number I'm on but I'm way too lazy for that...\n\n## AWK\n\n`awk 'END {print NR}' orphaned_datasets.txt` gives me the number of lines and I can alias this to whatever feels appropriate in my `zshrc`!\n\n__built-ins for the win!__", "date": "2022-04-04", "edit_link": "https://github.com/edit/main/pages/til/file-length.md", "today": "2022-06-25", "now": "2022-06-25 12:28:31.768096", "datetime": "2022-04-04 00:00:00+00:00", "slug": "til/file-length", "jinja": false, "output_html": "markout/til/file-length/index.html", "long_description": "I have a specific need for counting the number of lines in a file quickly. To get that list I run an internal tool like this: This simply parses our internal linter for the lines releated to my s3 linter utility and pipes those lines to a file. To ge", "year": 2022}, {"cover": "", "title": "Add colored indicators to your dataframes html representation", "tags": ["python", "data"], "status": "published", "templateKey": "til", "path": "pages/til/add-colored-indicators-to-your-dataframes-html-representation.md", "description": "First though... why? The docs for the  So we can write a function that returns  By default the function will be applied to all columns of the dataframe, but Con", "content": "[Mike Driscoll](https://twitter.com/driscollis) recently tweeted about making\ncolored out with pandas DataFrames and I just had to try it for myself\n\n# Use Case\n\nFirst though... why?\nMy biggest use case is a monitoring pipeline of mine... The details aside, the\noutput of my pipeline is a dataframe where each row has information about a\nfailed pipeline that I need to go look into. I dump that result to a simle html\nfile that's hosted on an internal site and the file is updated every couple of\nhours. Adding some colored indicators automatically to the rows to help me\nassess severity of each record would be a handy way to quickly get an\nunderstanding the state of our pipelines.\n\n# How?\n\nThe docs for the `applymap` method state simply:\n\n```\nApply a CSS-styling function elementwise.\n\nUpdates the HTML representation with the result.\n\n```\n\nSo we can write a function that returns `color: {color}` based on the dataframe\nvalues and when we drop that dataframe to html we'll have some simple css\nstyling applied automagically!\n\nBy default the function will be applied to all columns of the dataframe, but\nthat's not useful if the columns are different types which is usually the case.\nLuckily there is a `subset` keyword to only apply to the columns you need!\n\nConsider my example\n\n```python \nsandbox \uf7a1  main via 3.8.11(sandbox) ipython\n\u276f df = pd.read_csv(\"cars.csv\")\n\nsandbox \uf7a1  main via 3.8.11(sandbox) ipython\n\u276f def mpg_color(val: float):\n...:     color = \"red\" if val < 21 else \"green\"\n...:     return f\"color: {color}\"\n\nsandbox \uf7a1  main via 3.8.11(sandbox) ipython\n\u276f df.style.applymap(mpg_color, subset=\"mpg\").to_html(\"color.html\")\n```\n\nI want to quickly see if the `mpg` is any good for the cars in the cars dataset\nand I'll define \"good\" as better than 21 mpg (not great I know but just for the\nsake of discussion...)\n\nThe function returns an appropriate css string and after I `style.applymap` on just the `mpg` column we get this!\n\n\n<style type=\"text/css\">\n#T_95e99_row0_col1, #T_95e99_row1_col1, #T_95e99_row2_col1, #T_95e99_row3_col1 {\n  color: green;\n}\n#T_95e99_row4_col1 {\n  color: red;\n}\n</style>\n<table id=\"T_95e99\">\n  <thead>\n    <tr>\n      <th class=\"blank level0\" >&nbsp;</th>\n      <th id=\"T_95e99_level0_col0\" class=\"col_heading level0 col0\" >Unnamed: 0</th>\n      <th id=\"T_95e99_level0_col1\" class=\"col_heading level0 col1\" >mpg</th>\n      <th id=\"T_95e99_level0_col2\" class=\"col_heading level0 col2\" >cyl</th>\n      <th id=\"T_95e99_level0_col3\" class=\"col_heading level0 col3\" >disp</th>\n      <th id=\"T_95e99_level0_col4\" class=\"col_heading level0 col4\" >hp</th>\n      <th id=\"T_95e99_level0_col5\" class=\"col_heading level0 col5\" >drat</th>\n      <th id=\"T_95e99_level0_col6\" class=\"col_heading level0 col6\" >wt</th>\n      <th id=\"T_95e99_level0_col7\" class=\"col_heading level0 col7\" >qsec</th>\n      <th id=\"T_95e99_level0_col8\" class=\"col_heading level0 col8\" >vs</th>\n      <th id=\"T_95e99_level0_col9\" class=\"col_heading level0 col9\" >am</th>\n      <th id=\"T_95e99_level0_col10\" class=\"col_heading level0 col10\" >gear</th>\n      <th id=\"T_95e99_level0_col11\" class=\"col_heading level0 col11\" >carb</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th id=\"T_95e99_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n      <td id=\"T_95e99_row0_col0\" class=\"data row0 col0\" >Mazda RX4</td>\n      <td id=\"T_95e99_row0_col1\" class=\"data row0 col1\" >21.000000</td>\n      <td id=\"T_95e99_row0_col2\" class=\"data row0 col2\" >6</td>\n      <td id=\"T_95e99_row0_col3\" class=\"data row0 col3\" >160.000000</td>\n      <td id=\"T_95e99_row0_col4\" class=\"data row0 col4\" >110</td>\n      <td id=\"T_95e99_row0_col5\" class=\"data row0 col5\" >3.900000</td>\n      <td id=\"T_95e99_row0_col6\" class=\"data row0 col6\" >2.620000</td>\n      <td id=\"T_95e99_row0_col7\" class=\"data row0 col7\" >16.460000</td>\n      <td id=\"T_95e99_row0_col8\" class=\"data row0 col8\" >0</td>\n      <td id=\"T_95e99_row0_col9\" class=\"data row0 col9\" >1</td>\n      <td id=\"T_95e99_row0_col10\" class=\"data row0 col10\" >4</td>\n      <td id=\"T_95e99_row0_col11\" class=\"data row0 col11\" >4</td>\n    </tr>\n    <tr>\n      <th id=\"T_95e99_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n      <td id=\"T_95e99_row1_col0\" class=\"data row1 col0\" >Mazda RX4 Wag</td>\n      <td id=\"T_95e99_row1_col1\" class=\"data row1 col1\" >21.000000</td>\n      <td id=\"T_95e99_row1_col2\" class=\"data row1 col2\" >6</td>\n      <td id=\"T_95e99_row1_col3\" class=\"data row1 col3\" >160.000000</td>\n      <td id=\"T_95e99_row1_col4\" class=\"data row1 col4\" >110</td>\n      <td id=\"T_95e99_row1_col5\" class=\"data row1 col5\" >3.900000</td>\n      <td id=\"T_95e99_row1_col6\" class=\"data row1 col6\" >2.875000</td>\n      <td id=\"T_95e99_row1_col7\" class=\"data row1 col7\" >17.020000</td>\n      <td id=\"T_95e99_row1_col8\" class=\"data row1 col8\" >0</td>\n      <td id=\"T_95e99_row1_col9\" class=\"data row1 col9\" >1</td>\n      <td id=\"T_95e99_row1_col10\" class=\"data row1 col10\" >4</td>\n      <td id=\"T_95e99_row1_col11\" class=\"data row1 col11\" >4</td>\n    </tr>\n    <tr>\n      <th id=\"T_95e99_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n      <td id=\"T_95e99_row2_col0\" class=\"data row2 col0\" >Datsun 710</td>\n      <td id=\"T_95e99_row2_col1\" class=\"data row2 col1\" >22.800000</td>\n      <td id=\"T_95e99_row2_col2\" class=\"data row2 col2\" >4</td>\n      <td id=\"T_95e99_row2_col3\" class=\"data row2 col3\" >108.000000</td>\n      <td id=\"T_95e99_row2_col4\" class=\"data row2 col4\" >93</td>\n      <td id=\"T_95e99_row2_col5\" class=\"data row2 col5\" >3.850000</td>\n      <td id=\"T_95e99_row2_col6\" class=\"data row2 col6\" >2.320000</td>\n      <td id=\"T_95e99_row2_col7\" class=\"data row2 col7\" >18.610000</td>\n      <td id=\"T_95e99_row2_col8\" class=\"data row2 col8\" >1</td>\n      <td id=\"T_95e99_row2_col9\" class=\"data row2 col9\" >1</td>\n      <td id=\"T_95e99_row2_col10\" class=\"data row2 col10\" >4</td>\n      <td id=\"T_95e99_row2_col11\" class=\"data row2 col11\" >1</td>\n    </tr>\n    <tr>\n      <th id=\"T_95e99_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n      <td id=\"T_95e99_row3_col0\" class=\"data row3 col0\" >Hornet 4 Drive</td>\n      <td id=\"T_95e99_row3_col1\" class=\"data row3 col1\" >21.400000</td>\n      <td id=\"T_95e99_row3_col2\" class=\"data row3 col2\" >6</td>\n      <td id=\"T_95e99_row3_col3\" class=\"data row3 col3\" >258.000000</td>\n      <td id=\"T_95e99_row3_col4\" class=\"data row3 col4\" >110</td>\n      <td id=\"T_95e99_row3_col5\" class=\"data row3 col5\" >3.080000</td>\n      <td id=\"T_95e99_row3_col6\" class=\"data row3 col6\" >3.215000</td>\n      <td id=\"T_95e99_row3_col7\" class=\"data row3 col7\" >19.440000</td>\n      <td id=\"T_95e99_row3_col8\" class=\"data row3 col8\" >1</td>\n      <td id=\"T_95e99_row3_col9\" class=\"data row3 col9\" >0</td>\n      <td id=\"T_95e99_row3_col10\" class=\"data row3 col10\" >3</td>\n      <td id=\"T_95e99_row3_col11\" class=\"data row3 col11\" >1</td>\n    </tr>\n    <tr>\n      <th id=\"T_95e99_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n      <td id=\"T_95e99_row4_col0\" class=\"data row4 col0\" >Hornet Sportabout</td>\n      <td id=\"T_95e99_row4_col1\" class=\"data row4 col1\" >18.700000</td>\n      <td id=\"T_95e99_row4_col2\" class=\"data row4 col2\" >8</td>\n      <td id=\"T_95e99_row4_col3\" class=\"data row4 col3\" >360.000000</td>\n      <td id=\"T_95e99_row4_col4\" class=\"data row4 col4\" >175</td>\n      <td id=\"T_95e99_row4_col5\" class=\"data row4 col5\" >3.150000</td>\n      <td id=\"T_95e99_row4_col6\" class=\"data row4 col6\" >3.440000</td>\n      <td id=\"T_95e99_row4_col7\" class=\"data row4 col7\" >17.020000</td>\n      <td id=\"T_95e99_row4_col8\" class=\"data row4 col8\" >0</td>\n      <td id=\"T_95e99_row4_col9\" class=\"data row4 col9\" >0</td>\n      <td id=\"T_95e99_row4_col10\" class=\"data row4 col10\" >3</td>\n      <td id=\"T_95e99_row4_col11\" class=\"data row4 col11\" >2</td>\n    </tr>\n  </tbody>\n</table>", "date": "2022-06-04", "edit_link": "https://github.com/edit/main/pages/til/add-colored-indicators-to-your-dataframes-html-representation.md", "today": "2022-06-25", "now": "2022-06-25 12:28:31.768103", "datetime": "2022-06-04 00:00:00+00:00", "slug": "til/add-colored-indicators-to-your-dataframes-html-representation", "jinja": false, "output_html": "markout/til/add-colored-indicators-to-your-dataframes-html-representation/index.html", "long_description": "First though... why? The docs for the  So we can write a function that returns  By default the function will be applied to all columns of the dataframe, but Consider my example I want to quickly see if the  The function returns an appropriate css str", "year": 2022}, {"cover": "/static/webservers-and-indexes.png", "title": "Webservers-And-Indexes", "tags": ["homelab"], "status": "published", "templateKey": "til", "path": "pages/til/webservers-and-indexes.md", "description": "I host a lot of services in my homelab, but they A  Something that confused the heck out of me when I first started down the road of having a server was what a ", "content": "I host a lot of services in my homelab, but they're mostly dockerized applications so I have never had to care much about how content gets served up.\nToday I had several little concepts click into place regarding webservers, and it was a similar experience to when I started homelabing and didn't know what a \"server\" was in the first place.\n\n# Servers\n\nA \"server\" can have a lot of different meanings but specifically in my world it was a physical server, like my PowerEdge R610 which acts as my main \"home server\".\nBut then on my server, I have other servers... Jellyfin is my main media server - but that's obviously not a hardware thing, that's software. \nThis is certainly not a groundbreaking thing but it was a tiny piece to the puzzle that I was missing... that \"server\" is highly contextual.\n\n# Webservers \n\nSomething that confused the heck out of me when I first started down the road of having a server was what a webserver even was...\nI always thought the \"webserver\" was just \"a server that hosts a website\"... and yes, that's true, but also it wasn't true in how I understood \"server\".\nIt turns out that across my 40-odd dockerized services I have at home that I must have about 40-odd web servers running, each docker container is spinning up its own!\n\nSo something I have wanted to do for a long time is put my theology notes online for my small group to access whenever they might want... it doesn't need to be fancy or anything.\nMy issue was not knowing what to even Google. I tried \"How to serve up static html\" but that kind of search is for people who know what a \"static\" site is - I am not one of those people.\nI kept running across nginx and apache things, wordpress and other website building tools, etc.\nIn fact I only recently learned that JavaScript assets cann still be considered static so I am a complete baby in the web-dev space.\n\nWhat I really wanted was just a simple landing page with a link to each of my \"posts\" which are in the form of a single html file each that I can easily export from my tiddlywiki (I have a post about tiddlywiki [here](/tiddly-wiki))\n\nThe first win `python -m http.server` right in the directory I kept my html files in and that got me what I wanted functionally. \nBut then I wanted just a hair more organization...\nI started looking for a way to dynamically generate an index for a directory of html files but again the verbiage of that Google search just wasn't helping me - I didn't want anything complicated and I knew that what I wanted had to be easy...\n\n# The Index \n\nLuckily I randomly came across a SO that mentioned a Linux utility called `tree` which does exactly what I wanted!\n\nSee my TIL on `tree` [here]('/tree')\n\nSo now it goes like this:\n\n* Take notes on X in my tiddlywiki\n* Export that tiddler to a html file \n* Put that html file into a `notes` folder in my github repo for small group notes \n* Use `tree` to generate an `index.html` of each of those files in the `notes` directory\n* Use `python -m http.server` to start a web server that lands me at the `index.html` and now I can click through to any post!\n\nIt's not fancy but it's functional... \nThis site/blog is built with markdown and [markata](https://www.markata.dev) and I wanted way more functionality in my tech notes.\nBut for this simple use case I learned a ton about _how_ content gets served up on a webpage and my small group benefits from the easy access as well!", "date": "2022-03-06", "edit_link": "https://github.com/edit/main/pages/til/webservers-and-indexes.md", "today": "2022-06-25", "now": "2022-06-25 12:28:31.768110", "datetime": "2022-03-06 00:00:00+00:00", "slug": "til/webservers-and-indexes", "jinja": false, "output_html": "markout/til/webservers-and-indexes/index.html", "long_description": "I host a lot of services in my homelab, but they A  Something that confused the heck out of me when I first started down the road of having a server was what a webserver even was... So something I have wanted to do for a long time is put my theology ", "year": 2022}, {"cover": "/static/deques.png", "title": "Deques", "tags": ["python"], "status": "published", "templateKey": "til", "path": "pages/til/deques.md", "description": "I am working on a project to create a small system monitoring dashboard using the python  The repo is  I I needed a way to refresh my plotly charts with a fixed", "content": "I am working on a project to create a small system monitoring dashboard using the python `psutil` library.\n\nThe repo is [here](https://github.com/nicpayne713/not-netdata) (if you want actual system monitoring please use [netdata](https://www.netdata.cloud/)).\n\nI'm using `streamlit` and `plotly` for the webserver, design, and plotting at the moment.\n\n## My Use Case\n\nI needed a way to refresh my plotly charts with a fixed window of time so that I'm able to just see relevant recent data instead of cramming all data for all time into one plot that's 500 pixels wide...\n\nChecking the length of arrays or lists every time I get a new piece of data feels kind of dumb and I thought \"python must have a way to do this\"...\n\n> \"This\" meaning, update values in a fixed length array without reallocating memory or recreating a copy of the list\n\n## Deques\n\nEnter the `deque`. \nIt means \"double ended queue\" and is in general an `Iterable` that you can append values to either side or pop values from either side.\n\nThe init signature is straightforward enough and I'm sure there's more to them than I know yet but here's how I use it...\n\n```python\nfrom collections import deque\n\nmy_deque = deque([1,2,3])\n```\n\nThis gives us `my_deque`, created from an iterable, with several familiar methods like `index`, `extend`, `append`, etc.\nHowever there's some new ones too such as `appendleft` and `popleft`.\n\n```python\nmy_deque.appendleft('a')\nprint(my_dequqe)\n>>> deque(['a', 1, 2, 3])\n\nmy_deque.popleft()\n>>> 'a'\nprint(my_deque)\n>>> deque([1, 2, 3])\n```\n\nThese are handy ways to manipulate the iterable that I needed for the arrays I plot with plotly!\n\n\n__See my follow-up to this on using Deques with plotly and streamlit to create a quick \"dashboard\" with live streaming data!__\n\n[follow-up](/plotly-and-streamlit)", "date": "2022-03-31", "edit_link": "https://github.com/edit/main/pages/til/deques.md", "today": "2022-06-25", "now": "2022-06-25 12:28:31.768117", "datetime": "2022-03-31 00:00:00+00:00", "slug": "til/deques", "jinja": false, "output_html": "markout/til/deques/index.html", "long_description": "I am working on a project to create a small system monitoring dashboard using the python  The repo is  I I needed a way to refresh my plotly charts with a fixed window of time so that I Checking the length of arrays or lists every time I get a new pi", "year": 2022}, {"cover": "/static/typeddict.png", "title": "Typeddict", "tags": ["til", "python"], "status": "published", "templateKey": "til", "path": "pages/til/typeddict.md", "description": "Type hinting has helped me write code almost as much, if not more, than unit testing. One thing I love is that with complete type hinting you get a lot more out", "content": "Type hinting has helped me write code almost as much, if not more, than unit testing.\n\nOne thing I love is that with complete type hinting you get a lot more out of your LSP.\nTyping dictionaries can be tricky and I recently learned about `TypedDict` to do exactly what I needed!\n\n\n## The Problem\n\nIt might not be straight up obvious what the problem is, especially if you don't utilize tools like `mypy` or `flake8` in your development.\n\nMy handy-dandy `nvim-lsp` gives me a lot of feedback when I'm coding and it's immensely helpful.\n\nSo with the LSP giving me constant feedback here's the issue:\n\n```python\nfrom typing import Dict, List, Union\n\nmy_dict: Dict[str, Union[List[str], str]] = {\n    \"key_1\": \"val_1\",\n    \"key_2\": [\"ls_1\", \"ls_2\"],\n}\n\nmy_dict[\"key_2\"].pop()\n```\n\nWith the above script you'll get an annoying warning about using `pop` on `key_2`.\n\n\n![Alt text](/images/typed-dict-warning.png \"dict-warning\")\n\n\n## The Solution\n\nMaybe you can stomach getting yelled at by your LSP but I like complete silence if at all possible.\n\n`TypedDict`  was the saving grace.\n\n```python\nfrom typing import TypedDict\n\nMyDict = TypedDict(\"MyDict\", {\"key_1\": str, \"key_2\": List[str]})\n\nmy_typed_dict: MyDict = {\n    \"key_1\": \"val_1\",\n    \"key_2\": [\"ls_1\", \"ls_2\"],\n}\n\n\nmy_typed_dict[\"key_2\"].pop()\n```\n\n![Alt text](/images/typed-dict.png \"typeddict\")\n\n> I was able to import TypedDict from typing, mypy_extensions, and typing_extensions\n\nWith `TypedDict` you define your custom type, match the first argument to `TypedDict` with the name of the variable (idk why), then type hint each key you expect in the dict!\nIt's super easy and I think puts you into a position of being extremely explicit with your dictionary variables. \nThis isn't always desired or appropriate but in most of my use cases it is.\n\n## RTFM\n\nThere's other implementation of `TypedDict` and while writing this I saw that most of the docs define a `class` for the type like this:\n\n```python\nfrom typing import TypedDict\nclass MyDict(TypedDict):\n    key_1: str\n    key_2: List[str]\n\nmy_dict : MyDict = {'key_1': 'val_1', 'key_2': [\"ls_1\", \"ls_2\"]}\n\n```\n\n[pep docs](https://peps.python.org/pep-0589/)\n\n[mypy docs](https://mypy.readthedocs.io/en/latest/more_types.html#typeddict)", "date": "2022-04-15", "edit_link": "https://github.com/edit/main/pages/til/typeddict.md", "today": "2022-06-25", "now": "2022-06-25 12:28:31.768124", "datetime": "2022-04-15 00:00:00+00:00", "slug": "til/typeddict", "jinja": false, "output_html": "markout/til/typeddict/index.html", "long_description": "Type hinting has helped me write code almost as much, if not more, than unit testing. One thing I love is that with complete type hinting you get a lot more out of your LSP. It might not be straight up obvious what the problem is, especially if you d", "year": 2022}, {"cover": "", "title": "Plug Snapshot!", "tags": ["vim", "vim", "vim"], "status": "published", "templateKey": "til", "path": "pages/til/plug-snapshot.md", "description": ":PlugSnapshot, :w ~/dotfiles/nvim/snapshot.vim... keep your config safe with git", "content": ":PlugSnapshot, :w ~/dotfiles/nvim/snapshot.vim... keep your config safe with git!", "date": "2022-05-17", "edit_link": "https://github.com/edit/main/pages/til/plug-snapshot.md", "today": "2022-06-25", "now": "2022-06-25 12:28:31.768130", "datetime": "2022-05-17 00:00:00+00:00", "slug": "til/plug-snapshot", "jinja": false, "output_html": "markout/til/plug-snapshot/index.html", "long_description": ":PlugSnapshot, :w ~/dotfiles/nvim/snapshot.vim... keep your config safe with git", "year": 2022}, {"cover": "/static/git-ammend-no-edit.png", "title": "Git ammend to a commit", "tags": ["git"], "status": "published", "templateKey": "til", "path": "pages/til/git-ammend-no-edit.md", "description": "After carefully staging only lines related to a specific change and comitting I suddenly realized I missed one... darn, what do I do? Old me would have soft res", "content": "After carefully staging only lines related to a specific change and comitting I suddenly realized I missed one... darn, what do I do?\n\nOld me would have soft reset my branch to the previous commit and redone all my careful staging... what a PIA...\n\nNew me (credit: ThePrimeagen)...\n\n```bash\n# stage other changes I missed\ngit commit --amend --no-edit\n```", "date": "2022-03-04", "edit_link": "https://github.com/edit/main/pages/til/git-ammend-no-edit.md", "today": "2022-06-25", "now": "2022-06-25 12:28:31.768138", "datetime": "2022-03-04 00:00:00+00:00", "slug": "til/git-ammend-no-edit", "jinja": false, "output_html": "markout/til/git-ammend-no-edit/index.html", "long_description": "After carefully staging only lines related to a specific change and comitting I suddenly realized I missed one... darn, what do I do? Old me would have soft reset my branch to the previous commit and redone all my careful staging... what a PIA... New", "year": 2022}, {"cover": "/static/dataframe-to-styled-html.png", "title": "Dataframe-To-Styled-Html", "tags": ["python"], "status": "published", "templateKey": "til", "path": "pages/til/dataframe-to-styled-html.md", "description": "I wrote up a little on exporting DataFrames to markdown and html  But I Reminder that if you have a dataframe,  Well you can pass some  I don There are several ", "content": "I wrote up a little on exporting DataFrames to markdown and html [here](/dataframe-to-markdown)\n\nBut I've been playing with a web app for with lists and while I'm toying around I learned you can actually give your tables some style with some simple css classes! \n\n# To HTML\n\nReminder that if you have a dataframe, `df`, you can `df.to_html()` to get an HTML table of your dataframe.\n\nWell you can pass some `classes` to make it look super nice!\n\n# Classes and CSS\n\nI don't know anything really about CSS so I won't pretend otherwise, but as I was learning about bootstrap that's where I stumbled upon this...\n\nThere are several classes you can pass but I found really good luck with `table-bordered` and `table-dark` for my use case\n\n`df.to_html(classes=[\"table table-bordered table-dark\"])`\n\n<table border=\"1\" class=\"dataframe table table-bordered table-dark\">  <thead>\n<tr style=\"text-align: right;\">      <th>Unnamed: 0</th>      <th>mpg</th>\n<th>cyl</th>      <th>disp</th>      <th>hp</th>      <th>drat</th>\n<th>wt</th>      <th>qsec</th>      <th>vs</th>      <th>am</th>\n<th>gear</th>      <th>carb</th>    </tr>  </thead>  <tbody>    <tr>\n<td>Mazda RX4</td>      <td>21.0</td>      <td>6</td>      <td>160.0</td>\n<td>110</td>      <td>3.90</td>      <td>2.620</td>      <td>16.46</td>\n<td>0</td>      <td>1</td>      <td>4</td>      <td>4</td>    </tr>    <tr>\n<td>Mazda RX4 Wag</td>      <td>21.0</td>      <td>6</td>      <td>160.0</td>\n<td>110</td>      <td>3.90</td>      <td>2.875</td>      <td>17.02</td>\n<td>0</td>      <td>1</td>      <td>4</td>      <td>4</td>    </tr>    <tr>\n<td>Datsun 710</td>      <td>22.8</td>      <td>4</td>      <td>108.0</td>\n<td>93</td>      <td>3.85</td>      <td>2.320</td>      <td>18.61</td>\n<td>1</td>      <td>1</td>      <td>4</td>      <td>1</td>    </tr>    <tr>\n<td>Hornet 4 Drive</td>      <td>21.4</td>      <td>6</td>      <td>258.0</td>\n<td>110</td>      <td>3.08</td>      <td>3.215</td>      <td>19.44</td>\n<td>1</td>      <td>0</td>      <td>3</td>      <td>1</td>    </tr>    <tr>\n<td>Hornet Sportabout</td>      <td>18.7</td>      <td>8</td>\n<td>360.0</td>      <td>175</td>      <td>3.15</td>      <td>3.440</td>\n<td>17.02</td>      <td>0</td>      <td>0</td>      <td>3</td>      <td>2</td>\n</tr>  </tbody></table>\n\n\n# You try it!\n\nCrack open ipython and make a dataframe, then `df.to_html(classes=[\"table table-bordered table-dark\"])`, copy the output (minus the quote marks ipython uses to denote the string type) that into `my-file.html`, open that up in a browser and be amazed!\n\n> For added effeciency try using pyperclip to copy the output right to your clipboard!\n\n`pip install pyperclip` and then `pyperclip.copy(df.to_html(classes=[\"table table-bordered table-dark\"]))`", "date": "2022-05-07", "edit_link": "https://github.com/edit/main/pages/til/dataframe-to-styled-html.md", "today": "2022-06-25", "now": "2022-06-25 12:28:31.768144", "datetime": "2022-05-07 00:00:00+00:00", "slug": "til/dataframe-to-styled-html", "jinja": false, "output_html": "markout/til/dataframe-to-styled-html/index.html", "long_description": "I wrote up a little on exporting DataFrames to markdown and html  But I Reminder that if you have a dataframe,  Well you can pass some  I don There are several classes you can pass but I found really good luck with  df.to_html(classes=[\"table table-b", "year": 2022}, {"cover": "/static/skimpy.png", "title": "Skimpy", "tags": ["python"], "status": "published", "templateKey": "til", "path": "pages/til/skimpy.md", "description": "I work with data a lot, but the nature of my job isn When I However, Visidata is a terminal based application and I First thing to do is  This is super nice for", "content": "## EDA\n\nI work with data a lot, but the nature of my job isn't to dive super deep into a small amount of datasets,\nI'm often jumping between several projects every day and need to just get a super quick glance at some tables to get a high level view.\n\nWhen I'm doing more interactive exploration I've graduated from Jupyter cells with `df_N.head()` to using an amazing tool called [visidata](https://www.visidata.org/)\n\nHowever, Visidata is a terminal based application and I'm often in an iPython console... so is there a way to move even faster for my super quick summary views?\n\n__yes!__ \n\n## Skimpy\n\nFirst thing to do is `pip install skimpy` and then it's as easy to get some summary stats with `skimpy <data>`\n\n![Alt Text](/images/skimpy-zsh.png \"skimpy-zsh\")\n\nThis is super nice for seeing missing values in particular as well as the distribution shape of the data.\n\n## iPython\n\nBut wait... I just said I'm normally in an iPython session but that was called from zsh.. If I'm hoping back into zsh I might as well use visidata to have more powerful exploration at my fingertips.\nSo... can I see this table quickly without breaking my iPython workflow?\n\n__Of course you can with magic!__\n\n\n![Alt Text](/images/skimpy-ipython.png \"skimpy-ipython\")\n\n\nThe above assumes you're looking at a file, like you would in the terminal. \n`skimpy` works even better in iPython with `from skimpy import skim` then pass any DataFrame to `skim`!\n\n![Alt Text](/images/skimpy-ipython2.png \"skimpy-ipython2\")", "date": "2022-03-23", "edit_link": "https://github.com/edit/main/pages/til/skimpy.md", "today": "2022-06-25", "now": "2022-06-25 12:28:31.768151", "datetime": "2022-03-23 00:00:00+00:00", "slug": "til/skimpy", "jinja": false, "output_html": "markout/til/skimpy/index.html", "long_description": "I work with data a lot, but the nature of my job isn When I However, Visidata is a terminal based application and I First thing to do is  This is super nice for seeing missing values in particular as well as the distribution shape of the data. But wa", "year": 2022}, {"cover": "", "title": "Nextcloud permissions with ZFS and Ansible-NAS", "tags": ["homelab", "zfs"], "status": "published", "templateKey": "til", "path": "pages/til/nextcloud-permissions-with-zfs-and-ansible-nas.md", "description": "As the nextcloud docs say... if you want to write to an external volume that You want to self-host your own cloud and use a smart file system for convenience...", "content": "# TL;DR\n\nAs the nextcloud docs say... if you want to write to an external volume that\nlocation has to be writeable by the user/group `www-data` on the host system...\nso if that makes sense to you then this TIL probably isn't a ton of value.. if\nnot however, read on :)\n\n\n\n# Case Study\n\nYou want to self-host your own cloud and use a smart file system for convenience...\nNextcloud and ZFS are pretty common goto answers for each of those problems.\n\nMy home NAS is built on ZFS and among other things I have a `zpool` named\n`tank` and nested in there is a `tank/nas` dataset with several child zfs\ndatasets under that.\n\nI want to use nextcloud mainly for auto-uploading photos from my wife's and my phones for automatic backups.\nThe issue is that the nextcloud application (I run in Docker) is fixed as the\n`www-data` user and so any volume/folder that you want nextcloud to write to\nneeds to be permissioned such that `www-data` owns it... but I don't want\n`www-data` to own everything in my NAS... so what's a girl to do?\n\n# Solution\n\nWell, one way to go is to just utilize docker volumes, write the data in the\ncontainer to `/var/www/html` and let that be the place your data backsup to.\n\nI still wanted nextcloud to automatically write right to my NAS so I created a\n`nextcloud-upload` directory inside of `tank/nas/media/photos` (photos cause\nthat's all that gets automatically uploaded)\n\nThen I `chown -R www-data:www-data /tank/nas/media/photos/nextcloud-upload` so\nthat just that sub-folder is owned by `www-data`. Now everyone's happy!", "date": "2022-05-19", "edit_link": "https://github.com/edit/main/pages/til/nextcloud-permissions-with-zfs-and-ansible-nas.md", "today": "2022-06-25", "now": "2022-06-25 12:28:31.768158", "datetime": "2022-05-19 00:00:00+00:00", "slug": "til/nextcloud-permissions-with-zfs-and-ansible-nas", "jinja": false, "output_html": "markout/til/nextcloud-permissions-with-zfs-and-ansible-nas/index.html", "long_description": "As the nextcloud docs say... if you want to write to an external volume that You want to self-host your own cloud and use a smart file system for convenience... My home NAS is built on ZFS and among other things I have a  I want to use nextcloud main", "year": 2022}, {"cover": "", "title": "Filepath Completion in Neovim", "tags": ["vim"], "status": "published", "templateKey": "til", "path": "pages/til/filepath-completion-in-neovim.md", "description": "I Turns out I need to not be a dope and configure nvim-cmp to actually use it... For the sake of completeness here is how I currently (May 2022) configure compl", "content": "I've had `Plug 'hrsh7th/cmp-path'` in my plugins for ever but didn't notice\nuntil recently that I wasn't getting any filepath completion in vim!\n\n__Fuller setup instructions below the TLDR__\n\n# TL;DR\n\nTurns out I need to not be a dope and configure nvim-cmp to actually use it...\n\n\n```lua\nlocal cmp = require'cmp'\n\ncmp.setup({\n    -- removed rest of setup - see the rest in my dotfiles\n  sources = cmp.config.sources({\n    { name = 'path' },  -- This needs to be here!\n    })\n})\n```\n\n# My Setup\n\nFor the sake of completeness here is how I currently (May 2022) configure completion in Neovim usin `nvim-cmp`\n\n## Plugins\n\nI keep all my plugins in `plugins.vim`\n\n```vim\ncall plug#begin(s:plug_dir)\nPlug 'neovim/nvim-lspconfig'\nPlug 'hrsh7th/cmp-nvim-lsp'\nPlug 'hrsh7th/cmp-buffer'\nPlug 'hrsh7th/cmp-path'\nPlug 'hrsh7th/cmp-cmdline'\nPlug 'hrsh7th/nvim-cmp'\n\n\" For ultisnips users.\n<!-- \" Plug 'SirVer/ultisnips' -->\n<!-- \" Plug 'quangnguyen30192/cmp-nvim-ultisnips' -->\n\ncall plug#end()\n\n```\n\n## Vim Settings\n\nMy vim settings are also kept in their own file, `settings.vim`\n\n```vim\n\nset completeopt=menu,menuone,noselect\n\n```\n\n## nvim-cmp configuration\n\nI have a `cmp.lua` file that gets sourced in `init.lua` (file structure explained below) for configuring cmp.\n\n```lua\n\n  -- Setup nvim-cmp.\nlocal cmp = require'cmp'\n\ncmp.setup({\n  snippet = {\n    -- REQUIRED - you must specify a snippet engine\n    expand = function(args)\n      -- For `ultisnips` user.\n      vim.fn[\"UltiSnips#Anon\"](args.body)\n    end,\n  },\n  window = {\n      completion = cmp.config.window.bordered(),\n  },\n  mapping = {\n    ['<Down>'] = cmp.mapping.select_next_item({ behavior = cmp.SelectBehavior.Select }),\n    ['<Up>'] = cmp.mapping.select_prev_item({ behavior = cmp.SelectBehavior.Select }),\n    ['<C-d>'] = cmp.mapping.scroll_docs(-4),\n    ['<C-f>'] = cmp.mapping.scroll_docs(4),\n    ['<C-Space>'] = cmp.mapping.complete(),\n    ['<C-e>'] = cmp.mapping.close(),\n    ['<Tab>'] = cmp.mapping(cmp.mapping.select_next_item(), { 'i', 's' }),\n    ['<CR>'] = cmp.mapping.confirm({\n      behavior = cmp.ConfirmBehavior.Replace,\n      select = true,\n    })\n  },\n  sources = cmp.config.sources({\n    { name = 'nvim_lsp' },\n    { name = 'ultisnips' },\n    { name = 'buffer' },\n    { name = 'path' },\n    { name = 'tmux' },\n    })\n})\n\n```\n\n\nThe `sources` section is what was key for this post...\n\n# Piecing it together!\n\nMy `init.vim` sources plugins and then settings and then finally calls `init.lua`.\n`init.lua` sources my `cmp.lua` file and BANG! auto-completion.\n\n## More sources\n\nhrsh7th's wiki for `nvim-cmp` is [here](https://github.com/hrsh7th/nvim-cmp/wiki/List-of-sources) and has example configs as well as a list of sources...\n\n__Don't forget to configure and not just install!__\n\n[my dotfiles](https://github.com/nicpayne713/dotfiles)", "date": "2022-05-17", "edit_link": "https://github.com/edit/main/pages/til/filepath-completion-in-neovim.md", "today": "2022-06-25", "now": "2022-06-25 12:28:31.768165", "datetime": "2022-05-17 00:00:00+00:00", "slug": "til/filepath-completion-in-neovim", "jinja": false, "output_html": "markout/til/filepath-completion-in-neovim/index.html", "long_description": "I Turns out I need to not be a dope and configure nvim-cmp to actually use it... For the sake of completeness here is how I currently (May 2022) configure completion in Neovim usin  I keep all my plugins in  My vim settings are also kept in their own", "year": 2022}, {"cover": "/static/python-f-string-align.png", "title": "Python-F-String-Align", "tags": ["python"], "status": "published", "templateKey": "til", "path": "pages/til/python-f-string-align.md", "description": "I am personally trying to use  This little python script shows how options in the ", "content": "I am personally trying to use `logger` instead of `print` in all of my code, \nhowever I learned from [@Python-Hub] that you can align printouts using `print` with `f`-strings!.\n\nThis little python script shows how options in the `f`-string can format the printout.\n\n```python\n\nimport random\n\nvariables = \"Foo Bar Baz Bing\".split()\nscores = random.sample(range(1, 11), len(variables))\n\nprint(\"*\" * 30)\nprint(\"\\n\")\nprint(\"With 'varable' left aligned\")\nfor varable, score in zip(variables, scores):\n    print(f\"{varable:<10} | {score}\")\n\nprint(\"*\" * 30)\nprint(\"\\n\")\nprint(\"With 'varable' right aligned\")\nfor varable, score in zip(variables, scores):\n    print(f\"{varable:>15} | {score}\")\n\nprint(\"*\" * 30)\nprint(\"\\n\")\nprint(\"With 'varable' center aligned\")\nfor varable, score in zip(variables, scores):\n    print(f\"{varable:^5} | {score}\")\n\n```\n\n\n\n\n![Alt text](/images/py-print-align.png \"python print\")", "date": "2022-03-08", "edit_link": "https://github.com/edit/main/pages/til/python-f-string-align.md", "today": "2022-06-25", "now": "2022-06-25 12:28:31.768173", "datetime": "2022-03-08 00:00:00+00:00", "slug": "til/python-f-string-align", "jinja": false, "output_html": "markout/til/python-f-string-align/index.html", "long_description": "I am personally trying to use  This little python script shows how options in the ", "year": 2022}, {"cover": "/static/adblock-coverage.png", "title": "Adblock-Coverage", "tags": ["homelab", "til"], "status": "published", "templateKey": "til", "path": "pages/til/adblock-coverage.md", "description": "I run pi-hole at home for ad blocking and some internal DNS/DHCP handling. One thing I Credits to ", "content": "I run pi-hole at home for ad blocking and some internal DNS/DHCP handling.\n\n__pi hole posts on the way__\n\nOne thing I've never put too much thought in is asking \"how well am I doing at blocking?\"\nThere's lots of ways to measure that depending on what you care about but I just learned of [adblock tester](https://d3ward.github.io/toolz/adblock).\nIt's awesome and gave me a quick glimpse into how my pi-hole is performing on keeping my webpages clean and my DNS history private!\n\nCredits to [d3ward](https://github.com/d3ward/toolz) for the awesome tool!", "date": "2022-03-07", "edit_link": "https://github.com/edit/main/pages/til/adblock-coverage.md", "today": "2022-06-25", "now": "2022-06-25 12:28:31.768179", "datetime": "2022-03-07 00:00:00+00:00", "slug": "til/adblock-coverage", "jinja": false, "output_html": "markout/til/adblock-coverage/index.html", "long_description": "I run pi-hole at home for ad blocking and some internal DNS/DHCP handling. One thing I Credits to ", "year": 2022}, {"cover": "/static/fx-json.png", "title": "Fx-Json", "tags": ["til"], "status": "published", "templateKey": "til", "path": "pages/til/fx-json.md", "description": "It The installation with go was broken for me - both via the link and direct from the repo. Luckily  Usage is simple... ", "content": "[fx](https://github.com/antonmedv/fx) is an interactaive JSON viewer for the terminal.\n\nIt's a simple tool built with Charmcli's Bubble Tea.\n\n## Installation\n\nThe installation with go was broken for me - both via the link and direct from the repo.\nNow I'm not a gopher so I don't really know how to fix that.\n\nLuckily `npm install fx` also works and got me what I needed!\n\n## Usage\n\nUsage is simple... `fx <json file>`.\nThe Github has a few other ways such as `curl ... | fx` etc.", "date": "2022-04-19", "edit_link": "https://github.com/edit/main/pages/til/fx-json.md", "today": "2022-06-25", "now": "2022-06-25 12:28:31.768186", "datetime": "2022-04-19 00:00:00+00:00", "slug": "til/fx-json", "jinja": false, "output_html": "markout/til/fx-json/index.html", "long_description": "It The installation with go was broken for me - both via the link and direct from the repo. Luckily  Usage is simple... ", "year": 2022}, {"cover": "/static/psutil-01.png", "title": "Psutil-01", "tags": ["python"], "status": "published", "templateKey": "til", "path": "pages/til/psutils-01.md", "description": "Here Bonus Ipython tip", "content": "[Mike Driscoll](https://twitter.com/driscollis) has been posting some awesome posts about `psutil` lately.\nI'm interested in making my own system monitoring dashboard now using this library.\nI don't expect it to compete with Netdata or Glances but it'll just be for fun to see how Python can solve this problem!\n\n__Repo coming soon__\n\n## Example code:\nHere's a short snippit to get used/available/total RAM and disk space (on partitions that you probably care about)\n```python\n\nimport psutil\nimport socket\n\nprint(f\"System Memory used: {psutil.virtual_memory().used // (1024 ** 3)} GB\")\nprint(f\"System Memory available: {psutil.virtual_memory().available // (1024 ** 3)} GB\")\nprint(f\"System Memory total: {psutil.virtual_memory().total // (1024 ** 3)} GB\")\n\n\nprint(f\"Hostname: {socket.gethostname()}\")\n\npartitions = psutil.disk_partitions()\n\nfor part in partitions:\n    mnt = part.mountpoint\n    if \"snap\" in mnt or \"boot\" in mnt:\n        continue\n    disk = psutil.disk_usage(mnt)\n    print(f\"Usage at {mnt} on {part.device}: {disk.used // (1024 ** 3)} GB\")\n    print(f\"Free at {mnt} on {part.device}: {disk.free // (1024 ** 3)}GB\")\n    print(f\"Total at {mnt} on {part.device}: {disk.total // (1024 ** 3)}GB\")\n```\n\n> Bonus Ipython tip! Save this to a script called my_script.py and in Ipython you can %run -m my_script to run it!\n\n```bash\nproject \u21aa main v3.8.11 ipython\n\u276f %run -m system-monitor-psutils\nSystem Memory used: 25 GB\nSystem Memory available: 5 GB\nSystem Memory total: 31 GB\nHostname: ryzen-3600x\nUsage at / on /dev/nvme1n1p2: 81 GB\nFree at / on /dev/nvme1n1p2: 351 GB\nTotal at / on /dev/nvme1n1p2: 456 GB\n```", "date": "2022-03-16", "edit_link": "https://github.com/edit/main/pages/til/psutils-01.md", "today": "2022-06-25", "now": "2022-06-25 12:28:31.768193", "datetime": "2022-03-16 00:00:00+00:00", "slug": "til/psutils-01", "jinja": false, "output_html": "markout/til/psutils-01/index.html", "long_description": "Here Bonus Ipython tip", "year": 2022}, {"cover": "", "title": "Reset SSH key passphrase", "tags": ["homelab", "cli"], "status": "published", "templateKey": "til", "path": "pages/til/reset-ssh-key-passphrase.md", "description": "I got into a pickle where I encrypted the ssh keys I use for my SSH connections on LAN, but then I couldn ssh-keygen -p But I needed to remove the passphrase to", "content": "I got into a pickle where I encrypted the ssh keys I use for my SSH connections on LAN, but then I couldn't run my ansible playbook on my server! ssh-keygen -p and leave the new passphrase blank saved my day (although password protected key files are safer!)\n\n# TL;DR - just reset it to nothing\n\n`ssh-keygen -p` will let you reset the passphrase on your ssh keys (good for you! yay security!)\n\nBut I needed to remove the passphrase to quickly deploy an ansible playbook \ud83e\udd13", "date": "2022-06-13", "edit_link": "https://github.com/edit/main/pages/til/reset-ssh-key-passphrase.md", "today": "2022-06-25", "now": "2022-06-25 12:28:31.768200", "datetime": "2022-06-13 00:00:00+00:00", "slug": "til/reset-ssh-key-passphrase", "jinja": false, "output_html": "markout/til/reset-ssh-key-passphrase/index.html", "long_description": "I got into a pickle where I encrypted the ssh keys I use for my SSH connections on LAN, but then I couldn ssh-keygen -p But I needed to remove the passphrase to quickly deploy an ansible playbook \ud83e\udd13", "year": 2022}, {"cover": "/static/git-bisect.png", "title": "Git-Bisect", "tags": ["git"], "status": "published", "templateKey": "til", "path": "pages/til/git-bisect.md", "description": "I try to commit a lot, and I also try to write useful tests appropriate for the scope of work I Whether by laziness, ignorance, or accepted tech debt I don Befo", "content": "I try to commit a lot, and I also try to write useful tests appropriate for the scope of work I'm focusing on, but sometimes I drop the ball...\n\nWhether by laziness, ignorance, or accepted tech debt I don't always code perfectly and recently I was dozens of commits into a new feature before realizing I broke something along the way that none of my tests caught...\n\nBefore today I would've manually reviewed every commit to see if something obvious slipped by me (talk about a time suck \ud83d\ude29)\n\n__There must be a better way__\n\n# Bisect?\n\n`git bisect` is the magic sauce for this exact problem...\n\nYou essentially create a range of commits to consider and let `git bisect` guide you through them in a manner akin to Newton's method for finding the root of a continuous function.\n\n# How to do it?\n\nStart with `git bisect start` and then choose the first `good` commit (ie. a commit you know the bug isn't present in)\n\n```bash\n\nsandbox  \ue725 bisect-post  \uf21b \u00d71 via \ue235  v3.8.11(sandbox)  on \ue33d (us-east-1)\n\u276f git bisect start\n\nsandbox  \ue725 bisect-post (BISECTING)  \uf21b \u00d71 via \ue235  v3.8.11(sandbox)  on \ue33d (us-east-1)\n\u276f git bisect good 655332b\nbisect-post  HEAD         main         ORIG_HEAD\n5b31e1e  -- [HEAD]    add successful print (52 seconds ago)\n308247b  -- [HEAD^]   init another loop (77 seconds ago)\n4555c59  -- [HEAD^^]  introduce bug (2 minutes ago)\n9cf6d55  -- [HEAD~3]  add successful loop (3 minutes ago)\nbcb41c3  -- [HEAD~4]  change x to 10 (4 minutes ago)\n3c34aac  -- [HEAD~5]  init x to 1 (4 minutes ago)\n12e53bd  -- [HEAD~6]  print cwd (4 minutes ago)\n655332b  -- [HEAD~7]  add example.py (10 minutes ago)  # <- I want to start at this commit\n59e0048  -- [HEAD~8]  gitignore (23 hours ago)\nfb9e1fb  -- [HEAD~9]  add reqs (23 hours ago)\n\n```\n\n```bash\n\nsandbox  \ue725 bisect-post (BISECTING)  \uf21b \u00d71 via \ue235  v3.8.11(sandbox)  on \ue33d (us-east-1)\n\u276f git bisect bad 5b31e1e\nbisect-post                                                ORIG_HEAD\nHEAD                                                       refs/bisect/good-655332b6c384934c2c00c3d4aba3011ccc1e5b57\nmain\n5b31e1e  -- [HEAD]    add successful print (5 minutes ago)  # <- I start here with the \"bad\" commit\n308247b  -- [HEAD^]   init another loop (6 minutes ago)\n4555c59  -- [HEAD^^]  introduce bug (6 minutes ago)\n9cf6d55  -- [HEAD~3]  add successful loop (7 minutes ago)\nbcb41c3  -- [HEAD~4]  change x to 10 (8 minutes ago)\n3c34aac  -- [HEAD~5]  init x to 1 (9 minutes ago)\n12e53bd  -- [HEAD~6]  print cwd (9 minutes ago)\n655332b  -- [HEAD~7]  add example.py (14 minutes ago)\n59e0048  -- [HEAD~8]  gitignore (23 hours ago)\nfb9e1fb  -- [HEAD~9]  add reqs (23 hours ago)\n\n```\n\nAfter starting bisect with a \"good\" start commit and a \"bad\" ending commit we can let git to it's thing!\n\nGit checksout a commit somewhere about halfway between the good and bad commit so you can see if your bug is there or not.\n\n```bash\n\nsandbox  \ue725 bisect-post (BISECTING)  \uf21b \u00d71 via \ue235  v3.8.11(sandbox)  on \ue33d (us-east-1)\n\u276f git bisect bad 5b31e1e\nBisecting: 3 revisions left to test after this (roughly 2 steps)\n[bcb41c3854e343eade85353683f2c1c4ddde4e04] change x to 10\n\nsandbox  \ue725 HEAD (bcb41c38) (BISECTING)  \uf21b \u00d71 via \ue235  v3.8.11(sandbox)  on \ue33d (us-east-1)\n\u276f\n```\n\nIn my example here I have a python script with some loops and print statements - they aren't really relevant, I just wanted an easy to follow git history.\n\nSo I check to see if the bug is present or not either by running/writing tests or replicating the bug somehow.\n\nIn this session commit `bcb41c38` is actually just fine, so I do `git bisect good`\n\n```bash\n\nsandbox  \ue725 HEAD (bcb41c38) (BISECTING)  \uf21b \u00d71 via \ue235  v3.8.11(sandbox)  on \ue33d (us-east-1)\n\u276f git bisect good\nBisecting: 1 revision left to test after this (roughly 1 step)\n[4555c5979268dff6c475365fdc5ce1d4a12bd820] introduce bug\n\n```\n\nAnd we see that git moves on to checkout another commit...\n\nIn this case the next commit is the one where I introduced a bug\n\n`git bisect bad` then gives me:\n\n```bash\n\nsandbox  \ue725 HEAD (4555c597) (BISECTING)  \uf21b \u00d71 via \ue235  v3.8.11(sandbox)  on \ue33d (us-east-1)\n\u276f git bisect bad\nBisecting: 0 revisions left to test after this (roughly 0 steps)\n[9cf6d55301560c51e2f55404d0d80b1f1e22a33d] add successful loop\n```\n\nAt `4555c597` the script works as expected so one more `git bisect good` yields...\n\n```bash\nsandbox  \ue725 HEAD (9cf6d553) (BISECTING)  \uf21b \u00d71 via \ue235  v3.8.11(sandbox)  on \ue33d (us-east-1)\n\u276f git bisect good\n4555c5979268dff6c475365fdc5ce1d4a12bd820 is the first bad commit\ncommit 4555c5979268dff6c475365fdc5ce1d4a12bd820\nAuthor: ########################### \nDate:   Tue May 3 09:00:00 2022 -0500\n\n    introduce bug\n\n example.py | 2 +-\n 1 file changed, 1 insertion(+), 1 deletion(-)\n\n\n```\n\n# What happened?\n\nGit sliced up a range of commits based on me saying of the next one was good or bad and localized the commit that introduced a bug into my workflow!\n\nI didn't have to manually review commits, click through logs, etc... I just let git checkout relevant commits and I ran whatever was appropriate for reproducing the bug to learn when it was comitted!", "date": "2022-05-03", "edit_link": "https://github.com/edit/main/pages/til/git-bisect.md", "today": "2022-06-25", "now": "2022-06-25 12:28:31.768207", "datetime": "2022-05-03 00:00:00+00:00", "slug": "til/git-bisect", "jinja": false, "output_html": "markout/til/git-bisect/index.html", "long_description": "I try to commit a lot, and I also try to write useful tests appropriate for the scope of work I Whether by laziness, ignorance, or accepted tech debt I don Before today I would git bisect You essentially create a range of commits to consider and let ", "year": 2022}, {"cover": "/static/abstract-base-class.png", "title": "Abstract-Base-Class", "tags": ["python"], "status": "published", "templateKey": "til", "path": "pages/til/abstract-base-class.md", "description": "I don If you are creating a library with classes that you expect your users to extend, but you want to ensure that any extension has explicit methods defined th", "content": "## ABCMeta\n\nI don't do a lot of OOP currently, but I have been on a few heavy OOP projects and this `ABCMeta` and `abstractmethod` from `abc` would've been super nice to know about!\n\nIf you are creating a library with classes that you expect your users to extend, but you want to ensure that any extension has explicit methods defined then this is for you!.\n\n```python\nfrom abc import ABCMeta, abstractmethod\nclass Family(metaclass=ABCMeta):\n    @abstractmethod\n    def get_dad(self):\n        \"\"\"Any extension of the Family class must implement a `get_dad` method\"\"\"\n\nclass MyFamily(Family):\n    pass\n\n```\n\nIf I try to instantiate `MyFamily` I will not be allowed:\n```python\n\n\u276f my_fam = MyFamily()\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 <ipython-input-8-ecb8e21ce815>:1 in <module>                                                     \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\nTypeError: Can't instantiate abstract class MyFamily with abstract methods get_dad\n\n```\n\n![Alt text](/images/py-abc-meta.png \"abcmeta\")\n\nIn order for me to extend `Family` I have to implement the method `get_dad`\n\n```python\nclass MyFamily(Family):\n    def get_dad(self):\n        return \"Me\"\n```\n\nNow everything works as expected and I can sleep well knowing no one can extend my base class without creating methods I know they need.\n\n\n```python\n\nmy_fam = MyFamily()\n\nmy_fam.get_dad()\n'Me'\n\n```", "date": "2022-03-09", "edit_link": "https://github.com/edit/main/pages/til/abstract-base-class.md", "today": "2022-06-25", "now": "2022-06-25 12:28:31.768214", "datetime": "2022-03-09 00:00:00+00:00", "slug": "til/abstract-base-class", "jinja": false, "output_html": "markout/til/abstract-base-class/index.html", "long_description": "I don If you are creating a library with classes that you expect your users to extend, but you want to ensure that any extension has explicit methods defined then this is for you If I try to instantiate  In order for me to extend  Now everything work", "year": 2022}]}